{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vaibhav's Wiki","text":"<p>Hi..! Welcome to Vaibhav's personal Wiki. I am passionate Data Engineer having more than a decade of experience in building modern, scalable and durable data solutions on distributed systems.</p> <p>My Blog - Some of the very basic how to guides as I learn through</p> <p>Notes - Reference docs I prepared for personal learning.</p>"},{"location":"#my-work","title":"My Work","text":"<p>You can see my recent work around data analytics by visiting following links:</p> <ul> <li>Data Analytics on Kaggle</li> <li>Data Visualization on Tableau Public</li> </ul>"},{"location":"#kaggle","title":"Kaggle","text":"<p>Amazing place for data science enthusiasts. If you are learning data science you can find kernels and data sets on Kaggle. Some of my work are:</p> <ul> <li>VY Notebook - Data Science Notes</li> <li>Linear Regression in Machine Learning</li> <li>Distribution Plots using Seaborn</li> </ul> <p>Or you may see all my Kernels here.</p>"},{"location":"#tableau-public","title":"Tableau Public","text":"<p>Data Visualization has been my passion since I started my journey in Data and Analytics. You can find some of the dashboards I created in Tableau below:</p> <ul> <li>Whatsapp Chat Analysis - It shows chats by gender, time of the day and how chats grew over the month.</li> <li>Personal Trip Expense Report - It shows the expense by categories.</li> </ul> <p>Or click here to browse my Tableau Public Profile.</p>"},{"location":"#old-work","title":"Old Work","text":"<ul> <li>Easy Soft Sys - College days start up. Navigate to portfolio to see work delivered to clients.</li> </ul>"},{"location":"#about","title":"About","text":"<p>I am certified Big Data Hadoop developer by Horton Works and certified Mongo DB developer by Mongo DB.</p> <p>Previously, I was working as Senior Systems Engineer with \"Data and Analysis\" unit at Infosys Ltd, Chandigarh, India. My core area of work is Financial Services. I work with Business Intelligence team which delivers valuable insights on huge chunks of data.</p> <p>At Infosys, my client is Capital Group of Companies since Jan 2014. My major area of focus is on ETL process and reporting. It also includes AutoSys a job scheduler. Some of the tools I user are Informatica, Business Objects and Tableau. My primary skills used are Shell Scripting, PLSQL, Database and Java EE.</p> <p>Besides, I have been working on Bid Data POCs. Some of the tools which I have worked on are HiveQL, PIG, YARN and MapR.</p> <p>Previously I have worked on MapReduce 1.0 on a POC in which we used huge data set of a bank, managed it on Hadoop stack and reported using Tableau.</p> <p>Prior to Infosys, I have worked as Web Developer at Caterpillar Labs, IIT Kanpur. It was a startup working on art work e-commerce. Besides web development technologies it's major area of focus was on machine learning and recommendation engine.</p> <p>I graduated in Information Technology from UIET, CSJM University. Kanpur, Uttar Pradesh, India.</p>"},{"location":"#contact-me","title":"Contact me","text":"<p>Email me or reach to my social networks using below links.</p> <p>Happy Coding..! ;)</p>"},{"location":"Electronics%20and%20IoT/2021-07-27-electronics-notes/","title":"Electronics Notes, IoT, DIY, Hobby","text":"<p>Notes about IOT, Electronics etc.</p>"},{"location":"Electronics%20and%20IoT/2021-07-27-electronics-notes/#volts-amps-ohms-watts-hours","title":"Volts Amps Ohms Watts Hours","text":"<p>Voltage (V) Volts - pressure of water, or how fast the electrons are moving. volt can be measured in parallel, stays same.</p> <p>Current (I) Amps - amount of water, or number of electrons moving. Amps can be measured in series, stays same. The current through the load depends on the load and not on the supply. 2A supply means it can support load upto 2A, means it has enough electrons to move current upto 2A, however, the current drawn from supply in a circuit depends on the resistance in it, it does not depend on the supply. Higher resistance is less electrons flowing thru, hence less Amps.</p> <p>Resistor (R) Ohm - can be added in series to resist current (drops volts), so that bulb doesn't blow. bands tell ohms. </p> \\[ V = I.R , Ohm's Law \\] <p>Power (P) Watts - Power or rate of energy transfer or used.</p> \\[ P = V.I = \\frac{V^2}{R} = I^2.R \\] <p>Que - Caculate resitance required to power less volt component (LED) from high volt battery. You need: - fwd_voltage = electronic component = LED = 3.2 volts - source_volt = battery used / supply = 9v - amps = component amps = amps of LED = 24mA</p> <p>R = V / I , here find V remaining to be consumed by R, Amps remains same in series.</p> <p>R = (source_volt - fwd_volt) / Amps</p> <p>R = (9-3.2) / 0.024 = 240 ohms</p> <ul> <li>This resistor should be added in series to LED. Here, as in series, resistor is taking some volt and led is taking some volt.</li> </ul> <p>Flow in circuit  - current only flows when there is potential difference,  - current is drawn only when there is space for electrons to move, or the cricuit conducts.  - we can only change volts of source, but current drawn depends on load. we cannot provide high current. - current is amount of electrons flowing, depends inversely on resistance in circuit. - volts is pressure or how fast electrons are flowing. - a load, say LED, can only allow few electrons to move through (current) and at some pressure/pace (volts). if we increase volts, it will burn. - low resistance will allow more current to flow, hence more watts.  - high resistance allows less current to flow, in series it drops volts, or reduces pressure. - in parallel current gets multiple routes to move, hence overall current increases and resistance drops. - in series, resistances offer more restrictions hence less current and more resistance, - Hence   - Series, I is same, V is different   - Parallel, I is diff, V is same. - Example: A bulb, has resistance, this defines its volts and watts, say, 4V 1W for bike meter and 4v 20W for headlight. Volt can vary and hence Current and Watts will:    - 4V 1W =&gt; 16ohm (fixed), draws 0.25A   - now connect this to 12V source   - 12V 16ohm =&gt; draws 0.75A, becomes 9W.   - this is why the bulb blows when connected to high volts.</p> <p>Amp Hours - measures battery capacity, as in steady current flowing through one hour. 150Ah, inverter battery will give 15A for 10hours. A typical car battery with 12 volts rating has a capacity of 48 Ah. It means that when fully charged, the battery can deliver one amp for 48 hours, two amps for 24 hours and so on.</p>"},{"location":"Electronics%20and%20IoT/2021-07-27-electronics-notes/#components","title":"Components","text":"<p>LEDs - bulb, usually added with resister. This give resistance to circuit. Longer leg is +ve. - RGB has common anode and cathode LED. long to GND, light then cathode else anode. - Control brightness by PWM - pulse width modulation. - Mine is CC.</p> <p>Multimeter - measures volts, amps and resistance. Continuity, NPN and PNP, for AC and DC. When measuring AMPs do switch red com.</p> <p>Bread board - can be used for prototyping.</p> <p>Schematics is blueprint of a circuit.</p> <p>Potentiomenter is var resistor, or regulator.</p> <p>Capacitors - store chaege and act like battery  - unit is (F) Farads, and Volts it can handle.  - Prevents sudden start of motor, bulb, helps protect jerk in movements. - ceramic have no polarity.</p> <p>Diodes, current in one direction, valve - forward bias and reverse bias. - They take volts from the circuit, cylinderical with silver strip, negative. - Has volts drop, eg, 1.1v drop reduces volt by it. It eats volts in one direction. - acts as protection for led which can accept current in one directions only. - 1N4148 diode can be used upto 4V, single diode - 1N4007 rectifier diode, used &lt;1000V. Most used.</p> <p>Complete circuit: - resistor to reduce volts - capacitor to fade out led, in parallel - diode to prevent polarity</p> <p>Relays  - electronic switch, has electromagnet. another circuit makes switch operate. retangular box. its slow, so transistor was created. - Types:   - Electromechanical relay - has 5 terminals, 2 of electromagnet, 1 common and 1 Normally Open, 1 Normally Closed   - solid state relay</p> <ul> <li>Relay Oscillator:</li> <li>allows on-off loop, blinking light</li> <li>add capacitor in parallel</li> <li>make electromagnet go on and cut itself</li> <li>changing capacitor size will change frequency of on and off.</li> </ul> <p>Transistor - is base + cylindrical shaped.  - has 3 legs - base, collector and emmitter. Small +ve current in base completes the circuit.  - used for switching or amplifying. - Types:   - BJT - bipolar juction transistor - 2 types      - NPN - emmiter out, +ve signal in base     - PNP - emitter in, -ve signal in base   - MOSFET     - used to switch or amplify voltages in circuits.     - 3terminals, gate, drain and source.     - IRFZ24N - it blocks current until some volts is applied on gate. the more the volts on gate, more current it allows.      - A09N03N - MOsfet - eg, to supply small current we add high resistor in series, this can be photo resistor to make day/night switch. - models,    - NPN - BC547, 2N3904   - PNP - BC557, 2N3906 - 2N2222 is a common NPN bipolar junction transistor (BJT) used for general purpose low-power amplifying or switching applications. It is designed for low to medium current, low power, medium voltage, and can operate at moderately high frequency.</p> <p>Integrated Cuircuits (IC): - it has more than 1 circuit inside, can have componets like resistor, transistor or capacitor. called chip.  - save time, money, energy, space. - types   - Analogue/lienar IC - signal on gives cont output. eg, 7805, 555, LM386N   - Digital IC - no continuous output, but O/P is based on Logic Gates. eg, 7404 NOT Gate,  7408 AND Gate IC.    - Mixed IC -  .eg, ADC 0804, Analog to digital converter IC. - Forms - DIP, SMD, TO-220, eg,    - ATMEGA328P - Programmabel IC   - ESP8266 - wifi programmable - 555 timer IC is common,    - contains more than 20 transistors + more components.    - 8 pins, 8 is +ve, 1 -ve, 3 output.    - it has combination of logic to switch on and off.   - max load 200mA. if we power component more than this load the use mosfet. - Voltage regulator IC   - used to control voltage, eg, give different volts to diff components.   - Types:     - Linear volt regualtor - can only down volts, wasted volts as hear, to be used with heat sink. eg, 7805 - 5V, 7809 - 9V     - Switching volts regulator - wastes less energy, can down n up volts. eg, LM2678, LM2577. </p> <p>Transformer - step down - 220v to 12v, mobile charger - step up - inverter -</p> <p>Bridge Rectifier: - Converts AC to DC</p> <p>MicroControllers: - can be programmed to change current of output pins - Arduino, tiny computer on IC.  - AtTiny85</p> <p>Logic Gates: - XOR chip is IC with 14 pins for eg.</p> <p>Binary is number to base 2: - 8 bit computer can handle number till 8 bits. - binary half adder is used to add two numbers. - adders can be built using gates, AND XOR etc. which are ICs having pins. - We can extrnd this to have full binday adder and subtractor to add 8 bits numbers.</p> <p>PCB - Printed Circut Board are circuits on mdf board with sigle or multi layer copper connections. Surface Mount Components SMC are electronic components soldered on top of PCB. Through Hole Components THC are passed through hole and soldered on back of plate. Pick-and-Place machine use SMT - surface mount technology to place SMD surface mount devices on PCBs, JUKI is the company. Altium is US based.</p> <p>RF Communication module - nRF24L01 -  Rs 60. for drones. can be used for transmission and receiver. single chip radio transceiver for the world wide 2.4 - 2.5 GHz ISM band.</p> <p>Gyro and Accelero - MPU-6050 is  6-axis, cost 50. Micro Electro-mechanical system (MEMS),  It helps us to measure velocity, orientation, acceleration, displacement and other motion like features, also measure temp, -40 to 85</p> <p>Bluetooth - HC-06 is slave bluetooth module.</p> <p>Veroboard is used for prototypying before pcb, its like breadboard.</p> <p>Magnetic Sensor - A3144 - detects magnet when close</p> <p>Touch Switch - TTP223 , rs. 20. - is a PCB with A/B modes.</p> <p>Time - DS3231 RTC Rs. 250, module Precise Real-Time Clock Module is a low-cost, extremely accurate I\u00b2C real-time clock (RTC) with an integrated temperature-compensated crystal oscillator (TCXO) and crystal. The device incorporates a battery input and maintains accurate timekeeping when the main power to the device is interrupted</p> <p>LED RGB ws2812b 5v led Strip is controllable via Arduino, we can specify color and brightness of every single LED.</p>"},{"location":"Electronics%20and%20IoT/2021-07-27-electronics-notes/#tipsconcepts","title":"Tips/Concepts","text":"<ul> <li>Didode in parallel prevent volt volt reply on connct and disconnect.</li> <li>Capacitor in parallel prevent jerk and high volts reply. Reduces plasma.</li> <li>add 1kohm resistor in series to prevent damage by current.</li> <li>resistor could drop current however, the supply volt varies, to cover this use IC to for volt drop. To cover heat loss, and make efficient use 'DC-DC step-down buck converter'. 90% efficient. Ususally in car charger to convert 12v to 5v, more..</li> </ul>"},{"location":"Electronics%20and%20IoT/2021-07-27-electronics-notes/#arduino-kit","title":"Arduino KIT","text":"<p>Arduino boards having microporcessor. single board computer. uno for begineers, nano for breadboard. Kit has related components. It is open source microcontroller. It comes original and compatible copies. Arduino nano is 125 each and can be used as processing unit.</p> <p>It has libraries same like python to do complex stuff. Install using library manager.</p> <p>LED with 220ohm - 12mA current, LED 2V, Resistor 2.7</p> <p>Active Buzzer makes sound. It has IC for sound, green circuit is passive, black is active. Active makes sound on current while passive needs square waves with 2K and 5K freq. So we can send high low signal to a pin, varying by a delay of 1 to 10 ms. THis will make sound with freq depending on delay. We can pass notes to passive buzzer and make play any song.</p> <p>Tilt Sensor are used to detect inclination or orientation. They are reliable, low-power, long- lasting and very inexpensive. Can tell arduino about on/off based on orientation, then based on that we can make another component operate, like making LED on/off.</p> <p>servo motor is a geared one, only capable of rotating 180 degrees and is commanded by transmitting electrical pulses from your Arduino. Brown wire is GND, Red is 5V, orange is signal. Signal can be position 0-180, this will make servo move that degree as quickly as possible, then we can delay.  Need servo lib.</p> <p>Ultrasonic sensor can measure distance, HC-SR04 is inexpensive and very easy to use. Need lib. capacity of 2cm to 400cm </p> <p>DHT11 Temperature and Humidity Sensor, The sensor includes a sensor of wet components and a temperature measurement device. It returns binary data string, which is the coverted by library to tell temp and humidity.  has pin GND, Data and 5V,</p> <p>Analog Joystick Module is used to control components. 5 pins, GND, VCC +5, X, Y, SW Key. It has XY analog output which gives direction with magnitude. and key is digital.  - Key on press connects to GND. Todo: A pull-up resistor or pull-down resistor is a resistor used to ensure a known state for a signal. To get accurate readings from the Key/Select pin, it should be connected to VCC with a pull-up resistor, which we can do using the built in resistors on the UNO digital pins - Range of X or Y is from 0-1024, mid value is approx 512. - Switch is 1/0 pressed or free.</p> <p>IR Module using lib, we can program IR receiver.  - IR hexa decimal codes are required to interpret the OP. - IR RECEIVER SENSOR - IR detectors are essentially small microchips with a photocell that are created to detect infrared light, - They detect and send low signal else high 5v. - 3 pins, GND -, 5V, Signal. Signal send digitalvalues which are converted to HEX by library <code>case 0xFFA25D: Serial.println(\"CH-\"); break;</code>. </p> <p>LCD Display LCD1602  - 16 pins   - VSS: A pin that connects to ground   - VDD: A pin that connects to a +5V power supply   - VO: A pin that adjusts the contrast of LCD1602   - RS: A register select pin that controls where in the LCD\u2019s memory you are writing data: either the data register, which holds what is displayed on the screen, or an instruction register, which is where the LCD\u2019s controller looks for instructions on what to do next.   - R/W: A Read/Write pin that selects reading mode or writing mode   - E: An enabling pin that causes the LDC module to execute relevant instructions when supplied with low-level energy.   - D0-D7:Pins that read and write data   - A and K: Pins that control the LED backlight - The LCD display requires six UNO pins as digital outputs. Additionally, it needs 5V and GND connections.  - We need to set Potentiometer to control brightness of Letter (not backlit) and then reset Arduino to display.</p> <p>Thermistor is simply a thermal resistor - a resistor that changes its resistance according to the temperature. 100ohm or more per degree.</p> <p>74HC595 Shift Register  - The shift register is a kind of chip containing eight memory locations, with the values 1 or 0. We input the data using the 'Data' and 'Clock' pins of the chip to set these values on or off. 8 clock pins and 8 data pins. We can combine this with arduino analog PWN write to control brightness of LED. - Serial to Parallel Converter - useful to power multiple LEDs from one output pin. - Pin 1 of the chip is to the left of this notch. +5v - Pin, Q0. - right side is Q1-Q7 and ground at bottom - Pin 14, 12 and 11 connect to UNO. </p> <p>Stepper Motor is an electromechanical device which converts electrical pulses into discrete mechanical movements. Used for movements and controlled with pulse of current. Has driver module. </p>"},{"location":"Electronics%20and%20IoT/2021-07-27-electronics-notes/#small-projects-and-devices","title":"Small Projects and Devices","text":"<p>Electric Motor Speed Controller - using 555 timer IC - IP 4.5v-16v, OP &lt;200mA. pin1 ground, pin8 +ve.  - Motor, 12V, 1.5A. It needs more current, so we use mosfet  to power 12v motor and use signal from 555. - Mosfet - IRFZ24N - &lt;17A, &lt;55V - uses small current in gate pin1 to output more current from drain pin2. Source pin3 is ground. No current in gate, no flow of current. More volt in gate, more volts from drain.- Current to gate is given by 555 pin3. This volt is on/off pulse. this gives average volts and called Pulse Width Modulation. - Add 1k ohm resistor in series b/w 555pin3 and mosfetpin1 to prevent 555 in case mosfet malfunctions and allows 12v to flow. Also add 1k ohm resistor in parallel to discharge this current. Need Explaination. - When motor is turned off, it produces high volts to clear magnetic field, to prevent this add flyback diode 1N4007. parallel. - ref - https://www.youtube.com/watch?v=UPTU6nYSaMo</p> <p>5V Regulator design - Input is 9-12V with fluctuation, but output is constant 5v. - use an IC LM7805, takes in random 9-12v outs 5v.  - add capacitor in 0.22uF cap, in parallel to smooth drops - add 0.1uF ceramic cap, to smooth noise. - 10uf elector cap, and 0.1uF ceramic cap in OP to smooth out flow. - add diode in IP to prevent polarity fault. schottky diode has less drop so add it. - ref - https://www.youtube.com/watch?v=d-j0onzzuNQ - this has isse of heat loss by 7805.</p> <p>DTH - Free Dish - Settop Box - SMPS - 2658A1.PCB - 200 rs - converts 220v to 5v DC</p> <p>Fan Resistor 220v B1 R-783 - cap - 3.3uF 250V - res - 1m in parallel - res - 4.7ohm, 0.5w - 1p4t switch.</p> <ul> <li>SE 104j2A - capacitor, 100V 2A</li> <li>BT124 600E - Thyristors - TRIACs</li> <li>ref - https://www.youtube.com/watch?v=k4c3_yCfLWA</li> </ul> <p>diac triac light dimmer circuit - triac - bt136 - diac - DB3 - capacitor - 104k 440v - potentiomenter - 500k - resistor - 10k - ref    - en https://www.youtube.com/watch?v=OmBu3emRdV8   - hi https://www.youtube.com/watch?v=C1qGVaGgGOo</p> <p>Step-down buck converters - reduces volts without wasting energy.</p> <p>WiFi Relay Switch - ref - https://www.youtube.com/watch?v=TZnrHkjlgLk</p> <p>Malaysian Baloon - 6v, 40mA - working.</p> <p>Trimmer  - resistor - 100ohm 5%</p> <p>D Duke Adapter - 500mA or 0.5A at 12V = 6W max load.</p> <p>Balaji Adapter - OP = 12V 2A = 24W max load.</p> <p>Lights LEDs - MegaGold - 2.4W = 0.2A = 200mA. 10 lights with BalaJi Adapter. - Car Music RGB - 0.23A at 12V = 2.76W</p> <p>2CH RC Remote Control 27MHz - RC Remote Car</p> <p>DIYs: - building material:   - PVC sheets, PVC pipe to flat, PVC fanti.    - MDF boards   - Rubber sheets - tools:   - Small Drill Bits   - Glue Gun   - Fevi Quick - Elec:   - PCB Board, empty board   - Switches.    - Hot Glue Gun</p> <p>Todo: - Add remote to symphony cooler - Add mobile controlled Motor starter and water level monitor   - Solar tank level monitor, wifi/rf to send signals.   - Relay to start motor.</p> <p>References: - Learn Beginner Electronics YouTube - Multimeter Manual - https://www.petervis.com/meters/dt830d/dt830d-how-to-use-instructions.html - https://quadstore.in/ - Water plants auto - https://www.electromaker.io/blog/article/elecrow-smart-plant-watering-system-using-arduino-uno-review-and-tutorial - Channels:   - Circuit Digest - https://www.youtube.com/channel/UCy3CUAIYgZdAOG9k3IPdLmw   - Manmohan Pal - https://www.youtube.com/c/ManmohanPal   - tech Ideas - https://www.youtube.com/channel/UCNtV2t2MX3qGkBSD_uRtCGg   - The engiuneering mindset - https://www.youtube.com/c/Theengineeringmindset   - Mega Electronics - https://www.youtube.com/channel/UCl9W8s1E1aXmODa_8fTSbhw   - DD ELectro Tech - https://www.youtube.com/user/Deba9681895487 - </p>"},{"location":"Functional%20%26%20Management/2020-06-29-banking-notes/","title":"Banking and Finance Notes","text":""},{"location":"Functional%20%26%20Management/2020-06-29-banking-notes/#how-banks-make-money","title":"How Banks make money?","text":"<p>Banks makes money by following methods: 1.  Investment Banking: Most of the income Barclays make is by being an advisor to big corporations, government and individuals.  2.  Corporate Banking: It provides services for multinationals and large domestic corporate. It charges fee, interests for loans from these companies. 3.  By charging interest from borrowers. 4.  By charging various fees on services like,    a.  Account Fees: For different products in bank, it charges fee for maintenance and services.   b.  ATM fees: This is charged for ATM usage outside limit and when using other bank\u2019s ATM for services.   c.  Penalty charges: This can be for late credit card payment or missing a loan EMI.   d.  Commissions: They provide asset management for which brokers ask for commission. 5.  Merchant Service Charges: Barclays help merchants accept payments. For this bank charges fee for services. It also helps merchant by data analysis for this as well it charges other fees for value added services.</p> <p>Net Interest Margin (NIM): - (Interest Earned) - (Interest Paid). - NIM% = NIM / (Avg earning asset), where - AEA = Average earning asset = (Assets at the beginning of the year + Assets at the end of the year) / 2. - The difference between the interest income generated by banks  and the amount of interest paid out to their lenders (for example, deposits), relative to the amount of their (interest-earning) assets.</p> <p>Retail banking (Consumer Banking): - Individual consumers can manage their money, take credit, and deposit their money - Services include checking and savings accounts, mortgages, personal loans, credit cards, FD/RD.</p> <p>Income TAX - Form 26as has information on FD interest</p>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/","title":"Project Management Notes","text":"<p>Project Management is balancing between \"the project scope\" and \"the time, resources and budget\" you have. Manage \"the time, resource and budget\" to cover \"the project scope\". It includes planning process like requirement gathering, planning, solution design, code, test, deploy. We need to break down the project into doable work tasks. Once broken, estimate them. Finally assign them to get started. It can include the steps below:</p>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#1-initiation-and-ideation","title":"1 Initiation and Ideation","text":"<ul> <li>Business Requirement Gathering</li> <li>What are you solving and how will you solve it? The business use case.</li> <li>Define use case, scope and expectations. This is very high level and covers the business problem.</li> <li>Project should have a definite end - a product or a service. It should have definition of done.</li> <li>Above can result in <code>Business Requirements Document</code></li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#2-defining-goals-and-objectives","title":"2 Defining - Goals and Objectives","text":"<ul> <li>What needs to be done? How it can be done? Define the goal and objectives.</li> <li>Goal - should clearly and simply define a state considering most important factors.</li> <li>Objectives - they sould be specific, measurable, achievable, realistic and time-related. Documented. Also specify the category of aobjective:</li> <li>qualitative - improve experience. measure by survey rating</li> <li>financial - inc revenue by 15%</li> <li>operational - reduce number of notifications</li> <li>Definition of Done. This will help breaks down the problem into sub-tasks and defines what is expected.</li> <li>Above can shape the <code>High Level Document</code></li> <li>Identify the Stakeholders and in the document add their:</li> <li>objectives requirements and interests</li> <li>contribution</li> <li>what are they concerned about</li> <li>their line mangers, eg, if you need somoeone from finance team then take approval from their line managers.</li> <li>Be precise, take what matters and drop what doesn't. Clearly define what is in and out of scope, write it down.</li> <li>It can take several rounds with stakeholders to get correct objectives and scope.</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#3-planning-choose-a-strategy","title":"3 Planning - Choose a Strategy","text":"<ul> <li>Brainstorm with group and let ideas flow in.</li> <li>Write possible options to achieve an objectives. Then pick one of the option that covers all scenarios and meets all objectives and goals.</li> <li>Considerations to be made</li> <li>is the strategy feasible, achievable?</li> <li>are the risks acceptable: security, load balancing, new technologis challenges etc?</li> <li>culture - does it fit the org pattern?</li> <li>above can shape <code>Low Level Document</code></li> <li>Get a F2F sign-off here, over email and treat it as approved.</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#4-solution-design-modules-tasks-sub-tasks-deliverables","title":"4 Solution Design - Modules, Tasks, Sub-Tasks, Deliverables","text":"<ul> <li>Break the objectives/goals in to <code>modules</code>,  <code>technical work tasks</code> and <code>sub-tasks</code> using the strategy and define <code>deliverables</code>.</li> <li>Tasks - Add business understandings, definitions and calculations.</li> <li>Sub-task - Add definition of done. Managable and doable tasks.</li> <li>Deliverables - Identify them, clearly and quantifiably measure them</li> <li>Define Scenarios and map expectations - the above tasks should cover all scenarios and expectations.</li> <li>This sould define the <code>Software Design Document</code> can be planned on Jira and documented on Confluence.</li> <li>Take a technical sign-off and approval if required.</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#5-delivery-plan-estimate-assign","title":"5 Delivery Plan - Estimate Assign","text":"<ul> <li>Arrange work tasks in sequence, link them with dependencies, add duration.</li> <li>Resource Allocation - assign the tasks to resources. These can be done in Jira. Look out for blockers and unavailability.</li> <li>Make a realistic schedule - include holidays, dependencies.</li> <li>Deadlines - Management can set a deadline, you need to adjust schedule to meet it. Add resource, or break into phase. Do phase analysis - define the MVP to deliver early. Do Phase II enhancements etc.</li> <li><code>Gantt Chart</code> - optional reporting.</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#6-risk-assessment-clarify-assumptions","title":"6 Risk Assessment - Clarify Assumptions","text":"<ul> <li>Avoid risks that are based on assumptions, like someone will do the deployment, access would work.</li> <li>Will the business stop if solution is down? What if resource not available?</li> <li>What if data gets corrupt?</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#7-other-optional-documentsplans","title":"7 Other optional documents/plans","text":"<ul> <li>Budget - Add costs, include resources, softwares.</li> <li>Communication plan - scrums, weekly, daily</li> <li>Change Management Plan - approvals, what changes when, impact</li> <li>Procurement plan - to buy software, resources, contracting</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#8-execution-development","title":"8 Execution - Development","text":"<ul> <li>Write code and document it.</li> <li>Do pilot delivery - a quick delivery and test. If works, keep expanding by adding features.</li> <li>Monitoring and Controlling - evaluate and get it back on track if lagging or deviated.</li> <li>Keep unit testing the code.</li> <li><code>Deliverables</code> - code files, reports, documents. All should be in one place and version controlled.</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#9-qa-prepration-testing","title":"9 QA Prepration - Testing","text":"<ul> <li>Make test cases and test scenarios as you go.</li> <li>Identify Testers from Stake Holders.</li> <li>Make a <code>bug tracker</code> where any one can report bugs and it can be tracked.</li> <li><code>QA Document</code> - add test cases and their results.</li> <li>Get a UAT Sign Off of the deliverables.</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#10-deployment","title":"10 Deployment","text":"<ul> <li>Deploy in prod. Test it.</li> <li>Prod Env is secured and hence may require many access permissions. Please see this in advance.</li> <li>Change Management may be required here.</li> <li>Once deployed, do a Prod Testing.</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#11-handover-user-training","title":"11 Handover - User Training","text":"<ul> <li>Prepare a <code>Training Guide</code> - for end users. This can be video as well.</li> <li>Make a <code>Handover Document</code> - if this need to be handed over to maintenance team to work on manual tasks.</li> <li>Manual tasks - include scope, work required, frequency, risks etc.</li> <li>Contracts - get signed-off if required.</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#best-practices","title":"Best Practices","text":"<ul> <li>Deliverables and Documentation - All files and documentation at one place and all have access. Confluencem, Jira Boards and Shared Drives.</li> <li>Estimation - Set clear goals. Manage Workload.</li> <li>Communication - 1-1 meetings weekly. Daily Updates.</li> <li>Change Management - if change is required in between, follow a change management procedure and redo all steps and sign offs.</li> <li>Reviews - Doc, deliverable. Continuous review helps.</li> <li>Opennes - Be open, let them choose tool, let them choose way, keep them foucsed. Trust them.</li> <li>Risk Management - each team member is accountable to explain risk in their task to entire team. Don't let people assume the work, ask them and clarify it.</li> </ul>"},{"location":"Functional%20%26%20Management/2022-06-08-project-management/#links","title":"Links","text":"<ul> <li>Steps https://www.wrike.com/blog/foolproof-project-plan/</li> <li>Detailed https://www.smartsheet.com/content/software-project-management</li> <li>LinkedIn Learning - https://www.linkedin.com/learning/project-management-foundations-4</li> </ul>"},{"location":"JavaScript/2018-07-03-nativescript-notes/","title":"NativeScript Notes","text":"<p>NativeScript ( or tns Telerik NativeScript) is used to make native iOS and Android apps using Angular/TypeScript or JavaScript.</p>"},{"location":"JavaScript/2018-07-03-nativescript-notes/#setting-up-the-environment","title":"Setting up the environment","text":"<p>Required: - node - nativescript cli - native script cli uses npm</p> <p>Installation: - do npm install script and then add package</p> <ul> <li><code>$ sudo npm install nativescript -g</code> to install globally</li> <li><code>nativescript --version</code> to confirm installation</li> <li>tns is alias to nativescript</li> </ul> <p>iOS prerequisites - Xcode - Xcode CLT</p> <p>NS CLI - component - module - html</p>"},{"location":"JavaScript/2018-07-03-nativescript-notes/#application-architecture","title":"Application Architecture","text":"<p>How the appication is architectured.</p>"},{"location":"JavaScript/2018-07-03-nativescript-notes/#javascript","title":"JavaScript","text":"<ul> <li>app.ts is the entry point of application. We can do app level initialization here. But this is basically used to pass control to first module.</li> <li>we can have app.css to keep our global css rules.</li> <li>A folder for each module/view</li> <li>eg: home</li> <li>home folder can have three files. </li> <li>home-page.xml having xml for UI</li> <li>home-page.ts code behind file for xml above. Can call functions</li> <li>home-view-model.ts having data, binding and other logics.</li> </ul>"},{"location":"JavaScript/2018-07-03-nativescript-notes/#angular-script","title":"Angular Script","text":"<p>for each module we need: - home.component.html it has all html ui defined - home.component.css has css rules related to this module - home.component.ts has code behind the html ui - home.module.ts imports everything to one place - home-routing.module.ts routing for module ui</p> <p>Component is building block of all angular nativescript - Component defines UI elem and screens - root - app.component - child - pt_backlog - backlog module</p> <p>Modules - use one module per file as an ES15 standard</p>"},{"location":"JavaScript/2018-07-03-nativescript-notes/#notes","title":"Notes","text":"<ul> <li>Every UI element should be inside a layout, else only last UI element takes whole screen.</li> <li>Making Angular Groceries App</li> <li>All UI elements come in stack layout in app.component.ts and add their css to css files</li> </ul>"},{"location":"JavaScript/2018-07-03-nativescript-notes/#uninstaslling","title":"Uninstaslling:","text":"<ul> <li><code>$ sudo npm uninstall nativescript -g</code> to uninstall globally</li> </ul> <p>References: - Refer: https://docs.nativescript.org/angular/start/tutorial/ng-chapter-1#11-what-youre-building</p>"},{"location":"JavaScript/2018-07-12-javascript-notes/","title":"JavaScript Libraries and Frameworks Notes","text":""},{"location":"JavaScript/2018-07-12-javascript-notes/#node-js","title":"Node JS","text":"<p>Overview:</p> <ul> <li>node js is like asp/php/python, it makes use of JS for backend lang.</li> <li>you can run .js files outside browser</li> <li>npm is package manager for node js, like pip</li> <li>pakages are nothing but js libraries.</li> <li><code>npm install</code> downloads a package and it's dependencies defined in a <code>package.json</code> file and generates a node_modules folder with the installed modules.</li> </ul>"},{"location":"JavaScript/2018-07-12-javascript-notes/#angular-js","title":"Angular JS","text":"<p>Overview:</p> <ul> <li>We can define a resource in app folder eg: hero.ts to include the class with it's members.</li> <li>We can make components for various behaviors of our app. eg:</li> <li>heroes - to list all heroes</li> <li>hero-detail - to keep functionality for one hero</li> </ul>"},{"location":"JavaScript/2018-07-12-javascript-notes/#typescript","title":"Typescript","text":"<p>It is superset of javascript, basically used to make use of new features of JS coming with ES6, ES7 and compile them back to old ES3 that can be used with IE and Safari as well.</p>"},{"location":"JavaScript/2018-07-12-javascript-notes/#javascript","title":"JavaScript","text":"<p>Functions can do whatever we want them to do.. :)</p> <p>How Functions behave:</p> <p>We can make a function in JS by defining it like we define a variable.</p> <pre><code>myFunction = function(arg1, arg2, arg3) {\n// all that you want to do\n// use args or may be they are optional\n//...\nreturn myResult;\n}\n</code></pre> <p>another way is, tree is a function and data is argument:</p> <pre><code>myFunction = arg1 =&gt; {\n// all that you want to do\n// use args or may be they are optional\n//...\nreturn myResult;\n}\n</code></pre> <p>now this function can return a variable which could possibly be any data/object/json etc.</p> <p>Function can now be used to get the result</p> <pre><code>myFunction(arg1, arg2).done( function (data) {\n// Now do what you want to do with result in data\n});\n</code></pre> <p>Now we see that, done would be triggered once myFunction is completed. All jQuery promises provide a done method that takes a callback.</p> <p>JS Objects Concept</p> <ul> <li>JS Object can hold anything, they can even hold another function.</li> <li>They are accessed using . DOT notation.</li> </ul>"},{"location":"JavaScript/2018-07-12-javascript-notes/#how-to-quickly-scrape-all-links-from-page","title":"How to quickly scrape all links from page","text":"<p>Use JS path to get all a tags you are interested in, do inspect, go to div having all a, then right click and copy <code>JS Path</code>. Then add path till <code>a</code> tag to this <code>div JS Path</code>. Now that you have JS Path to all the anchor tags, you can iterate over them and make a list then copy. You can execute this in console directly, eg:</p> <ul> <li>JS Path to div <code>#root &gt; div &gt; div.sc-AxjAm.tlQbp &gt; div &gt; main &gt; div &gt; div &gt; div</code></li> <li>JS Path to all a tags <code>article &gt; div:nth-child(2) &gt; div &gt; div:nth-child(5) &gt; a</code></li> <li>Join both together, then loop</li> </ul> <pre><code>links = '';\ndocument.querySelectorAll(\"#root &gt; div &gt; div.sc-AxjAm.tlQbp &gt; div &gt; main &gt; div &gt; div &gt; div &gt; article &gt; div:nth-child(2) &gt; div &gt; div:nth-child(5) &gt; a\").forEach(function (e) {links+=\"yourCmd \"+e.href+\" \\n\";})\ncopy(links)\n</code></pre> <p>This copies the output to clipboard. Related Posts:</p> <ul> <li>NativeScript Notes</li> <li>ECMA6 Notes</li> <li>React JS Notes</li> </ul>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/","title":"ECMA 6+ Notes","text":"<p>ECMAScript is a JavaScript standard meant to ensure the interoperability of web pages across different web browsers.</p> <p>ECMA 6 or ES 6 or ECMAScript 2015, added new feature to javascript which are highlighted below.</p>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/#new-changes-in-es6","title":"New Changes in ES6","text":"<p>Some changes in ES6 compared to ES5</p>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/#variables","title":"Variables","text":"<p><code>let</code> is used to allow block scope of variables.</p> <p><code>const</code> is used to define a fixed value to variable, if it changes, it throws error and does not allow to change.</p> <p>Template strings are used to print formatted strings. use <code>Name is $(fName)</code>, here fName is a variable. it cna be multiline too.</p> <pre><code>let greeting = `Hello, ${name}!`;\n</code></pre>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/#functions","title":"Functions","text":"<p><code>for ... of</code> new for loop.</p> <pre><code>for (name of names) {\nconsole.log(name)\n}\n</code></pre> <p><code>new Symbol()</code> can be added to an object (dict) to give it a unique identifier that never conflicts with its other keys.</p> <p><code>new Map()</code> is new data type that can be used to hold key and value of any type, we can have non-string keys. We can mix the data types. both key and value can be value or key. it is iterable in order of insersion.</p> <p><code>new Set()</code> have unique values. it has index 0,1...n.</p> <p><code>...</code> is spread operator and is used to flatten an array.</p> <p>Object's key/values can be functions as well. In this case objects behave as class having static instance.</p> <p>Desctucturing - If in function argumnet we see <code>{}</code> thats destructuring of object. We can only recieve key of object using this syntax. Eg, <code>function App ( {emp} ) {...}</code></p> <pre><code>// destructuring array\nconst [a,b] = [1,2]; // a=1, b=2\n// destructuring object\nconst {p,q} = {p:1,q:2} // use same key name\n// p=1, q=2\n</code></pre>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/#import-and-export","title":"Import and Export","text":"<p>Now you can import functions from modules that export it. This is more robust dependency management compared to using <code>&lt;script&gt;</code> tag.</p> <p>A JavaScript module is a JS file. It can have a function with <code>export</code> to make use of it in another file. Or import entire JS file.</p> <pre><code>export GRAVITY = 9.81;\nexport default function abc() {\nconsole.log('from abc in file1.js !');\n}\n</code></pre> <p>Another file can import it. As it is <code>default</code>, it can be imported with any name.</p> <pre><code>import abc from './file1.js';\nimport { GRAVITY } from './file1.js';\nimport './index.css';\n</code></pre> <p>Provide relative path of file (module). <code>.js</code> is optional. <code>index.css</code> has no export and hence entire file is imported without using <code>from</code> keyword.</p> <p>Other way is to use library, like <code>react</code>. You can import without absolute path. <code>Module</code> is a unit of software that is basically a file that you can refer. It usually has variables and functions. <code>Library</code> is collection of modules that is distributed as <code>package</code> and is managed by a manager like npm or pip. Library has multiple files.</p> <pre><code>import React from 'react';\n</code></pre>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/#arrow-functions","title":"Arrow Functions","text":"<p><code>=&gt;</code> arrow funcitons.</p> <pre><code>let funcName = (arg1, arg2) =&gt; {\nstatement1;\nreturn statement2;\n}\n</code></pre> <p>If there is one argument then remove the parantheses brackets, if there is only one statement the remove the curly barckets and the return keyword. Eg,</p> <pre><code>const square = x =&gt; x * x; // no  ()\nconst mult = (x, y) =&gt; x * y; // no {}\nconst mult = (x, y) =&gt; {\nconst result = x * y;\nreturn result;\n};\ndata.map(element =&gt; return (console.log(element))\n);\n</code></pre> <p><code>map</code> is function of <code>Array</code> class.</p> <p><code>generator</code> functions can be used to add a pause to execution of function and make it execute in parts using <code>yield</code> keyword. That is a funciton can yield many outputs or returns. It can be iterated calling <code>funcName.next()</code></p>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/#promises-and-asynchronous","title":"Promises and Asynchronous","text":"<p>Promises and Asynchronous behaviour - Async means that there is delay in response when requested. A promise is a proxy object that is returned to the caller of an asynchronous operation running in the background. This object can be used by the caller to keep track of the background task and obtain a result from it when it completes.</p> <p>In JS code, promise is executed in background and the execution moves to next statement. Later promise can either resolve or reject and the chained methods gets executed.</p> <p>A Promise is in one of these states:</p> <ul> <li>pending: initial state, neither fulfilled nor rejected.</li> <li>fulfilled: meaning that the operation was completed successfully.</li> <li>rejected: meaning that the operation failed.</li> </ul> <p>When we define then Promise takes a callBackFunction as an argument.</p> <p><code>myPromiseObj.then(funcName() {}</code> anything in this funcName is passed as <code>resolve</code> to Promise.</p> <p>This object can be used by the caller to keep track of the background task and obtain a result from it when it completes. We can use <code>then</code> in chain to execute one func on top of other.</p> <pre><code>const myPromise = new Promise((resolve, reject) =&gt; {\nsetTimeout(() =&gt; {\nresolve('foo');\n}, 300);\n});\nmyPromise\n.then()\n.catch(e =&gt; console.log(e)); // prints error if \n// Example\nfetch('http://example.com/data.json') // returns apromise\n.then(r =&gt; r.json()) // output of promise is passed to r\n.then(data =&gt; console.log(data)) // chained functions\n.catch(error =&gt; console.log(`Error: ${error}`)); // Error cached\n</code></pre> <p>This is good, but all execution has to be chained which makes it different than a normal function. To make the syntax same as normal funciton we can use <code>async... await</code></p>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/#async-and-await","title":"Async and Await","text":"<p>Async/Await - we can define an async function, which executes a Promise(), and then await until the promise is resolved. This makes synchronous execution for a asynchronous call.</p> <pre><code>async function f() {\nconst r = await fetch('https://example.com/data.json');\nconst data = await r.json();\nconsole.log(data);\n}\n</code></pre> <p>Error handling can be done using <code>try.. catch</code> block in this.</p>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/#classes","title":"Classes","text":"<p>Now you can use classes in ES6.</p>"},{"location":"JavaScript/2022-07-11-js-ecma6-notes/#jsx-javascript-xml","title":"JSX - JavaScript XML","text":"<p>This is not part of ES6, but is an extension to make it easier to use HTML in JavaScript. It lets us write HTML inline and this templates are eaisier to maintain. More in React</p> <p>Links</p> <ul> <li>Modern JavaScript - https://blog.miguelgrinberg.com/post/the-react-mega-tutorial-chapter-1-modern-javascript</li> <li>Learning ECMAScript 6+ (ES6+) - https://www.linkedin.com/learning/learning-ecmascript-6-plus-es6-plus/using-modern-javascript-today</li> </ul>"},{"location":"JavaScript/2022-07-14-react-js-notes/","title":"React JS Notes","text":"<p>It is a JS Library, it can be added to HTML page as other JS libraries (like jQuery). It is used to create single-page applications. It uses only one HTML page <code>index.html</code> and then all changes to page and routes are managed strictly by JS events.</p> <p>Build in Browser using https://react.new/</p> <p>Upgrade browser with <code>React Developer Tool</code> browser extension. It shows react components.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#react-traditional-style-without-nodejs","title":"React Traditional Style - Without Node.js","text":"<p>Scripts to include, Both React and ReactDOM are available over a CDN.</p> <pre><code>&lt;script crossorigin src=\"https://unpkg.com/react@18/umd/react.development.js\"&gt;&lt;/script&gt;\n&lt;script crossorigin src=\"https://unpkg.com/react-dom@18/umd/react-dom.development.js\"&gt;&lt;/script&gt;\n</code></pre> <p>Add this to page and you can use react.</p> <p><code>ReactDOM</code> class is available, call its <code>render()</code> function to render elements in DOM.</p> <p><code>React</code> class is available, call its <code>createElement()</code> function to create new DOM elements.</p> <pre><code>&lt;body&gt;\n&lt;div id=\"root\"&gt;&lt;/div&gt;\n&lt;script type=\"text/javascript\"&gt;\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(\nReact.createElement(\"h1\",null,\"Hi\"), // adds as heading\n// \"&lt;p&gt;hi&lt;/p&gt;\", // does not add as p, adds as string\ndocument.getElementById(\"root\")\n);\n&lt;/script&gt;\n&lt;/body&gt;\n</code></pre> <p>We can write HTML in JS to add it to DOM. HTML is written as string in, eg <code>\"&lt;h1&gt;\"</code> tag. However to directly write HTML in JS not as string, JS is extended and JSX is introduced.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#jsx-and-babel","title":"JSX and Babel","text":"<p>JSX is not HTML, JSX is JavaScript XML, an extension of JavaScript that allows writing HTML in JS. To execute it, we need a compiler that can convert JSX to JS and HTML.</p> <p>JSX and JS go one in another and can be nested using {}s. When you start JS put in {} , then can again write HTML and again nest JS in {}.</p> <p>Babel is a JS compiler. You can insert plane HTML without quotes. Also add variable names with JS code within curly braces <code>&lt;p&gt;Hi {name.toUpperCase()}&lt;/p&gt;</code>. Eg:</p> <p>Babel converts JSX <code>&lt;p&gt;Hi&lt;/p&gt;</code> to JS <code>React.createElement(\"p\", null, \"Hi\");</code> on the fly.</p> <pre><code>...\n  &lt;script src=\"https://unpkg.com/@babel/standalone/babel.min.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div id=\"root\"&gt;&lt;/div&gt;\n&lt;script type=\"text/babel\"&gt; //type is changed from javascript to babel\nlet msg = \"Tom\";\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(\n&lt;p&gt;Hi {msg}&lt;/p&gt;, // JSX, text without quotes. Renders as react element using babel}\n);\n&lt;/script&gt;\n&lt;/body&gt;\n</code></pre> <p>Babel can be said to be like Jinja in Flask and Blade in Laravel but not same.</p> <p>Conceptually - Using React, HTML can be written in JS, which renders it and adds it to a HTML DOM element, eg <code>root</code>. Now that it is in JS, it can be programmed and be data driven and dynamic. To manage the HTML code we split the UI into independent and reusable <code>Components</code>.</p> <p>This was all in file and now we will use node and create react app. file use is okay for prototype but prod should use app.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#react-with-nodejs","title":"React with Node.js","text":"<p>Node.js creates structure for react project with Babel, file bundling etc.</p> <p><code>npx create-react-app app_name</code> create a folder with all required libraries like react, react dom, babel, react scripts etc.</p> <p>NPX, a package in node, that lets you run and execute packages without having to install them locally or globally.</p> <p><code>cd app_name</code> then <code>npm start</code> to run a web server using node and serve this app on <code>http://localhost:3000</code> as a dev build.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#understanding-the-folder-structure-and-files","title":"Understanding the folder structure and files","text":"<p><code>package.json</code> - All dependencies can be seen in this file.</p> <p><code>node_modules</code> dir has all dendencies installed by Node.js.</p> <p><code>public</code> dir is used to serve app. It has index.html and other static files.</p> <p><code>build</code> dir will be added later when you create a prod build.</p> <p><code>manifest.json</code> - In <code>public</code> dir this file provides information about the app in JSON format. It is used by mobile and desktop when we install this app. We can update <code>short_name</code> and <code>name</code> here.</p> <p><code>src</code> dir has app code.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#running-existing-project","title":"Running existing project","text":"<p>To run an existing react project, that you have downloaded, you need to install all the dependencies and that can be done by <code>npm install</code>. It uses <code>package.json</code> to install all dependencies.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#adding-dependencies","title":"Adding Dependencies","text":"<pre><code>npm install bootstrap@5.2.0 react-bootstrap react-router-dom serve\n</code></pre> <p><code>serve</code> is a static file web server that can be used to run the production version of the React application.</p> <p>Notice here, that it is not only JS packages but also CSS frameworks like <code>bootstrap</code>. <code>import 'bootstrap/dist/css/bootstrap.min.css';</code> to load only CSS file. <code>import Container from 'react-bootstrap/Container'</code> loads the library.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#the-indexjs-file","title":"The index.js File","text":"<p><code>index.js</code> is entry point to render app to DOM. <code>public/index.html</code> has <code>&lt;div id=\"root\"&gt;&lt;/div&gt;</code> that is used in <code>index.js</code>. <code>render()</code> function renders the app and takes JSX tree, as argument, that structures the entire app.</p> <p>Note: Any code that is non-JavaScript standard like import of file and JSX are converted to standart-JavaScript then they are served on browser. <code>import varName from 'filename.ext'</code> can be used to pull any file say image as var. Eg, in <code>app.js</code>, <code>import \"./styles.css\";</code> is non- javascript kind of code. This is later compiled and transformed to make it standard JS code and execute on browser. JSX uses CamelCase wile HTML lower.</p> <p>Each component is in its own file, eg, <code>app.js</code>, <code>header.js</code>.</p> <p><code>export default App</code> means export this component (function) as default App component.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#react-components","title":"React Components","text":"<p>React Component is a building block of app, one of the UI component (same as a template in Flask). Eg, <code>Header</code>, <code>Sidebar</code>, <code>Content</code> and <code>Footer</code>. This lets split whole page in to independent, dynamic, resuable components. Hence, eaisier to manage and control them using JS.</p> <p>Conceptually, components are like JavaScript functions. They accept arbitrary inputs (called \u201cprops\u201d) and return React elements (JSX) describing what should appear on the screen. Eg,</p> <pre><code>function Header(props) {\nreturn &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;;\n}\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(&lt;Header name=\"Sara\" /&gt;;\n</code></pre> <p>So previously we could only render standard tags like <code>&lt;div&gt;</code>, <code>&lt;p&gt;</code> etc. but now can render custom tags like <code>&lt;Header /&gt;</code>, also pass properties (props) to it. Remember, every <code>ComponentFunction</code> follows CamelCase. It should return only one JSX tag like <code>&lt;div&gt;</code>, <code>&lt;p&gt;</code> or <code>&lt;Header /&gt;</code>. To return muliple elements, wrap them in div, and if you want to avoid extra divs added to dom, use <code>&lt;React.Fragment&gt;blah blah&lt;/React.Fragment&gt;</code> or shorthand <code>&lt;&gt; ...</code>.</p> <p>Props make components dynamic. You can use logic in any component, like hiding sidebar when loggin in. Props is basically an <code>object</code> (key-value), its properties can passed in JSX in same way as we pass props in HTML <code>&lt;App name=\"Tom\" yob={1990}&gt;</code>, they are passed where we render a component. It can be recieved as <code>function App(props)</code> or as using ES6 as <code>function App ({name, yob})</code>.</p> <p>To loop lists of elements use <code>map()</code> of <code>Array</code> class, and include a <code>key</code> attribute with a unique value per element. React requires it to efficiently rerender only part of list. It can be inline function, eg, <code>arr.map( (a) =&gt; (a.length) )</code>.</p> <p>Add conditional rendering expressions with the <code>&amp;&amp;</code> (if-then) and <code>?:</code> (if-then-else) operators. Expression on right of &amp;&amp; is executed when left is true. Eg, <code>data.length === 0 &amp;&amp; &lt;p&gt;There is nothing to show here.&lt;/p&gt;</code></p> <pre><code>function Posts() {\nconst posts = [\n{id: 1, text: 'Hello, world!'},\n{id: 2, text: 'The Next Post'},\n];\nreturn (\n&lt;&gt;\n{posts.length === 0 ?\n&lt;p&gt;There are no blog posts.&lt;/p&gt;\n:\nposts.map(p =&gt; {\nreturn (\n&lt;p key={post.id}&gt;{post.text}&lt;/p&gt;\n);\n})\n}\n&lt;/&gt;\n);\n}\n</code></pre> <p>Good strategy is to build multiple components with each having one purpose. Eg, <code>Header</code> <code>Sidebar</code>. Add <code>ContentArea</code> that can be swapped based on naigation.</p> <p>Components can be nested to build hierarchy. Eg,</p> <pre><code>&lt;Body name=\"Tom\"&gt;\n&lt;Posts /&gt;\n&lt;/Body&gt;\n</code></pre> <p>If the component is called with children, like above, then a <code>children</code> key is included in <code>props</code> object, which is equal to child components, here <code>&lt;Posts /&gt;</code>.</p> <pre><code>function Body({name, children}) {\nreturn (...);\n}\n</code></pre>"},{"location":"JavaScript/2022-07-14-react-js-notes/#class-implementation","title":"Class Implementation","text":"<p>Component can also be a ES6 Class, that <code>extends React.Component</code> and implements <code>render() {}</code>.</p> <pre><code>class Post extends React.Component {\nrender() {\nreturn &lt;h2&gt;Post Title!&lt;/h2&gt;;\n}\n}\n</code></pre>"},{"location":"JavaScript/2022-07-14-react-js-notes/#react-bootstrap","title":"React-Bootstrap","text":"<p>React-Bootstrap provides <code>Container</code> and <code>Stack</code> components to design layout of website.</p> <pre><code>  return (\n&lt;Container fluid className=\"App\"&gt; // className is HTML class\n...  // &lt;-- no changes to JSX content\n&lt;/Container&gt;\n);\n</code></pre>"},{"location":"JavaScript/2022-07-14-react-js-notes/#hooks","title":"Hooks","text":"<p>useState Hook</p> <p>useState is a function in React class that can be used to manage state of App.</p> <p><code>useState(arg)</code> arg can be any data type.</p> <p>it returns array having <code>[stateValue, setStateFunc] = setState(\"a Value\")</code> we can use them in app to manage states. We can have multiple states of app, so make multiple instance of this function.</p> <p>useEffect Hook</p> <p>These are like eventListners that can listen to some variable and if there is a change in its state they can do something.</p> <p><code>useEffect(funcDoSomething, [varToListenTo]);</code> when arg2 changes, arg1 is executed. arg2 = [] then only run once, arg2 can have multiple vars to listen to.</p> <p>useReducer Hook can be used to manage state and effect at once.</p> <p>It takes two arguments, arg1 is function to execute on state change, and arg2 is initial state value.</p> <p>It returns, 1 state value, 2 function to change state. Eg, state is counterValue</p> <pre><code>const [counterValue, setCounterValue] = useReducer(\n(a) =&gt; a++, //\n0\n);\n</code></pre> <p>ToDo: Need some more understanding here on, how to use reducer to do different action based on param passed?</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#handling-forms-14-07-2022","title":"Handling Forms - 14-07-2022","text":"<p>Uncontrolled Component: <code>useRef</code> we can create instance of this function and attach that to form inputs.</p> <pre><code>const empName = useRef();\n&lt;input ref={empName} ...&gt;\n// on Submit\na = empName.current.value; // GET\nempName.current.value = 'A'; // SET\n</code></pre> <p>We can get and set the values using <code>empName.current.value</code></p> <p>Controlled Component: <code>useState</code> and bind to input tag using <code>onChange</code>:</p> <pre><code>const [empName, setEmpName] = useState(\"\");\n&lt;input value={empName} onChange={ (e) =&gt; setEmpName(e.target.value) } ...&gt;\n// on Submit\na = empName; // GET\nsetEmpName(\"A\"); // SET\n</code></pre> <p>Custom Hooks</p> <p>We can create our own custom hooks that can be reused based on our requirements. They have to start with <code>use...</code>. They instantiate another hook within them. Eg, we can make a custom hook to handle form events like setting default value, getting current value, onChange events, validation etc.</p> <p>Others: formik.org, react-hook-form.com, usehooks.com</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#routes","title":"Routes","text":"<p>React makes SPA, only one page is served from server. So <code>react-router-dom</code> is library that can be used to manage routes on browser.</p> <p>In <code>index.js</code> or<code>App.js</code> you can create <code>&lt;Route&gt;</code> in <code>&lt;Routes&gt;</code> having <code>path</code> and <code>element</code>.</p> <pre><code>import { App, About, Contact } from \"./App\";\nimport { BrowserRouter, Routes, Route, Navigate } from react-router-dom\";\nReactDOM.render(\n  &lt;BrowserRouter&gt;\n    &lt;Routes&gt;\n      &lt;Route path=\"/\" element={&lt;App /&gt;} /&gt;\n      &lt;Route path=\"/about\" element={&lt;About /&gt;} /&gt;\n      &lt;Route path=\"/contact\" element={&lt;Contact /&gt;} /&gt;\n      &lt;Route path=\"*\" element={&lt;Navigate to=\"/\" /&gt;} /&gt;\n    &lt;/Routes&gt;\n  &lt;/BrowserRouter&gt;,\n  document.getElementById(\"root\")\n);\n</code></pre> <p>You can link routes to create hierarcy. Use <code>{ Link }</code> a XHR, to add link to component in DOM. It is like <code>url_for</code> in Flask.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#routes-with-bootstrap","title":"Routes with Bootstrap","text":"<p>Bootstrap has <code>Nav.Link</code> component that creates nav-item, it has <code>as</code> and <code>to</code> props to work with React-Route's <code>NavLink</code>.</p> <pre><code>import Navbar from \"react-bootstrap/Navbar\";\nimport Nav from \"react-bootstrap/Nav\";\nimport { NavLink } from 'react-router-dom';\nexport default function Sidebar() {\nreturn (\n&lt;Navbar sticky=\"top\" className=\"flex-column Sidebar\"&gt;\n&lt;Nav.Item&gt;\n&lt;Nav.Link as={NavLink} to=\"/\"&gt;Feed&lt;/Nav.Link&gt;\n&lt;/Nav.Item&gt;\n&lt;Nav.Item&gt;\n&lt;Nav.Link as={NavLink} to=\"/explore\"&gt;Explore&lt;/Nav.Link&gt;\n&lt;/Nav.Item&gt;\n&lt;/Navbar&gt;\n);\n}\n</code></pre>"},{"location":"JavaScript/2022-07-14-react-js-notes/#parameters-and-routes","title":"Parameters and Routes","text":"<p>To define a route with a dynamic section, the path attribute of the Route component uses a special syntax with a colon prefix:</p> <pre><code>&lt;Route path=\"/user/:username\" element={&lt;UserPage /&gt;} /&gt;\n</code></pre> <p>The component referenced by the element attribute or any of its children can use the <code>useParams()</code> hook function to access the dynamic parameters of the current URL as an object.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#development-and-structuring","title":"Development and Structuring","text":"<p>Following steps can help you build a React App:</p> <ol> <li>Plan the UI that you want to build as a wireframe or sketch.</li> <li>Break the UI into top level Components like header, sidebar, content and footer.</li> <li>Build these components and place in <code>src/components</code></li> <li>Think of logical pages that use the above components but have different low level componenets. Eg, ProfilePage and FeedPage, both have header, sidebar and content BUT the content will have different sub-components like profile or posts.</li> <li>Build these components and place in <code>src/pages</code></li> <li>Build routes for these Pages and update the header.</li> </ol> <p>State - use to set and get data, like responses, session variables and state (loading, error, done)</p> <p>Effect - use to do functionality like - fetch, or any function.</p> <p>If a function returns JSX then it is a react component.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#testing","title":"Testing","text":"<p><code>Jest.js</code> is used to write test.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#deployment","title":"Deployment","text":"<p>App can be deployed to Netlify.com</p> <p>Do <code>npm run build</code> and then <code>build</code> folder has prod ready code to deploy.</p>"},{"location":"JavaScript/2022-07-14-react-js-notes/#resources","title":"Resources","text":"<p>Links</p> <ul> <li>React Tutorial Miguel Grinberg - https://blog.miguelgrinberg.com/post/the-react-mega-tutorial-chapter-2-hello-react</li> <li>LinkedIn Learning - https://www.linkedin.com/learning/react-js-essential-training</li> <li>React Docs - https://reactjs.org/docs/hello-world.html</li> </ul> <p>Next</p> <ul> <li>ReactNative</li> <li>GraphQL</li> </ul>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/","title":"Mac and Linux Ways","text":"<p>Here are some basic understandings and commands that can be used on UNIX terminal and eventually on Mac.</p>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#mac-specific","title":"Mac Specific","text":""},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#homebrew","title":"Homebrew","text":"<ul> <li>package manager for mac,</li> <li>cask are usually GUIs apps like Sublime.</li> <li>formulae are packages, CLIs, like node.</li> </ul> <p>Brew Global Commands:</p> <ul> <li><code>brew update</code> - Update brew and cask list, not packages</li> <li><code>brew upgrade</code> -  Upgrade all packages</li> <li><code>brew list</code> - List installed packages and casks</li> <li><code>brew outdated</code> - List outdated packages?</li> <li><code>brew doctor</code> - Diagnose brew issues</li> <li><code>brew cleanup</code> - cleans all packages</li> <li><code>brew services list</code>- lists all services installed</li> </ul> <p>Brew Commands:</p> <ul> <li><code>brew install git</code> -  Install a package</li> <li><code>brew uninstall git</code> -  Remove/Uninstall a package</li> <li><code>brew upgrade git</code> -  Upgrade a package</li> <li><code>brew switch git 2.5.0</code> -  Change versions</li> <li><code>brew list --versions git</code>  See what versions you have</li> <li><code>brew cleanup git</code> Remove old versions</li> </ul> <p>Brew Cask (GUI) commands:</p> <ul> <li><code>brew install --cask firefox</code> Install the Firefox browser</li> <li><code>brew list --cask</code>  List installed applications</li> </ul>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#others","title":"Others","text":"<p><code>diskutil list</code> - lists all disks</p> <p>Format a disk from Mac terminal:</p> <ul> <li><code>diskutil eraseDisk FILE_SYSTEM DISK_NAME DISK_IDENTIFIER</code></li> <li> <p>eg: <code>diskutil eraseDisk FAT32 VY_Disk /dev/disk2</code> or use ExFAT</p> </li> <li> <p><code>/Volumes/PenDrive</code> location of usb mounts</p> </li> </ul> <p>Copy to clipboard</p> <ul> <li><code>$ pbcopy &lt; my_filename.ext</code> it copies the content of file to clipboard.</li> <li>It is helpful to quickly copy RSA key to clipboard which you need to paste on, may be, GitHub.</li> </ul> <p>Androids</p> <ul> <li><code>~/.android</code> - google utility folder</li> <li><code>avdmanager list avd</code> - lists all android virtual devices installed.</li> </ul> <p>Uninstalling: Usually check for following dirs and remove:</p> <ul> <li><code>sudo rm /usr/local/mypkg</code></li> <li><code>sudo rm -rf /usr/local/var/mypkg</code></li> <li><code>sudo rm -rf /usr/local/mypkg*</code></li> <li><code>sudo rm -rf /Library/StartupItems/mypkg*</code></li> <li><code>sudo rm -rf /Library/PreferencePanes/MyPkg*</code></li> <li><code>rm -rf ~/Library/PreferencePanes/MyPkg*</code></li> <li><code>sudo rm -rf /Library/Receipts/mypkg*</code></li> <li><code>sudo rm -rf /Library/Receipts/MyPkg*</code></li> </ul>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#ubuntu-specific","title":"Ubuntu Specific","text":"<p>Ubuntu is debain based os, others are Mint, Elementary and PoP OS.</p> <p>Debain uses dpkg packaging system, for install/uninstall software.</p> <p>Packages are maintained in repositories, Main, Universe, Restricted and Multiverse.</p> <ul> <li><code>sudo add-apt-repository universe</code> to enable a repo.</li> </ul> <p>PPA - Personal Package Archive - allows application developers to create their own repositories to distribute.</p> <ul> <li><code>sudo add-apt-repository ppa:mkusb/ppa</code> add a ppa repo</li> </ul> <p>APT - Advanced Package Tool is CLT UI that works with core libraries to handle the installation and removal of software on Debian, Ubuntu. IT manages dependencies, config files and upgrades/downgrades.</p> <p><code>apt-get</code> performs installation, search, updates to pkg available on system. works with <code>sudo</code> only.</p> <p><code>sudo apt-get update</code> - updates local copy of packages database. The result has :</p> <ul> <li>Hit: no change in pkg</li> <li>Get: update available, downloads details but not the update</li> <li>Ign: ignores.</li> </ul> <p><code>sudo apt-get upgrade</code> updates core system and apps installed. For one package update <code>sudo apt-get upgrade [package_name]</code>.</p> <p><code>sudo apt-get install [pkg1] [pkg2]</code> if you know the name of apps.</p> <p><code>sudo apt-get remove [package_name]</code> to uninstall. but kepps config files.</p> <p><code>sudo apt-get autoremove</code> cleans up unwanted pkg.</p> <p><code>apt list --installed</code> see all that's installed.</p> <p>apt=most common used command options from apt-get and apt-cache. It is high level wrapper on old apt-get. Use apt for better UI and info like summary and progress bar.</p> <p>Install a <code>.deb</code> file, eg, Chrome:</p> <ul> <li><code>sudo dpkg -i /path/to/foo.deb</code> installs.</li> <li><code>sudo apt-get install -f</code> fix-broken dependencies.</li> </ul> <p>Or simply use below to install with dependencies</p> <ul> <li><code>sudo apt install ./name.deb</code></li> </ul> <p><code>dpkg</code> does not handle dependency, while <code>apt</code> does. apt under the hood uses dpkg.</p>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#first-steps","title":"First Steps","text":"<p>Do following in a new install</p> <ul> <li>update and upgrade <code>sudo apt update &amp;&amp; sudo apt upgrade</code></li> <li>codecs flash and fonts <code>sudo apt install ubuntu-restricted-extras</code></li> <li>vlc - <code>sudo apt install vlc</code></li> <li>chrome <code>wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb</code> then <code>sudo dpkg -i google-chrome-stable_current_amd64.deb</code></li> <li>more - https://itsfoss.com/things-to-do-after-installing-ubuntu-20-04/</li> </ul> <p>Clean up</p> <ul> <li>Delete apps - <code>sudo apt purge thunderbird</code></li> <li>remove sw dependencies <code>sudo apt autoremove</code></li> <li>remove partially installed packages <code>sudo apt autoclean</code></li> <li>remove cache <code>sudo apt clean</code></li> </ul> <p>Speed up</p> <ul> <li>disable animations <code>gsettings set org.gnome.desktop.interface enable-animations false</code></li> </ul> <p>Dev Softwares</p> <ul> <li>sublime <code>sudo snap install sublime-text --classic</code></li> <li>vs code <code>sudo snap install code --classic</code></li> </ul> <p>Install Git and Gh:</p> <ul> <li>install git and essentials <code>sudo apt-get install build-essential procps curl file git</code></li> <li>set up git - <code>git config --global user.name \"YOUR NAME\"</code></li> <li>set up git - <code>git config --global user.email \"YOUR EMAIL ADDRESS\"</code></li> </ul> <p>Install Jupyter notebook, see Python Notes https://iyadavvaibhav.github.io/python-notes/</p> <p>Install nsepa and Citrix Workspace:</p> <ul> <li><code>wget http://ftp.br.debian.org/debian/pool/main/n/network-manager/libnm-util2_1.6.2-3+deb9u2_amd64.deb http://ftp.br.debian.org/debian/pool/main/n/network-manager/libnm-glib4_1.6.2-3+deb9u2_amd64.deb</code> download debs</li> <li><code>sudo apt install ./libnm-util2_1.6.2-3+deb9u2_amd64.deb ./libnm-glib4_1.6.2-3+deb9u2_amd64.deb</code> install the debs downloaded</li> <li>Download .deb file from Citrix, amd 64</li> <li><code>cd ~/Downloads</code></li> <li><code>sudo dpkg -i icaclient_21.4.0.11_amd64.deb</code></li> <li><code>sudo apt-get install -f</code></li> </ul>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#others_1","title":"Others","text":"<ul> <li><code>lsblk</code> lists disk</li> </ul> <p>Check graphics card installed</p> <ul> <li>check hardware using lshw (list hardware) is a small Linux/Unix tool which is used to generate the detailed information of the system's hardware configuration from various files in the /proc directory. E.g. to see graphics driver - <code>sudo lshw -c video</code></li> <li>check loaded modules using lsmod - it shows which loadable kernel modules are currently loaded. <code>lsmod | grep radeon</code></li> <li>The glxinfo program shows information about the OpenGL and GLX implementations running on a given X display. <code>sudo apt install mesa-utils</code> and <code>glxinfo -b</code></li> <li>Check boot message for graphics card in use <code>dmesg | grep -i radeon</code></li> </ul> <p>Windows on Linux</p> <ul> <li><code>sudo apt-get install playonlinux</code></li> <li>installs wine too, 32 bit</li> <li>to install a program, create a virtual machine and install it.</li> <li>to install nfsmw</li> <li>create a machine, 32bit</li> <li>add drivers dcdx9 and vcrun6</li> <li>more on install https://www.youtube.com/watch?v=lUqU_uf-o9E</li> <li>more on download https://www.youtube.com/watch?v=no8-fB4MX00&amp;t=1s</li> </ul>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#users-and-groups","title":"Users and Groups","text":"<p>List all users</p> <ul> <li><code>getent passwd</code></li> <li><code>compgen -u</code></li> <li><code>cut -d: -f1 /etc/passwd</code></li> </ul> <p>List all groups</p> <ul> <li><code>compgen -g</code></li> </ul> <p>Add user to group - <code>sudo adduser username group</code></p>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#os-setup-and-virtualization","title":"OS Setup and Virtualization","text":"<p>USB Installation</p> <ul> <li>works like a charm,</li> <li>make a bootable live usb/cd - https://linuxhint.com/create_bootable_linux_usb_flash_drive/</li> <li>boot from it and install to another USB - https://www.fosslinux.com/10212/how-to-install-a-complete-ubuntu-on-a-usb-flash-drive.htm</li> <li>space and drive speed is a issue.</li> <li>Clean grub of mac - https://apple.stackexchange.com/questions/337189/unwanted-grub-on-macos-high-sierra</li> <li>Tripe boot mac - https://www.youtube.com/watch?v=B0EuYHFeLAA</li> <li>First steps on Ubuntu - https://www.youtube.com/watch?v=GrI5c9PXS5k</li> </ul> <p>Virtual box add on:</p> <ul> <li><code>sudo apt update</code></li> <li><code>sudo apt install virtualbox-guest-dkms virtualbox-guest-x11 virtualbox-guest-utils</code></li> </ul>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#linux-ways","title":"Linux Ways","text":"<p>ENVs:</p> <ul> <li><code>source activate [path to env]</code> activates env</li> <li><code>source deactivate</code> deactivates enn</li> </ul> <p>Emac Basic</p> <ul> <li>Press <code>ctrl + c + x</code> to save and exit a file.</li> </ul> <p>Other</p> <ul> <li>everything global is installed in <code>/usr/local/bin/</code></li> <li> <p>use PostMan for http requests to REST routes</p> </li> <li> <p><code>rmdir</code> removed empty dir</p> </li> <li><code>rm -rf</code> removes non/empty dir and files forcefully</li> <li><code>rm</code> removes files not directories.</li> </ul>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#youtube-dl","title":"youtube-dl","text":"<ul> <li><code>youtube-dl --extract-audio --audio-format mp3 -o \"%(title)s.%(ext)s\" http://www.youtube.com/watch?v=fdf4542t5g</code> -o is --output of filename.</li> </ul>"},{"location":"Tech%20Notes/2018-06-28-mac-linux-terminal/#mobile-linux","title":"Mobile Linux","text":"<p>Everything related to iOS and Android Linux.</p> <p>Install Termux App on Android.</p> <pre><code>termux-setup-storage\ntermux-change-repo\npkg update\n\npkg install git\ngit --version\n\npkg install python\n\npkg install openssl\n\npip install --upgrade youtube-dl\n\nyoutube-dl -i PLyAyDdlMr3GOHFBt0IzgvED-n3uhwyrhd\n</code></pre> <p>Install on <code>a-Shell</code> iOS App</p> <pre><code>cd ~/code\nwget -qO- http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86/apk-tools-static-2.10.6-r0.apk | tar -xz sbin/apk.static &amp;&amp; ./sbin/apk.static add apk-tools &amp;&amp; rm sbin/apk.static\napk add python3\napk add py3-pip\npip install youtube-dl\n</code></pre>"},{"location":"Tech%20Notes/2018-07-02-git-notes/","title":"Git and GitHub","text":"<p>Git is version control software to track changes in source code. GitHub is cloud storage for Gits.</p> <ul> <li>Do not remove this line (it will not be displayed) {:toc}</li> </ul>"},{"location":"Tech%20Notes/2018-07-02-git-notes/#what-is-git","title":"What is Git","text":"<ul> <li>Git is a distributed version-control system (VCS) for tracking changes in source code during software development.</li> <li>We check-in and check-out files to git and it keeps a track of the history.</li> <li>On mac it was pre installed as part of Xcode Command Line Tools.</li> <li><code>git --version</code> to check the version of git installed.</li> </ul>"},{"location":"Tech%20Notes/2018-07-02-git-notes/#how-to-clone-a-repository-from-githubcom","title":"How to clone a repository from GitHub.com","text":"<ul> <li><code>git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY</code></li> <li>eg, <code>git clone https://github.com/miguelgrinberg/microblog.git</code></li> <li>This will bring all the files from remote to local directory with git repository on local folder.</li> <li>Now if you have permission to commit to this repo then you can authenticate to push, else change the remote to another repo that you can push to.</li> </ul>"},{"location":"Tech%20Notes/2018-07-02-git-notes/#how-to-set-up-git-on-a-local-folder","title":"How to set up Git on a Local Folder","text":"<p>Setup Git</p> <ul> <li>On any folder, do this once</li> <li>eg, <code>mkdir myProject</code> then <code>cd myProject</code></li> <li><code>git init</code> this will create a local git repository on your local drive. Now if you need to add this to a remote git repository, for example, a repository on github.com or bitbucket then you need to add remote to this folder.</li> </ul> <p>Now once you have written your code, you can add and commit new code to local git:</p> <p>Add and Commit code</p> <ul> <li><code>git add .</code> adds all files to git. To add one file, pass filename.</li> <li><code>git diff</code> shows changes made to files.</li> <li><code>git commit -m \"Message\"</code> commits to git with message.</li> <li>optional, <code>cat .gitignore</code> add files that you want git to ignore</li> </ul> <p>Add Remote</p> <ul> <li>Create a new repository on GitHub.com</li> <li>on your local folder, <code>git remote add  [name] [url]</code> will add remote. Here, <code>name</code> can be origin and <code>url</code> is https/ssh url of git repo created online on GitHub.com. Use SSH if you have SSH authentication setup.</li> <li>Once remote is added to your local git then you can push or pull the files based on the commands below.</li> </ul> <p>Syncing local and remote</p> <ul> <li><code>git pull</code> pulls updates from remote to local</li> <li><code>git push</code> pushes the committed changes from local to remote. We can also specify remote name and branch here. eg:</li> <li><code>git push -u origin master</code>.</li> </ul>"},{"location":"Tech%20Notes/2018-07-02-git-notes/#ssh-authentication-to-push-to-remote","title":"SSH Authentication to push to remote","text":"<p>You can connect to GitHub using the Secure Shell Protocol (SSH), which provides a secure channel over an unsecured network. It can help connect one machine to another using keys and thus avoiding to provide username and password/token on each request. <code>id_rsa.pub</code> is default public key.</p> <ul> <li>if <code>~/.ssh/id_rsa.pub</code> exists do <code>cat ~/.ssh/id_rsa.pub</code> else generate SSH Key <code>ssh-keygen</code>, passphrase is optional.</li> <li>copy the content, Open GitHub, click your profile icon, settings, SSH and GPC Keys, Click on the new ssh key button.</li> <li>enter any title and key that you copied</li> <li>more - https://docs.github.com/en/authentication/connecting-to-github-with-ssh</li> </ul> <p>Checking</p> <ul> <li>Check using <code>ssh -T git@github.com</code></li> <li>Output should say <code>Hi &lt;user_name&gt;! You've successfully authenticated, but GitHub does not provide shell access.</code></li> </ul> <p>Fixing SSH issue</p> <ul> <li>error - <code>ssh: connect to host github.com port 22: Connection refused</code></li> <li>change ssh config to use new url and port, Override SSH settings <code>gedit ~/.ssh/config</code> and add</li> </ul> <pre><code># Add section below to it\nHost github.com\n  Hostname ssh.github.com\n  Port 443\n</code></pre> <ul> <li>save and try again.</li> <li>Change your git remote to use SSH URL instead of HTTPS <code>git remote set-url origin git@github.com:YOUR-USERNAME/REPO-NAME.git</code></li> </ul>"},{"location":"Tech%20Notes/2018-07-02-git-notes/#get-and-set-remotes","title":"Get and Set Remotes","text":"<ul> <li><code>git remote -v</code> do on a folder to check remotes added.</li> <li><code>git remote get-url --all REMOTE-NAME</code> to see URL of remote.</li> <li><code>git remote set-url origin https://github.com/YOUR-USERNAME/YOUR-REPO.git</code> to update remote on a folder.</li> </ul>"},{"location":"Tech%20Notes/2018-07-02-git-notes/#handling-conflicts","title":"Handling Conflicts","text":"<p>If you push to git from two different repositories then there may be conflict. eg, you push from mac repo and a cloud repo or ubuntu repo. To handle conflict:</p> <ul> <li>Open conflicted file in editor and look for <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code> .</li> <li>You'll see the changes from the HEAD or base branch (github usually) after the line <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code></li> <li><code>========</code>, it divides your changes from the other branch as <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;YOUR_BRANCH_NAME</code></li> <li>You can decide if you want keep your branch changes or not. If you want to keep the changes what you did, delete the conflict marker they are, <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;</code> and then do a merge.</li> <li>Once done, <code>add commit push</code> :)</li> </ul>"},{"location":"Tech%20Notes/2018-07-02-git-notes/#version-controlling-in-git","title":"Version controlling in GIT","text":"<p>You can see previous versions of file in your git repository.</p> <ul> <li>to see the checkins done</li> </ul> <pre><code>&gt; git reflog\n044cf0e (HEAD -&gt; master) HEAD@{0}: commit: updates\naae1995 HEAD@{1}: commit (initial): first commit\n</code></pre> <ul> <li><code>git show HEAD@{1}:path/to/file.ext</code> show file on terminal</li> <li>press down arrow to navigate and <code>q</code> to quit</li> </ul> <p>or</p> <ul> <li><code>git show -1 filename</code> - shows difference with last revision</li> <li>use -1 or -2 or -3 and so on for going into history.</li> </ul>"},{"location":"Tech%20Notes/2018-07-02-git-notes/#adding-rsa-for-password-free-sync-on-mac","title":"Adding RSA for password free sync on Mac","text":"<p>GitHub is excellent for code repositories online to share work, collaborate or keep a backup.</p> <p>I have followed an excellent post by Karl Broman on the same. To summaries the flow:</p> <ul> <li>Install git on local drive.</li> <li>Setup RSA for SSL: RSA is used for making safe authentications via SSL.</li> <li>Setup GitHub Account: You will get an online space to upload your code with version controlling.</li> </ul> <p>Related Posts:</p> <ul> <li>Further to this post, Set up Github Pages and Jekyll to get started with blogging and personal site.</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/","title":"Flask - WebFramework in Python","text":"<p>Flask is a microframework in Python. It is used to create a webapp. It can start a python web server. It can handle HTTP requests. It can also be used to make a webapp API.</p> <ul> <li>Do not remove this line (it will not be displayed) {:toc}</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#python-interpretors-and-environments","title":"Python Interpretors and Environments","text":""},{"location":"Tech%20Notes/2019-03-05-flask-notes/#why-setup-a-virtual-environment","title":"Why setup a Virtual Environment","text":"<p>Why? - It gives different apps isolation and personal environment so that modules don't interfere and it is easy when we have to productionize the app.</p>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#what-is-python-virtual-environment","title":"What is Python Virtual Environment?","text":"<ul> <li>Basically a folder having python core binaries and use this new installation to install libraries that will be specific to this installation (or environment).</li> <li>It is an isolated environment</li> <li>When you activate a virtual environment, your PATH variable is changed. The Scripts directory of <code>venv_app</code> is put in front of everything else, effectively overriding all the system-wide Python software.</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#set-correct-python-interpretor","title":"Set correct Python Interpretor","text":"<ul> <li>there may be many python version and interpreter installed on your machine. eg, <code>/bin/python3</code> or <code>~/anaconda/bin/python</code></li> <li>identify the default, <code>which python</code></li> <li>find all installed binaries, <code>locate \"bin/python\"</code> - this lists all the binaries including venvs.</li> <li>to set anaconda python as default python, add following to <code>~/.bashrc</code></li> <li><code>export PATH=\"/home/username/anaconda3/bin:$PATH\"</code></li> <li>then restart terminal or do <code>source ~/.bashrc</code></li> <li>check with <code>which python</code>, should be using anaconda.</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#how-to-use-virtual-environments-in-python3","title":"How to use Virtual Environments in Python3?","text":"<ul> <li>Create</li> <li>python3 or python depends on your installation.</li> <li><code>python3 -m venv venv_app</code> - creates folder venv_app</li> <li><code>source venv_app/bin/activate</code> - activates this environment, for use</li> <li>check using <code>python3 -V</code> or <code>which python3</code></li> <li>now do <code>python -m pip install &lt;package-name&gt;</code> - this installs the package only in this environment.</li> <li> <p>add <code>#!venv_app/bin/python3</code> on top of main.py file to make it use this python.</p> </li> <li> <p>Deactivate</p> </li> <li> <p><code>deactivate</code> to deactivate the current env.</p> </li> <li> <p>View &amp; Share</p> </li> <li><code>python -m pip list</code> to see installed libs.</li> <li> <p><code>python3 -m pip freeze</code> to share libs.</p> <ul> <li><code>python3 -m pip freeze &gt; requiremeents.txt</code> to make a file.</li> <li><code>python3 -m pip install -r requirements.txt</code> to install all libs from file</li> </ul> </li> <li> <p>Delete</p> </li> <li><code>rm -r venv_app</code> delete</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#flask-hello-world","title":"Flask Hello World","text":"<ul> <li>Create a python file <code>main.py</code>:</li> </ul> <pre><code>#!venv_app/bin/python3\nfrom flask import Flask\napp = Flask(__name__)\n@app.route('/')\ndef index():\nreturn \"Hello from Flask App!\"\nif __name__ == '__main__':\napp.run(debug=True)\n</code></pre> <ul> <li> <p>run using <code>python main.py</code></p> </li> <li> <p>access at <code>http://localhost:5000</code> in browser.</p> </li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#run-a-flask-app-ways","title":"Run a Flask App - Ways","text":"<ul> <li>Use Python</li> <li> <p>using <code>path_to_python3 path_to_main.py</code></p> </li> <li> <p>Make the file executable using</p> </li> <li><code>chmod a+x main.py</code> to make app executable.</li> <li> <p><code>./main.py</code> runs app on localhost in debug mode using <code>#!venv_app/bin/python3</code>.</p> </li> <li> <p>Using Flask CLI</p> </li> <li><code>export FLASK_APP=main.py</code> will make an variable that tells python which app to run.</li> <li><code>flask run</code> executes the app or if flask is not in path then do <code>python -m flask run</code></li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#flask-native-modules","title":"Flask native modules","text":"<ul> <li><code>Flask</code> - is a class of which instance <code>app</code> is created using <code>app = Flask(__name__)</code></li> <li><code>redirect</code> - takes URL to redirect to.</li> <li><code>url_for</code> - takes function name as str and gives its URL.</li> <li><code>redirect(url_for(\"profile\"))</code></li> <li><code>session</code> - can be used to store values, specific to current session, it is server side. Helps to pass values from one function to another.</li> <li><code>session[\"username\"] = username</code></li> <li>permanent sessions store session data for a timeperiod</li> <li><code>flash</code> - lets send extra messages to frontend</li> <li><code>flash(\"The message\", \"info\")</code> message and level.</li> <li><code>get_flashed_messages()</code> to get messages</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#flask-blueprints","title":"Flask Blueprints","text":"<ul> <li>Blueprint lets us divide app into mini apps. It is a collection of views, templates, static files that can be applied to an application. Blueprints are a great way to organize your application.</li> <li>you can have a sub-folder for mini app, having its own static and templtes folder, just app <code>__init__</code> to the sub-folder and import it to main app</li> <li>In a functional structure, each blueprint is just a collection of views. The templates are all kept together, as are the static files.</li> </ul> <pre><code>from flask import Blueprint\nsecond = Blueprint(\"second\", __name__)\n@second.route(\"/home\")\ndef home():\nreturn (\"from second\")\n</code></pre> <ul> <li>and in <code>app.py</code></li> </ul> <pre><code>from flask import Flask\nfrom second import second\napp = Flask(__name__)\napp.register_blueprint(second, url_prefix=\"\")\n@app.route(\"/\")\ndef home():\nreturn \"Hi\"\nif __name__ == \"__main__\":\napp.run(debug=True)\n</code></pre> <ul> <li>Link - http://exploreflask.com/en/latest/blueprints.html</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#flask-logging","title":"Flask Logging","text":"<pre><code>import logging\napp = Flask(__name__)\nlogging.basicConfig(level=logging.DEBUG)\napp.logger.info('some log')\n</code></pre>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#deployment-pythonanywhere","title":"Deployment - PythonAnywhere","text":"<ul> <li>WSGI configuration</li> <li>On your Web App Configuration page, open \"WSGI configuration file\", and ensure you add your project folder to code below.</li> </ul> <pre><code>import sys\n# add your project directory to the sys.path\nproject_home = u'/home/username/mysite'\nif project_home not in sys.path:\nsys.path = [project_home] + sys.path\n# import flask app but need to call it \"application\" for WSGI to work\nfrom flask_app import app as application  # noqa\n</code></pre>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#access-virtual-machine-localhost-flask-app-on-a-host-machine","title":"Access \"Virtual Machine localhost flask app\" on a host machine","text":"<ul> <li> <p>Suppose on an Ubuntu VM a flask app is running on localhost and you want to access it from you host machine that is Mac.</p> </li> <li> <p>Run flask app with <code>app.run(host='0.0.0.0', debug=True)</code></p> </li> <li>This tells your operating system to listen on all public IPs.</li> <li>then access <code>192.168.10.33:5000</code> from host machine.</li> </ul> <p>Now that our app is running we can add a database to this app. We will use FlaskSQLAlchemy package for this. Or Pandas.</p>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#flask-app-for-machine-learning-pandas","title":"Flask App for Machine Learning - Pandas","text":"<p>You can use Flask it with Pandas, Matplot and other ML libraries to make it easily usable for end users.</p> <ul> <li>Import all your libs in flask app that you have used in Jupyter NB.</li> <li>Add code and functions to read data and perform tasks.</li> <li>Flask routes are executed for each request, so keep data reads outsite to read them once.</li> <li><code>return render_template( 'search.html', data=df_result.to_html(classes='table table-striped table-hover')</code> - to_html makes html table that can be passed to html page.</li> <li><code>{{ data|safe }}</code> - safe makes it as markup and browser renders it.</li> </ul> <p>Reference:</p> <ul> <li>https://sarahleejane.github.io/learning/python/2015/08/09/simple-tables-in-webapps-using-flask-and-pandas-with-python.html</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#add-database-to-flask-app-using-flask-sqlalchemy","title":"Add DataBase to Flask app using Flask-SQLAlchemy","text":"<p>It is an extension of <code>SQLAlchemy</code> which is ORM for major databases.</p> <ul> <li>It is an designe for Flask that adds support for SQLAlchemy to your application.</li> <li>You can define table as a class, called model, with member variables as column names.</li> <li>Create an object to make new instance.</li> <li>Example</li> </ul> <pre><code>class User(db.Model):\n__tablename__ = 'users'\nid = db.Column(db.Integer, primary_key = True)\nusername = db.Column(db.String(50), unique=True)\nadmin = db.Column(db.Boolean)\ncreated_on = db.Column(db.DateTime(), default=datetime.now)\nupdated_at = db.Column(db.DateTime, default=datetime.utcnow, onupdate=datetime.now)\ndb.session.add(obj)     # insert new record to database\ndb.session.delete(obj)  # delete a record from database\ndb.session.commit()     # updates modifications in object if any\n# where clause\nuser = User.query.filter_by(username=data['username']).first() # or .all()\n# select * from users\nusers = User.query.all()\n</code></pre>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#creating-tables","title":"Creating Tables","text":"<p>Once you have created a db model in flask app, you can create db and tables using follwing steps:</p> <ul> <li><code>python</code></li> <li><code>from main import db</code> main is filename of flask app</li> <li><code>db.create_all()</code> creates all tables from model class</li> </ul> <p>Now you can check SQL for tables created. You can do:</p> <ul> <li><code>sqlite3 filename.db</code></li> <li><code>.tables</code></li> </ul> <p>Caution:</p> <ul> <li>Provide full path to db file, so that apache can locate it.</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#migrations-using-alembic-and-flask-migrate","title":"Migrations using Alembic and Flask-Migrate","text":"<p>Alembic is a database migration tool for SQLAlchemy. It generates Py script to keep the database schema update with the models defined. It help upgrade and roll back the schemas.</p> <ul> <li>We use library <code>Flask-Migrate</code> library that helps use alembic for flask apps using cli</li> <li><code>flask db migrate -m 'relations_added'</code> - creates migration script to update the tables. -m helps add caption to file.</li> <li><code>flask db upgrade</code> can be used to run the Py file generated and this will updated the database schema.</li> </ul> <p>Reference:</p> <ul> <li>https://www.digitalocean.com/community/tutorials/how-to-make-a-web-application-using-flask-in-python-3</li> <li>SQLite explorer - https://sqlitebrowser.org/</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#rest-api-and-restful-web-services-the-basics-guide","title":"REST API and RESTful Web Services - The Basics Guide","text":"<p>What is REST?</p> <ul> <li>Client and Server are separate</li> <li>Stateless: No information from a request is stored on client to be used in other requests, eg, no session can be started, if authentication is required, username and password need to be sent with every request.</li> </ul> <p>What is RESTful API:</p> <ul> <li>There is a resource, eg, tasks</li> <li>It has endpoints for CRUD operations</li> <li>HTTP methods, GET PUT POST DELETE, are used for these operations.</li> <li>Data is provided with these requests in no particular format but usually as:</li> <li>JSON blob in request body, or</li> <li>Query String arguments as portion of URL.</li> </ul> HTTP Method URI Action GET http://[hostname]/todo/api/v1.0/tasks Retrieve list of tasks GET http://[hostname]/todo/api/v1.0/tasks/[task_id] Retrieve a task POST http://[hostname]/todo/api/v1.0/tasks Create a new task PUT http://[hostname]/todo/api/v1.0/tasks/[task_id] Update an existing task DELETE http://[hostname]/todo/api/v1.0/tasks/[task_id] Delete a task <p>Data of a task can be, JSON blob, as:</p> <pre><code>{\n'id': 1,\n'title': 'Title of a to do task',\n'description': 'Description of to do task', \n'done': False\n}\n</code></pre> <ul> <li>This API can be consumed by client side app which can be single page HTML.</li> <li>Note, JSON obect is defined in python as dict, <code>jonify</code> converts and send as JSON Object.</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#serving-over-https","title":"Serving over HTTPS","text":"<ul> <li>generate certificate key using <code>openssl req -x509 -newkey rsa:4096 -nodes -out cert.pem -keyout key.pem -days 365</code></li> <li>allow insecure connection to localhost in chrome, paste <code>chrome://flags/#allow-insecure-localhost</code></li> <li>https://blog.miguelgrinberg.com/post/running-your-flask-application-over-https</li> </ul> <p>Refinement Needed</p>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#flask-mega-tutorial-notes","title":"Flask Mega Tutorial Notes","text":"<p>This is understanding of a tutorial by Miguel Grinberg, we are learning to create a micro-blogging site using flask and other dependencies.</p> <ul> <li><code>from flask import Flask, render_template, request, url_for, flash, redirect</code> libraries</li> <li>render_template - to show html page.</li> <li>request - to get get/post data</li> <li>url_for - to get path for resource</li> <li> <p>flash - to store messages to session var, handy for form error messages.</p> <ul> <li>add session info, after <code>app = Flask(__name__)</code> add <code>app.secret_key = 'super secret key'</code></li> </ul> </li> <li> <p><code>before_request</code> is a flask native function that gets executed before any request is made. It can be used to update last_seen timestamp.</p> </li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#flask-debugging","title":"Flask Debugging","text":"<ul> <li><code>export FLASK_DEBUG=0</code> makes the environment PROD else it is DEV or TEST.</li> <li><code>export FLASK_APP=microblog.py</code> will make an variable that tells python which app to run.</li> <li><code>flask run</code> executes the app or if flask is not in path then do <code>&gt; python -m flask run</code></li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#flask-db-migrate-commands","title":"Flask DB Migrate commands","text":"<p>We can use to create table and manage changes</p> <ul> <li><code>flask db init</code> creates migration folder</li> <li><code>flask db migrate</code> creates scripts for table creation</li> <li><code>flask db upgrade</code> executes scripts to make table changes</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#start-a-python-email-server","title":"Start a python email server","text":"<p><code>(venv) $ python -m smtpd -n -c DebuggingServer localhost:8025</code> this command starts emulated email server.</p> <p>Some variables that we might need to export:</p> <pre><code>export MAIL_SERVER=smtp.googlemail.com\nexport MAIL_PORT=587\nexport MAIL_USE_TLS=1\nexport MAIL_USERNAME=email_id@domain.com\nexport MAIL_PASSWORD=\"password\"\n</code></pre>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#making-new-forms-in-app","title":"Making new forms in app","text":"<p>We need to define following:</p> <ul> <li>(M) Form Class - in module <code>forms.py</code> make class with fields and validate functions.</li> <li>(V) HTML template - in <code>templates</code> folder, rendered from route with form passed as data. Display form elements, errors and hidden_tag.</li> <li>(C) Route - in module <code>routes.py</code>, define new route, import form class and use. On submit, create object of model class to save/query data.</li> <li>Create a link to access new route.</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#saving-form-data-and-tables","title":"Saving form data and tables","text":"<ul> <li> <p>ORM can be created in <code>modules.py</code>. We can define classes for our tables. Classes also define relationships. Have functions to query data.</p> </li> <li> <p>use them in routes to save and query data.</p> </li> <li>uses db service to handle database operations.</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#routespy","title":"Routes.py","text":"<ul> <li> <p>imports forms classes from <code>forms.py</code> for:</p> <ol> <li>getting form data via POST</li> <li>passing form to html template for making form components and displaying errors</li> </ol> </li> <li> <p>import orm model classes from <code>models.py</code> for:</p> <ol> <li>making object of DB Table model</li> <li>submit and commit new data</li> <li>query model class to fetch data</li> </ol> </li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#app-__init__py","title":"app __init__.py","text":"<p>This defines app folder as a python package.</p> <ul> <li>It is where we link all modules (.py files in folder app) together as a Flask app.</li> <li>We import routes, models and error modules here.</li> </ul> <p>Services: Here we have various services defined. We use them across app like db, login, mail.</p>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#blueprints","title":"Blueprints","text":"<p>We can separate out application logical modules in application. Like we can create separate package for auth, error handling etc.</p> <p><code>from flask import Blueprint</code> is what we need to use.</p> <p>To register a blueprint, the <code>register_blueprint()</code> method of the Flask application instance is used. When a blueprint is registered, any view functions, templates, static files, error handlers, etc. are connected to the application.</p> <p>In each Blueprint package we make __initII.py that has name information and imports routes. Routes further imports and links model and controller.</p>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#open-doubts","title":"Open doubts","text":"<ul> <li>UserMixin?</li> <li>Why we pass Classes as param to Class?</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#making-modules","title":"Making modules","text":"<ul> <li>to get url_for use <code>module.route</code>. For eg: main.index.</li> <li>But to render template use only <code>index.html</code> without module name.</li> </ul>"},{"location":"Tech%20Notes/2019-03-05-flask-notes/#elastic-search","title":"Elastic Search","text":"<ul> <li>You can install elastic search by <code>brew install elasticsearch</code> on mac.</li> <li>Access <code>http://localhost:9200</code> to view service JSON output.</li> <li>Also, install in python <code>pip install elasticsearch</code></li> <li>To have launched start elasticsearch now and restart at login: <code>brew services start elasticsearch</code></li> <li>Or, if you don't want/need a background service you can just run: <code>elasticsearch</code></li> </ul> <p>Todo: Search not working, fix it.</p>"},{"location":"Tech%20Notes/2019-06-05-sql-notes/","title":"MySQL, SQLite and SQLAlchemy Notes","text":""},{"location":"Tech%20Notes/2019-06-05-sql-notes/#sqlite","title":"SQLite","text":"<ul> <li>It is a micro database that can work in memory or a saved in file, eg, <code>store.db</code> .</li> <li>Queries are same as any other SQL.</li> <li>if <code>sqlite3</code> is installed then it opens a shell in command line, just like mysql shell.</li> <li>to open a file, use <code>.open FILENAME</code> to open an existing database.</li> <li><code>sqlite3 my.db</code> to work on this file</li> <li><code>.mode csv</code> and then <code>.import data.csv orders</code> loads csv to db, creates if not exists.</li> <li><code>.schema orders</code> to check.</li> </ul>"},{"location":"Tech%20Notes/2019-06-05-sql-notes/#sqlalchemy","title":"SQLAlchemy","text":"<ul> <li>It is ORM for Python, has two parts</li> <li>CORE - can be used to manage SQL from python,</li> <li>ORM - can be used in Object oriented way to access SQL from python.</li> <li>FlaskSQL_Alchemy is extenstion to SQLAlchemy.</li> <li>ORMs allow applications to manage a database using high-level entities such as classes, objects and methods instead of tables and SQL. The job of the ORM is to translate the high-level operations into database commands.</li> <li>It is an ORM not for one, but for many relational databases. SQLAlchemy supports a long list of database engines, including the popular MySQL, PostgreSQL and SQLite.</li> <li>The ORM translates Python classes to tables for relational databases and automatically converts Pythonic SQLAlchemy Expression Language to SQL statements</li> </ul>"},{"location":"Tech%20Notes/2019-06-05-sql-notes/#mysql","title":"MySQL","text":"<p>Installation: - <code>brew install mysql</code></p> <p>Start Server: - <code>brew services start mysql</code> - background mysql start - <code>mysql.server start</code> - no background - <code>mysql_secure_installation</code> - run this and set root pwd, etc. - <code>ps -ef | grep mysqld</code> process</p> <p>Login: - <code>mysql -u root -p</code> then enter password</p> <p>Queries:</p> <pre><code>show DATABASES;\ncreate user 'bob_me'@'localhost' identified with mysql_native_password by 'bob_pwd';\ncreate database bob_db;\ngrant all privileges on bob_db.* to 'bob_me'@'localhost';\n</code></pre> <p>Client - SequelPro: - Connecting:   - Standard   - Host: 127.0.0.1   - enter uername and password and connect.</p> <p>Shutdown Server: - <code>mysql.server stop</code> - stops server</p> <p>Other Notes: - Column and Table names are case-sensitive. - <code>mysqladmin</code> is also installed</p> <p>Trouble Shooting: - If you see error in clients, eg Sequel Pro, it might not be ready yet for a new kind of user login, link. - do <code>ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'newrootpassword';</code> then try loggin in.</p>"},{"location":"Tech%20Notes/2019-06-05-sql-notes/#mongo-db","title":"Mongo DB","text":""},{"location":"Tech%20Notes/2019-06-05-sql-notes/#redis","title":"Redis","text":"<ul> <li><code>redis-server</code> to start the server.</li> </ul> <p>More on: - Flask SQL Alchemy</p>"},{"location":"Tech%20Notes/2020-07-14-google-cloud-platform-notes/","title":"Google Cloud Platform (GCP) and its Services","text":"<p>Google Cloud Platform is cloud service from Google just like AWS and Azure. It provides SaaS, PaaS and IaaS.</p> <ul> <li>Do not remove this line (it will not be displayed) {:toc}</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-google-cloud-platform-notes/#gcp-services","title":"GCP Services","text":"<ul> <li>GCP Firebase as datastoage engine.</li> <li>GCP App Engine is PaaS for deploying web apps on cloud:</li> <li>App Engine also helps us deploy dockers and containers</li> <li>hands on colab.</li> <li>GCP Compute Engine provides VMs, which is like IaaS.</li> <li>GCP Cloud Machine Learning:</li> <li>Offers pretrained models with biggest library.</li> <li>Cloud Vision API, Video Intelligence</li> <li>Identify ojects, landmarks, celebrities, colors, million other entities</li> <li>NLP API, Translations,  Text2Speech, Seech2Text API</li> <li>Auto ML trains on your data using expertise from already trained neurals</li> <li>Provides interface to train, evaluate and proof onjects on your own data.</li> <li>SaaS with latest TensorFlow, PyTorch and SKLearn on VMs with TPU and GPU support</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-google-cloud-platform-notes/#gcp-compute-engine","title":"GCP Compute Engine","text":"<p>Google Compute Engine is a cloud service from GCP which offers Infrastructure as a Service. You can get a machine with configurations as required and it can be easily scaled.</p> <p>Example, start a Micro Machine <code>f1-micro</code>, with Ubuntu Server installed on  20gb HDD, in region 'us-central', and can allow traffic 'http and https'. This is also free for lifetime.</p>"},{"location":"Tech%20Notes/2020-07-14-google-cloud-platform-notes/#ubuntu-server-on-gce-free-for-lifetime-complete-guide","title":"Ubuntu Server on GCE free for lifetime [Complete Guide]","text":"<p>Get an instance on GCE as per requirement or as in example above. We will configure it and add swap memory, then we will make it web server by installing Apache. We will also install MySQL database and PHP/Python as backend languages to serve web apps.</p>"},{"location":"Tech%20Notes/2020-07-14-google-cloud-platform-notes/#install-gcloud-on-mac","title":"Install gcloud on mac","text":"<ul> <li>Follow this guide.</li> <li>Install <code>gcloud</code> on workstation machine, mac, <code>wget &gt; tar -xf &gt; install.sh &gt; gcloud init</code></li> <li>Connect to the VM machine using ssh gcloud command, get it from the SSH dropdown on GCP console near VM.</li> <li>Command: <code>gcloud beta compute ssh --zone \"us-central1-a\" \"vm_name\" --project \"project_name\"</code> this adds to known hosts.</li> <li>This allows you to ssh to GCE host machine from your terminal on workstation.</li> </ul> <p>Congratualations, you have your own linux machine on cloud, free for lifetime and is scalable. It is time to get your hand dirty.</p>"},{"location":"Tech%20Notes/2020-07-14-google-cloud-platform-notes/#transferring-files-to-and-from-gce-instance","title":"Transferring files to and from GCE instance","text":"<ul> <li>You can transfer files using various options mentioned in this guide.</li> <li>we are using <code>gcloud</code> cli to transfer files between workstation and gce instance.</li> <li>Upload <code>gcloud compute scp local-file-path instance-name:dir-on-instance</code></li> <li><code>instance-name</code> is name given during creation of instance</li> <li><code>dir-on-instance</code> is address where you need to copy, eg, <code>~</code></li> <li>Download <code>gcloud compute scp --recurse instance-name:remote-dir local-dir</code></li> <li>to login to server using command <code>gcloud beta compute ssh --zone \"us-central1-a\" \"instance-name\" --project \"project-name\"</code>.</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-google-cloud-platform-notes/#gcp-firebase","title":"GCP Firebase","text":"<p>Firebase is cloud based, app-backend service that is scalable and it helps in authentication, database, file storage, hosting, crashlytics, messeging, adMob, analytics, campaigns etc.</p> <p>It has following components:</p> <ul> <li>ML Kit</li> <li>Has in build ML models for text analytics, image recongnition etc.</li> <li>Works on device or on cloud</li> <li> <p>Can add Tensor flow functions/models to firebase functions and it will be hosted and serverd.</p> </li> <li> <p>Firebase Authentication</p> </li> <li>Google, fb, twitter etc</li> <li>Account based</li> <li>Gives back user information, unique_id, name, photo,</li> <li> <p>manages sessions</p> </li> <li> <p>Cloud functions</p> </li> <li>responses to event, like welcome email on sign in</li> <li>triggers on database events</li> <li>modify files uploaded</li> <li>send cloud messaging messages to other users.</li> <li>build API from database</li> <li> <p>All written in JS using node and then deployed using CLI</p> </li> <li> <p>Firebase Hosting</p> </li> <li>Static files hosting</li> <li>on SSD, serves SSL,</li> <li> <p>can host PWA</p> </li> <li> <p>Firebase Storage</p> </li> <li> <p>Store files, secure them, reliable.</p> </li> <li> <p>Firebase Realtime Database</p> </li> <li>Store and sync data in realtime, even offline</li> <li>It is NoSql database.</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-google-cloud-platform-notes/#firestore-notes","title":"Firestore Notes","text":"<p>It is NoSQL database storage engine in firebase</p> <ul> <li>Works online and offline</li> <li>Stores data in collections</li> </ul> <p>Initialization:</p> <ul> <li>create database</li> <li>create table, called collection, users</li> <li>create rows, called document, doc_id</li> </ul> <p>More on Flutter notes.</p>"},{"location":"Tech%20Notes/2020-07-14-google-cloud-platform-notes/#app-script","title":"App Script","text":"<p>Create app script https://developers.google.com/apps-script/add-ons/translate-addon-sample</p>"},{"location":"Tech%20Notes/2020-07-14-text-editors-notes/","title":"Text Editors - Customizations/Plugins/Hacks for Sublime, VS Code and Vim","text":"<p>If you love coding text editors are your sharp tools. Customizing them to your needs will help you save time and increase productivity.</p>"},{"location":"Tech%20Notes/2020-07-14-text-editors-notes/#sublime-text","title":"Sublime Text","text":""},{"location":"Tech%20Notes/2020-07-14-text-editors-notes/#how-to-use-python-script-with-key-bindings","title":"How to use Python script with Key Bindings","text":"<p>We will create a sample package that will display time on status bar when we press keys <code>ctrl+shift+c</code>, just for demonstrating.</p> <ul> <li> <p>Create a new python file inside Users Package Library. On Mac with Sublime Text 3 it can be created at: <code>Users/yourname/Library/Application Support/Sublime Text 3/Packages/User/any_name.py</code></p> </li> <li> <p>In the python file add following content:</p> </li> </ul> <pre><code>import sublime\nimport sublime_plugin\nimport time\nclass MyCustomMessageCommand(sublime_plugin.WindowCommand):\n# Command shows message on Status Bar\ndef run(self):\nnow = time.strftime(\"%c\")\nmessage = \"The time is \" + now\nsublime.status_message(message)\n</code></pre> <ul> <li>Add the key bindings:</li> </ul> <pre><code>[\n{\n\"keys\" : [\"ctrl+shift+c\"], \n\"command\" : \"my_custom_message\" \n}\n]\n</code></pre> <p>As you can see that the python class name becomes the command name with _ added.</p> <p>Now on pressing <code>ctrl+shift+c</code> you can execute this python file which displays the time on status bar in this case.</p> <p>You can use this feature to unlimited possibilities. I used it to add timestamp to file whenever it was saved.</p> <p>References:</p> <ul> <li>https://forum.sublimetext.com/t/automatically-updated-timestamp/7156/7</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-text-editors-notes/#extending-sublime-text-for-markdown-support","title":"Extending Sublime Text for Markdown Support","text":"<p>If you want more syntax highlighting and better preview of what you write then you can extend Sublime Text by installing  package, follow steps below:</p> <ul> <li>Type: <code>Cmd + Shift + P</code> to open package manager.</li> <li>Then type <code>install package</code> and hit enter. This will provide you list of available packages from packagecontrol.io</li> <li>Next when you get dropdown type <code>Markdown</code> and this will list you all markdown related packages.</li> <li>You can select <code>Markdown Editing</code> to install the package. This provides much better highlighting and preview.</li> </ul> <p>I, personally, didn't like it much and was a bit distracting for me. So I removed this package. But you may like it.</p> <p>Removing a package from Sublime Text:</p> <ul> <li>press <code>Cmd + Shift + P</code> and</li> <li>then type <code>remove package</code>. This will give you list of packages installed and</li> <li>next select <code>Markdown Editing</code> to remove it.</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-text-editors-notes/#vs-code","title":"VS Code","text":"<p>Recently VS Code turned out to be best editor for Markdown however it is bit heavy. Also 'Markdown Preview' extension on Chrome makes it handy to preview markdown files. VS Code can be logged in using GitHub to start sync.</p> <ul> <li>Extensions can be disabled when not in use.</li> <li>open <code>~/.config/Code/User/settings.json</code> to add extension configurations</li> <li>add below codes within the curly braces</li> </ul> <p>Markdownlint disable rules:</p> <pre><code>  \"markdownlint.config\": {\n\"default\": true,\n\"MD007\": { \"indent\": 4 }\n}\n</code></pre> <p>cSpell disable code check in Markdown code blocks:</p> <pre><code>    \"cSpell.languageSettings\": [\n{\n// use with Markdown files\n\"languageId\": \"markdown\",\n// Exclude code inline and multiline both\n\"ignoreRegExpList\": [\n\"^\\\\s*```[\\\\s\\\\S]*?^\\\\s*```\",\n\"\\\\s`[\\\\s\\\\S]*?\\\\s*`\",\n]\n}\n],\n</code></pre>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/","title":"Web Server Configuration Guide - Ubuntu Apache WSGI Python Flask MySQL PHP","text":"<p>Web server can be a linux server that takes client requests and provides a response. It can host a web app. Web app can be backed by a programming language that and process data and store this data in  a database.</p> <p>In this article we would be covering all that is listed in contents below. It is assumed that you have a machine with Ubuntu server installed. It can be on cloud like GCP or AWS etc, or it can be on a virtual machine via Vagrant and Virtual Box.</p> <ul> <li>Do not remove this line (it will not be displayed) {:toc}</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#ubuntu-server-configuration","title":"Ubuntu Server Configuration","text":"<p>Now that you are connected to host VM via SSH, let's start with:</p> <ul> <li>Updating Ubuntu: <code>sudo apt update &amp;&amp; sudo apt upgrade</code></li> <li>Check ram and CPU usage <code>htop</code> , we see that we do not have swap memory, this helps in low ram system when on high load.</li> </ul> <p>Adding the swap memory:</p> <ul> <li>Lets allocate 1GB swap memory: <code>sudo fallocate -l 1G /swapfile</code></li> <li><code>sudo dd if=/dev/zero of=/swapfile bs=1024 count=1048576</code></li> <li>Assign the correct permission to swapfile: <code>sudo chmod 600 /swapfile</code></li> <li>make the swap: <code>sudo mkswap /swapfile</code></li> <li>Turn on the swapfile: <code>sudo swapon /swapfile</code></li> <li>edit the fstab file: <code>sudo nano /etc/fstab</code></li> <li>add this line to the end of file: <code>/swapfile swap swap defaults 0 0</code></li> <li>mount the files: <code>sudo mount -a</code></li> <li>Check ram and cpu again to verify swap: <code>htop</code></li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#install-apache2-php-and-mysql-using-lamp","title":"Install Apache2 PHP and MySQL using LAMP","text":"<p>Install the softwares using below commands:</p> <ul> <li><code>sudo apt install tasksel</code></li> <li><code>sudo tasksel install lamp-server</code></li> <li><code>sudo apt install php-curl php-gd php-mbstring php-xml php-xmlrpc</code></li> <li>Get the external IP address: <code>curl ifconfig.me</code></li> <li>Type http://123.456.789.10 , the external IP address in browser to see served page.</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#domain-configuration","title":"Domain Configuration","text":"<p>Now we need to point the domain to this webserver so that we can access files server. We will use <code>example123.com</code> in this example.</p> <p>Only if you do not have a domain to point to server and want to use vhost:</p> <ul> <li>Modify the host file to add virtual hosts: <code>sudo nano /etc/hosts</code></li> <li>To map 'example123.com' tp IP address add this line: <code>35.111.00.111 example123.com</code></li> </ul> <p>If you have a domain,</p> <ul> <li>edit DNS records and add 'A-record' with the external IP address, for eg:</li> <li>host: *</li> <li>IP: 35.111.00.111</li> <li>then, <code>example123.com</code> will open the page from GCP VM machine.</li> <li>You may also reserve static external IP address of VM on GCP:</li> <li>From your GCP dashboard find 'Networking &gt; External IP addresses'.</li> <li>Now click the down arrow under the 'Type' column and select 'Static' for the External IP address which is connected to your instance of GCP Compute Engine.</li> <li>By reserving a Static IP Address you will not loose your access to website after server outages or restarts.</li> </ul> <p>Point a domain to external IP Address:</p> <ul> <li>To Name servers, add DNS name server, eg, <code>dns1.india-to.com</code></li> <li>on DNS Management of the DNS server</li> <li>add Host Name '@', record type 'A (Address)', then your IP Address</li> <li>add Host Name 'www', record type 'A (Address)', then your IP Address</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#add-sites-to-apache-server","title":"Add sites to Apache Server","text":"<p>Now we need to enable sites on apache we server to connect domain with dir of files to be served by this server.</p> <ul> <li><code>cd /etc/apache2/sites-available/</code></li> <li><code>ls -l</code></li> <li>Copy configuration file for new domain to be added: <code>sudo cp 000-default.conf example123.com.conf</code></li> <li>Let us switch to root user: <code>sudo su</code></li> <li>edit: <code>nano example123.com.conf</code> and add following content to file:</li> </ul> <pre><code>&lt;Directory /var/www/html/example123.com&gt;\nRequire all granted\n&lt;/Directory&gt;\n&lt;VirtualHost *:80&gt;\nServerName example123.com\nServerAlias www.example123.com\nServerAdmin webmaster@localhost\nDocumentRoot /var/www/html/example123.com\nErrorLog ${APACHE_LOG_DIR}/error.log\nCustomLog ${APACHE_LOG_DIR}/access.log combined\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li>disable default site: <code>a2dissite 000-default.conf</code></li> <li>enable new site: <code>a2ensite example123.com.conf</code></li> <li>restart apache service: <code>systemctl reload apache2</code></li> <li>go to web documents location: <code>cd /var/www/html</code></li> <li>create directory for new site: <code>mkdir example123.com</code></li> <li><code>cd example123.com</code> go in the directory.</li> <li><code>nano index.html</code> create an html file.</li> <li>Now open browser and goto 'example123.com' you should see the contents of index.html.</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#mysql-database-configuration","title":"MySQL Database Configuration","text":"<p>Now we will configure MySQL database so that we can use that in our web apps.</p> <ul> <li><code>mysql -u root</code></li> </ul> <pre><code>&gt; CREATE DATABASE db123;  &gt; GRANT ALL ON db123.* TO 'db123_user' IDENTIFIED BY 'db123_pwd!';  &gt; quit;  </code></pre> <ul> <li>Secure the installation <code>mysql_secure_installation</code>, select Y for all or as per your need.</li> </ul> <p>Configure PHP:</p> <ul> <li><code>nano /etc/php/7.2/apache2/php.ini</code></li> <li>update:</li> </ul> <pre><code>upload\\_max\\_filesize = 20M  \npost\\_max\\_size = 21M\n</code></pre> <p>Congratulations, can you believe we are already done! We can create and host any webapp on one of the Worlds biggest cloud infrastructure, GCP, and its scalable!</p>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#wordpress-installation-on-ubuntu-server","title":"Wordpress Installation on Ubuntu Server","text":"<p>Now that we have Apache, PHP and MySQL configured, connected and working together, we can start developing web apps. Wordpress is a easy to use CMS in PHP, lets get started with installing wordpress.</p> <ul> <li><code>cd /var/www/html/example123.com/</code></li> <li>download wordpress: <code>wget https://wordpress.org/latest.tar.gz</code></li> <li>Unzip the file <code>tar -xvf latest.tar.gz</code></li> <li><code>mv wp-config-sample.php wp-config.php</code></li> <li><code>nano wp-config.php</code></li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#web-server-tuning","title":"Web Server Tuning","text":"<p>Configure MPM_Prefork.conf to manage apache load performance:</p> <ul> <li><code>nano /etc/apache2/mods-enabled/mpm_prefork.conf</code></li> <li>Update to below:</li> </ul> <pre><code>&lt;IfModule mpm_prefork_module&gt;\nStartServers    1\nMinSpareServers   2\nMaxSpareServers   5\nMaxRequestWorkers 10\nMaxConnectionsPerChild  1000\n&lt;/IfModule&gt;\n</code></pre> <p>Tune the new Apache install:</p> <ul> <li><code>cd ~</code></li> <li><code>wget https://raw.githubusercontent.com/richardforth/apache2buddy/master/apache2buddy.pl</code></li> <li><code>chmod +x apache2buddy.pl</code></li> <li><code>./apache2buddy.pl</code></li> <li>This perl script tells the health of Apache server.</li> </ul> <p>User guides:</p> <ul> <li>video used: YouTube</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#how-to-add-multiple-domains-on-apache-web-server","title":"How to add multiple domains on Apache Web Server","text":"<p>We can host multiple sites on one server with vistual hosts in Apache.</p> <ul> <li>create conf file <code>cd /etc/apache2/sites-available/</code> then <code>sudo cp 000-default.conf addonDomain.com.conf</code></li> <li>open conf file using nano, then edit</li> </ul> <pre><code>&lt;Directory /var/www/html/addonDomain.com&gt;\nRequire all granted  \n&lt;/Directory&gt;\n&lt;VirtualHost *:80&gt;\nServerName example.com\nServerAlias www.example.com\nServerAdmin admin@example.com\nDocumentRoot /var/www/example.com/html\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li>make folders <code>mkdir var/www/html/addonDomain.com/</code></li> <li>enable site:</li> </ul> <pre><code>a2ensite example.com.conf\nsystemctl reload apache2\n</code></pre> <ul> <li>More details here.</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#serve-python-files-on-apache-web-server","title":"Serve Python files on Apache Web Server","text":"<p>Python files can be served via CGI or WSGI. Python is language, Apache2 is webServer, CGI and WSGI are protocol to help web server and language talk to each other.</p>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#cgi-scripts","title":"CGI Scripts","text":"<p>Common Gateway Interface or CGI provides interface for web server to serve HTML from console programs like Python.</p> <ul> <li>By CGI we can directly open <code>index.py</code> in browser and it works like PHP file. It outputs the result of the script file.</li> <li>default directory is <code>/usr/lib/cgi-bin/</code>, you can add, <code>hello.cgi</code> here and open in browser.</li> <li>enable in apache: <code>a2dismod mpm_event</code></li> <li>enable cgi module: <code>a2enmod mpm_prefork cgi</code></li> <li>to make new dir for cgi files, add following to site conf file:</li> </ul> <pre><code>&lt;VirtualHost *:80&gt;\n...\n&lt;Directory /var/www/html/cgi_dir&gt;\nOptions +ExecCGI\nAddHandler cgi-script .py .cgi\n# DirectoryIndex index.py\n&lt;/Directory&gt;\n...\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li>CGI runs script when requested, where as WSGI runs script with start of WebServer.</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#wsgi","title":"WSGI","text":"<p>Web Server Gateway Interface or WSGI is a simple calling convention for web servers to forward requests to web applications.</p> <ul> <li><code>mod_wsgi</code> is an Apache HTTP Server module that provides a WSGI compliant interface for hosting Python based web applications. It is an alternative to CGI.</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#flask-wsgi-and-apache2-on-linux-hands-on","title":"Flask, WSGI and Apache2 on Linux [Hands On]","text":"<p>Assuming you have a working apache2 server and you have python installed. Next we need to install pip and WSGI module.</p> <p>Python 3:</p> <ul> <li><code>sudo apt-get install libapache2-mod-wsgi-py3</code></li> <li><code>sudo apt-get install python3-pip</code></li> </ul> <p>Python 2:</p> <ul> <li><code>sudo apt-get install libapache2-mod-wsgi</code></li> <li><code>sudo apt-get install python-pip</code></li> </ul> <p>Enable WSGI and install flask:</p> <ul> <li><code>a2enmod wsgi</code> enable the WSGI module in apache.</li> <li><code>pip3 install flask</code></li> </ul> <p>Set up directory and flask files:</p> <ul> <li><code>mkdir /var/www/apps</code> this contains all flask apps.</li> <li><code>mkdir /var/www/apps/blog</code> this is our first app.</li> <li><code>mkdir /var/www/apps/blog/lib</code> this contains code for our blog app.</li> <li><code>mkdir /var/www/apps/blog/lib/static</code> this will serve static files for our blog app.</li> </ul> <p>Make flask app:</p> <ul> <li><code>sudo nano /var/www/apps/blog/lib/main.py</code></li> </ul> <pre><code>from flask import Flask\napp = Flask(__name__)\n@app.route('/')\ndef hello_world():\nreturn 'Hello from Flask blog app!'\nif __name__ == '__main__':\napp.run()\n</code></pre> <p>Make this folder as python module, (important):</p> <ul> <li><code>touch /var/www/apps/blog/lib/__init__.py</code></li> <li>We created <code>__init__.py</code> as a blank file. This is important and is required to import our lib folder as a python module. Now we can import <code>lib.main</code> in wsgi file.</li> </ul> <p>Add the wsgi file:</p> <ul> <li><code>sudo nano /var/www/apps/blog/app.wsgi</code></li> </ul> <pre><code>import sys\nsys.path.insert(0, '/var/www/apps/blog')\nfrom lib.main import app as application\n</code></pre> <p>Configuring virtual hosts conf file to make it work:</p> <ul> <li>Imp, in exmple below, replace 'myapps.com' with your domain name.</li> <li><code>cd /etc/apache2/sites-available/</code></li> <li>add a site file <code>cp 000-default.conf myapps.com.conf</code></li> <li><code>nano myapps.com.conf</code></li> </ul> <pre><code>&lt;VirtualHost *:80&gt;\nServerName myapps.com\nServerAdmin webmaster@localhost\n# This is path for homepage of myapps.com\nDocumentRoot /var/www/html/myapps.com\n# App: blog, URL: http://myapps.com/myblog\nWSGIScriptAlias /myblog /var/www/apps/blog/app.wsgi\n&lt;Directory /var/www/apps/blog&gt;\nOrder deny,allow\nAllow from all\n&lt;/Directory&gt;\nAlias /myblog/static /var/www/apps/blog/lib/static\n# Enables passing of authorization headers\nWSGIPassAuthorization On\n# logs configuration\nErrorLog ${APACHE_LOG_DIR}/error.log\nCustomLog ${APACHE_LOG_DIR}/access.log combined\n&lt;/VirtualHost&gt;\n</code></pre> <ul> <li><code>a2ensite myapps.com.conf</code> enable the site.</li> <li><code>service apache2 restart</code> restart the service.</li> <li>Now visit <code>http://myapps.com/myblog/</code> to access app.</li> </ul> <p>If you see errors:</p> <ul> <li><code>tail -30 /var/log/apache2/error.log</code> shows you error logs from apache server.</li> </ul>"},{"location":"Tech%20Notes/2020-07-14-web-server-notes/#multiple-flask-apps-and-adding-user-and-group-to-process","title":"Multiple Flask apps and adding user and group to process","text":"<ul> <li>Add new user <code>adduser apps</code>, remember password here</li> <li>update primary group <code>usermod -g www-data apps</code></li> <li> <p>give permission to group <code>chmod 775 /home/apps</code></p> </li> <li> <p>login with new user <code>apps</code></p> </li> <li><code>git clone the_web_app.git</code></li> <li><code>python3 -m venv venv</code></li> <li><code>source venv/bin/activate</code></li> <li><code>pip3 install -r requirements.txt</code></li> <li><code>touch __init__.py</code></li> <li>add <code>app.wsgi</code></li> </ul> <pre><code>#!venv/bin/python3\nimport sys\nimport logging\nlogging.basicConfig(stream=sys.stderr)\nsys.path.insert(0, '/home/apps/todo')\nfrom lib.main import app as application\n</code></pre> <p>Apache v-host file:</p> <pre><code>&lt;VirtualHost *:80&gt;\nServerName py.ess.com\nServerAdmin webmaster@localhost\nDocumentRoot /var/www/html/py.ess.com\nWSGIScriptAlias /site1 /var/www/apps/app1/app1.wsgi\n&lt;Directory /var/www/apps/app1&gt;\nOrder deny,allow\nAllow from all\n&lt;/Directory&gt;\nAlias /site1/static /var/www/apps/app1/static\nWSGIScriptAlias /flap /var/www/apps/flap/app.wsgi\n&lt;Directory /var/www/apps/flap&gt;\nOrder deny,allow\nAllow from all\n&lt;/Directory&gt;\nWSGIScriptAlias /todo /var/www/apps/todo/app.wsgi\n&lt;Directory /var/www/apps/todo&gt;\nOrder deny,allow\nAllow from all\n&lt;/Directory&gt;\nWSGIDaemonProcess todo-client user=apps group=www-data threads=5\nWSGIScriptAlias /todo-client /home/apps/todo/app.wsgi\n&lt;Directory /home/apps/todo&gt;\nWSGIApplicationGroup todo-client\nWSGIProcessGroup todo-client\nOrder deny,allow\nAllow from all\nRequire all granted\n&lt;/Directory&gt;\nAlias /todo-client/static /home/apps/todo/lib/static\nWSGIPassAuthorization On\n# WSGIDaemonProcess site2 user=myserviceuser group=myserviceuser threads=5 python-home=/$\n# WSGIScriptAlias /site2 /var/www/apps/app2/application.wsgi\n# &lt;Directory /var/www/apps/app2&gt;\n#     WSGIApplicationGroup site2\n#     WSGIProcessGroup site2\n#     Order deny,allow\n#     Allow from all\n# &lt;/Directory&gt;\n# logs configuration\nErrorLog ${APACHE_LOG_DIR}/error.log\nCustomLog ${APACHE_LOG_DIR}/access.log combined\n&lt;/VirtualHost&gt;\n</code></pre> <p>Note:</p> <ul> <li>ensure group has permission to database and log directories.</li> </ul> <p>To Do:</p> <ul> <li>WSGIDaemonProcess helloworldapp user=www-data group=www-data threads=5</li> <li>WSGIProcessGroup</li> <li>Multiple Apps: Apache virtualhost is only for domain/sub-domain, to add more apps with different directories, add directory tags to configuration file. If the two Flask apps are running on the same domain just as subfolders, then you only need one VirtualHost but you\u2019ll need multiple WSGIScriptAlias directives.</li> </ul> <p>Links:</p> <ul> <li>Muiltiple flask apps usgin Apache2 and Ubuntu.</li> </ul>"},{"location":"Tech%20Notes/2020-07-15-flutter-notes/","title":"Flutter BLoC and Firebase Notes","text":"<p>Google Flutter is a framework for developing applications for iOS, Android and Web with same code base. It uses Dart as programming language. The dart code gets compiled to java/javascript/swift for native performance. It has rich open source libraries to plug in.</p> <ul> <li>Do not remove this line (it will not be displayed) {:toc}</li> </ul> <p>Everything is a widget here :)</p>"},{"location":"Tech%20Notes/2020-07-15-flutter-notes/#quickstart","title":"Quickstart:","text":"<ul> <li><code>flutter create --org com.codeo myapp</code> create a basic app</li> <li><code>flutter pub get</code> gets all packages</li> <li><code>flutter run -d Chrome</code> runs flutter app on device chrome</li> <li><code>pod setup</code> makes ios deploy faster</li> <li><code>flutter build web/apk</code> builds app for publishing</li> </ul>"},{"location":"Tech%20Notes/2020-07-15-flutter-notes/#structure","title":"Structure:","text":"<ul> <li>assets - hold images/fonts</li> <li>lib - all the dart code</li> <li>we can make folders specific for activity</li> <li>models - hold class for data in the app like users.dart, deserialize json</li> <li>pages - hold screens, each .php page or route</li> <li>widgets - component on pages, like progress bar</li> <li>all folders are packages, eg, lib package, src package, models package.</li> </ul>"},{"location":"Tech%20Notes/2020-07-15-flutter-notes/#basic-workflow","title":"Basic Workflow:","text":"<ul> <li>make product class, init constructor, add factory, the get and set functions</li> <li>link this to sqflite</li> <li>have a db_provider</li> <li>and an init function</li> <li>crud functions</li> <li>or firebase</li> <li>get reference of a collection in instance.</li> <li>from ref get the snapshots and build.</li> </ul>"},{"location":"Tech%20Notes/2020-07-15-flutter-notes/#development","title":"Development:","text":"<ul> <li><code>add(a,b) =&gt; a + b</code> creates function inline and returns valuel, can be without name.</li> <li><code>setState(){}</code> - rebuilds the app. usually used in onTap(){}.</li> <li>wrapping parameters in {} make them named params and we have to specify the name when passing them in call. eg: <code>emp({String name, Int age})</code>, when called, <code>emp(name: 'Jon', age: 34)</code>.</li> <li><code>initState() {}</code> function gets executed in every State ful widget. can be used to call all what we want to initialize, like db fetch etc.</li> <li><code>&lt;Future&gt;</code> needs to be handled. Either we can use <code>.then((data) {...})</code> or we can add keywords <code>async...await</code> to the functions.</li> <li><code>factory</code> before a func declaration makes it accessible outside class just like static.</li> <li>To preserve state of app, use mixin keep alive.</li> <li>await and async are good to wait for a process to finish and the execute the rest.</li> <li>Null-aware Operators in Dart</li> </ul> <p>Navigation: - We can generate route on the go with Navigator.pop or push.</p>"},{"location":"Tech%20Notes/2020-07-15-flutter-notes/#function-calling","title":"Function calling:","text":"<ul> <li>When work needs to be done on call, then pass refrence.</li> <li>When function builds/returns then call with ()s.</li> <li>Here is the difference:</li> <li>onPressed: _incrementCounter is a reference to an existing function is passed. This only works if the parameters of the callback expected by onPressed and _incrementCounter are compatible.</li> <li>onPressed: _incrementCounter() _incrementCounter() is executed and the returned result is passed to onPressed. This is a common mistake when done unintentionally when actually the intention was to pass a reference to _incrementCounter instead of calling it.</li> </ul> <p>External Links: - appicon.co - app icons - icons8.com - use icons for free - vecteezy.com - icons - canva.com - create own design</p>"},{"location":"Tech%20Notes/2020-07-15-flutter-notes/#flutter-state-management","title":"Flutter State Management:","text":"<p>Flutter allows to use many kind of state management architectures like Redux, BLoC, MobX and many more. These all are commonly used architecture to layer out UI from Database/WebAPIs.</p>"},{"location":"Tech%20Notes/2020-07-15-flutter-notes/#bloc-business-logic-component","title":"BLoC - Business Logic Component","text":"<ul> <li>separates UI from Business logic (Database and Network).</li> <li><code>Sink&lt;data&gt;</code> - data in - events</li> <li><code>Stream&lt;data&gt;</code> - data out - state</li> </ul> <ul> <li>BLoC component converts a stream of incoming events into a stream of outgoing states. </li> <li>Close bloc references in despose() methods.</li> <li>It has:</li> <li>Bloc</li> <li>BlocBuilder</li> <li> <p>BlocProvider</p> </li> <li> <p>Reactive Programming, whenever there is new data coming from the server. We have to update the UI screen</p> </li> <li> <p>KeyNote: never make any network or db call inside the build method and always make sure you dispose or close the open streams.</p> </li> <li> <p>Single Instance - all screens have access to bloc, there is one instance in the app</p> </li> <li> <p>Scoped Instance - only widget to which the bloc is exposed has access</p> </li> <li> <p>PublishSubject: Starts empty and only emits new elements to subscribers. There is a possibility that one or more items may be lost between the time the Subject is created and the observer subscribes to it because PublishSubject starts emitting elements immediately upon creation.</p> </li> <li> <p>BehaviorSubject: It needs an initial value and replays it or the latest element to new subscribers. As BehaviorSubject always emits the latest element, you can\u2019t create one without giving a default initial value. BehaviorSubject is helpful for depicting \"values over time\". For example, an event stream of birthdays is a Subject, but the stream of a person's age would be a BehaviorSubject.</p> </li> <li> <p>We pass the blocProvider to MaterialRoute and then it houses all the variables to be passed. This acts as inheritedWidget.</p> </li> <li>MovieApp - Part 2</li> </ul> <p>The workflow of the Counter App: - add packages - create events as enum. - create state - in this app, state is <code>int</code> so we don't create state class. - create bloc to take events, map it, and return state,    - <code>class CounterBloc extends Bloc&lt;CounterEvent, int&gt; {...}</code>   - here override init state   - and override mapEventsToState - instantiate bloc in main using <code>BlocProvider&lt;Bloc&gt;{}</code>.  - create Page, get bloc,  - get bloc, <code>final CounterBloc counterBloc = BlocProvider.of&lt;CounterBloc&gt;(context);</code> - body will be <code>BlocBuilder&lt;CounterBloc, int&gt;();</code> to build UI based on state. - action event, <code>onPressed: () {counterBloc.add(CounterEvent.increment);}</code> - based on Flutter Counter App tutorial by @felangel.</p> <p>Redux: - provides routing as well - works with store reducer concept</p> <p>ScopedModel - Updates only model in scope, not the whole widget tree - Have to notifyListeners() on each state change</p> <p>Links: - Flutter Architecture Samples to build ToDo apps - The Essential Guide to UI Engineering</p>"},{"location":"Tech%20Notes/2020-07-15-flutter-notes/#firestore-and-flutter","title":"Firestore and Flutter","text":"<p>Connecting apps: - iOS:   - add GoogleService-Info.plist in xCode    - add reverseClientId in key CFBundleURLSchemes Runner/info.plist - android:   - add google-services.josn and    - make sure applicationId is correct in app/build.gradle</p> <p>Reading data (Flutter): - Create a reference to instance of a collection, <code>userRef = Firestore.instance.collection('users')</code>. - The above has functions to fetch records as snapshot of data - It returns future which need to be handled. - use StreamBuilder - stream: Firestore.instance.collection('users').snapshots(), - reference object accepts chain of functions like <code>where()..orderBy()..limit()</code> etc. for compound queries. - <code>FutureBuilder</code> - reads data in database - <code>StreamBuilder</code> - provides stream to data, shows new user added.</p> <p>NoSQL Structuting: - do not nest collections - keep data flatten - copy data but mostly stored in the way it is best to be fetched.</p> <p>Create/Update/Delete data (Flutter): - onTap: () =&gt; record.reference.updateData({'key': new_value}) - now its all linked from front to back - all sync and updates offline and online. - reference has function like <code>add()..updateData()..delete()</code>. They all return <code>&lt;Future&gt;</code>.</p> <p>Triggers: - user firestore triggers to listen and act on the change in a collection. - It uses node js to write and deploy the functions. - Triggers can be created by using Trigger functions. Steps:   - <code>firebase login</code> - login to your google account to use CLI.   - <code>firebase init fucntions</code> - create fucntions folder in flutter project.   - <code>firebase deploy --only functions</code> - deploy functions to firebase cloud.</p> <p>References: - Felix Angelov - https://www.youtube.com/watch?v=knMvKPKBzGE - Brian Egan - https://www.youtube.com/watch?v=Wfc5LMgWaRA</p>"},{"location":"Tech%20Notes/2020-07-15-vagrant-virtualbox-notes/","title":"Vagrant and Virtual Box Notes","text":"<p>Vagrant is a CLI to create virtual box with all configurations in a file. </p> <ul> <li>Install vagrant by downloading from site. It installs package with CLI.</li> <li>Create following file in any folder, say <code>~/vagrant/vagrantfile</code>:</li> </ul> <pre><code>Vagrant.configure(\"2\") do |config|\nconfig.vm.box = \"ubuntu/xenial64\"\nconfig.vm.network \"private_network\", ip: \"192.168.33.10\"\nconfig.vm.provider \"virtualbox\" do |vb|\nvb.memory = \"1024\"\nend\nend\n</code></pre> <ul> <li>Then run <code>vagrant up</code>. On first run it will download and install ubuntu 16.04, 1GB, at 192.168.33.10. In subsequent run it will just start the VM.</li> <li>do, <code>vagrant ssh</code> to ssh to new vm.</li> <li><code>vagrant halt</code> to stop a VM</li> <li>DANGER ZONE: <code>vagrant destroy</code> to delete a VM</li> </ul>"},{"location":"Tech%20Notes/2020-08-1-pelican-python-notes/","title":"Pelican - Static Site Generator in Python - Basic Guide","text":"<p>Pelican is a static site generator in Python.</p>"},{"location":"Tech%20Notes/2020-08-1-pelican-python-notes/#quickstart","title":"Quickstart","text":"<ul> <li>make a dir</li> <li>create virtual env <code>python3 -m venv venv</code></li> <li>upgrade pip <code>./venv/bin/pip install --upgrade pip</code></li> <li>activate env <code>source venv/bin/activate</code></li> <li>install pelican <code>pip install pelican</code></li> <li>create basic files and dir <code>pelican-quickstart</code></li> <li>start server <code>make devserver</code> go to <code>http://localhost:8000/</code></li> <li>It builds the <code>output</code> dir</li> </ul>"},{"location":"Tech%20Notes/2020-08-1-pelican-python-notes/#structure","title":"Structure","text":"<ul> <li><code>content</code> holds all contents,</li> <li>articles can be created as <code>.md .html or .rst</code></li> <li><code>content/pages</code> dir can have pages</li> <li><code>content/downloads</code> can serve static content for pages</li> <li><code>{static}/downloads/logo.jpg</code> copies file to output and links to path</li> <li> <p>Samples can be found on docs and eg site</p> </li> <li> <p><code>output</code> has all the static pages built using content and theme.</p> </li> </ul>"},{"location":"Tech%20Notes/2020-08-1-pelican-python-notes/#adding-theme","title":"Adding Theme","text":"<ul> <li>Make a themes dir anywhere</li> <li>add this dir to <code>pelicanconf.py</code> as <code>THEME = 'themes_dir/my_theme</code></li> <li><code>my_theme</code> should have two folders</li> <li><code>static</code> all your css, js and images can go here, copied as-is to output</li> <li><code>templates</code> templates for various mandatory pages<ul> <li><code>base.html</code> starting point, links <code>href=\"{{ SITEURL }}/theme/css/my.css\"&gt;</code></li> <li><code>page.html</code> imports base and adds content for pages</li> <li><code>index.html</code> lists all blog articles</li> <li>other pages are mandatory like article, author, tag etc can be copied from here.</li> </ul> </li> </ul>"},{"location":"Tech%20Notes/2020-08-1-pelican-python-notes/#dev-sprints","title":"Dev Sprints","text":"<ul> <li>Go to folder</li> <li>activate env <code>source venv/bin/activate</code></li> <li>start server <code>make devserver</code> go to <code>http://localhost:8000/</code></li> <li>this will launch the site and will autoupdate site on page modifications.</li> </ul>"},{"location":"Tech%20Notes/2020-08-1-pelican-python-notes/#production-push","title":"Production Push","text":"<ul> <li>Delete the output dir</li> <li>run <code>make devserver</code> go to <code>http://localhost:8000/</code> verify all changes</li> <li>then make an archive of <code>output</code> folder and upload to your host.</li> <li>for GCP please follow steps here.</li> </ul>"},{"location":"Tech%20Notes/2020-08-1-pelican-python-notes/#tips-and-tricks","title":"Tips and Tricks","text":"<ul> <li>to replace blog index page, add following metas to any new <code>homepage.html</code></li> </ul> <pre><code>  &lt;meta name=\"save_as\" content=\"index.html\" /&gt;\n&lt;meta name=\"url\" content=\"index.html\" /&gt;\n</code></pre> <ul> <li>sorting menu items</li> <li>in <code>pelicanconf.py</code> add <code>PAGE_ORDER_BY = 'reversed-date'</code></li> <li>then in pages meta add <code>&lt;meta name=\"date\" content=\"2020-07-26 07:00\" /&gt;</code></li> <li>you can then change time to sort pages in menu bar.</li> </ul> <p>Minimum html required for pages:</p> <pre><code>&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;My super title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n        This is the content of my super blog post.\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>To do:</p> <ul> <li>exclude some pages from menu</li> <li>add articles</li> </ul>"},{"location":"Tech%20Notes/2021-05-05-datascience-notes/","title":"Data Science Learning Notes","text":"<p>Table of Index:</p> <p>Maths: - Probability - Statistics - Statistics Advance - Distributions</p> <p>Data ETL EDA Wrangling - Python Notes - Data Structures - Pandas Notes - Series, DataFrame, Heirarchy - Data Handling - Pre-Processing, EDA, Transformation - Dimentionality Reduction - PCA, LDA, Kernel PCA</p> <p>Regression - supervised:   - Linear Regression   - Simple Linear Regression   - Multiple Linear Regression   - Polynomial Linear Regression   - Support Vector Regression   - Decision Tree Regression   - Random Forest Regression</p> <p>Classification - supervised: - Logistic Regression - confusion matrix, accuracy, sigmoid, CAP Curve - KNN - K Nearest Neighbour Classifier - Euclidean distance - SVM - Support Vector Machines - Maximum Margin Hyperplane - Kernel SVM - Map to Higher Dimention, Kernel Trick, Gaussian RBF Kernel - Naive Bayes - Bayes Theorem - Decision Tree Classifier - Random Forest Classifier - entropy, ensemble learning</p> <p>Clustering - unsupervised: - K-Means Clustering - wcss, choosing k-value, k-means++ - Hierarchical Clustering - Agglomerative, Dendrogram</p> <p>Association Rule - Unsupervised: - Apriori Algorithm - market basket analysis, lift, support, confidence, todo - Eclat (Part 5 todo) - FP Growth</p> <p>Other Bonus Extra: - Model Select  - Reinforcement Learning (Part 6 todo)    - Upper Confidence Bound   - Thompson Sampling</p> <p>NLP: - Cleaning, stemming, nltk, bag of words, tokenization</p> <p>Deep Learning: (Part 8 todo)</p> <p>Dimentionality Reduction: - PCA - max variance, unsupervised - LDA - max class separation, supervised - Kernel PCA - kernel trick on non-linearly separable dataset - SVD - GDA - Generalized Discriminant Analysis</p>"},{"location":"Tech%20Notes/2021-05-05-python-notes/","title":"Python Conda PIPs Venv RegEx R","text":""},{"location":"Tech%20Notes/2021-05-05-python-notes/#other-python-notes","title":"Other Python Notes","text":"<ul> <li>Python Coding Kaggle</li> <li>Pandas Kaggle</li> <li>Flask - back end web framework micro</li> <li>Pelican - Static Site Generator</li> </ul>"},{"location":"Tech%20Notes/2021-05-05-python-notes/#web-scraping-selenium","title":"Web Scraping - Selenium","text":"<p>Install web driver</p> <ul> <li>visit <code>https://chromedriver.chromium.org/downloads</code> and download version same as your browser version.</li> <li>unzip and move <code>chromedriver</code> to <code>/usr/local/bin/chromedriver</code></li> </ul> <p>More - https://realpython.com/modern-web-automation-with-python-and-selenium/</p>"},{"location":"Tech%20Notes/2021-05-05-python-notes/#data-science-setup","title":"Data Science Setup","text":"<p>Python installed in Ubuntu or Mac should not be used. Instead create a Virtual Environment for it.</p> <p>Virtual Environments can be created using conda or venv module. Each virtual environment has its own Python binary and can have its own independent set of installed Python packages in its site directories. More here.</p> <p>Quick Start on Linux more here.</p> <pre><code>sudo apt update\nsudo apt install python3 python3-dev python3-venv\n\nsudo apt-get install wget\nwget https://bootstrap.pypa.io/get-pip.py\nsudo python3 get-pip.py\n\npip --version\n\ncd your-project\npython3 -m venv env\n\nsource env/bin/activate\n</code></pre> <p>This will create a dir <code>env</code> and will have its own python, python3, pip and pip3. Now you can install any packages and this will not interfere with system.</p> <p>Conda is package and virtual environment manager, like pip, for any language\u2014Python, R, Ruby and more. It is a CLI and can</p> <ul> <li>install packages like flask, jupyter, pandas etc.</li> <li>can manage envs, virtual environment is separate python and its packages. This means each project you work on can have its own set of packages.</li> </ul> <p>Anaconda is toolkit for Data Science. Along with conda it includes ds and ml libraries (500Mb) installed.</p> <p>Anaconda Navigator is GUI to use conda.</p> <p>Miniconda includes conda and python but not much libraries.</p> <p>So we can use conda alone to create a development virtual environment for new data science project or use miniconda.</p> <p>Installing Conda on Linux</p> <ul> <li><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</code></li> <li><code>bash Miniconda3-latest-Linux-x86_64.sh</code> this installs python, conda, vevn and others, all in virtual environment.</li> <li>After installation, restart terminal, and it will load new environmnet called 'base'. Now python is not the default system python.</li> <li>If you'd prefer that conda's base environment not be activated on startup, set the auto_activate_base parameter to false: <code>conda config --set auto_activate_base false</code></li> <li>more: https://www.projectdatascience.com/step-by-step-guide-to-setting-up-a-professional-data-science-environment-on-a-linux/</li> </ul> <p>Quick Start using Conda for a new project</p> <pre><code>conda deactivate\nmkdir prj1\ncd prj1\nconda create -n prj1env pandas numpy jupyter scikit-learn matplotlib seaborn\nconda activate prj1env\ntouch README.md\ncode .\n</code></pre> <p>Run <code>conda activate prj1env</code> in code shell.</p> <p>Undo <code>conda deactivate &amp;&amp; conda remove --name prj1env --all</code> and remove files if any.</p> <p>Install Jupyter in the venv. Now that we have an environment (base) you can use it, or create a new. Then</p> <ul> <li><code>pip install jupyter</code></li> <li><code>which jupyter</code> shows <code>/home/vaibhav/code/miniconda3/bin/jupyter</code> it does not effect the system python.</li> </ul> <p>Conda Basic Commands</p> <ul> <li><code>conda update conda</code> updates conda</li> <li><code>conda install PACKAGENAME</code> installs pkg to default/active env</li> <li><code>conda update PACKAGENAME</code> updated pkg</li> <li><code>pip install pkg</code> aslo intalls to active env</li> </ul> <p>Conda Env Commands</p> <ul> <li><code>conda create --name py35 python=3.5</code> created new env called 'py35' and installs py 3.5</li> <li><code>conda activate py35</code> activates</li> <li><code>conda list</code> lists all packages installed in active env.</li> <li><code>conda remove --name my-env --all</code> deletes the env.</li> </ul> <p>Jupyter name comes from Julia, Python and R,</p> <ul> <li>formerly known as IPython Notebook</li> <li>it is pkg, same as flask</li> <li><code>pip install jupyter</code> or it comes installed in Anaconda dist.</li> <li><code>jupyter notebook</code> runs a server to server jupyter notebooks at http://localhost:8888/tree</li> </ul> <p>R and RStudio are statistical language and IDE,</p> <ul> <li><code>r</code> or <code>R</code> - R console in terminal</li> <li><code>Rscript my.r</code> - executes file in terminal</li> <li><code>Rscript -e \"getwd()\"</code> - executes cmd in terminal, can quickly install a library.</li> <li><code>R CMD BATCH my.r</code> runs R script and saves output to <code>my.r.Rout</code></li> <li>To make R Script executable like <code>./my.r</code> then:</li> <li>set permission to 755</li> <li>add correct <code>#!</code> to top of file</li> </ul> <pre><code>#!/usr/bin/env Rscript\nsayHello &lt;- function(){\nprint('hello')\n}\nsayHello()\n</code></pre>"},{"location":"Tech%20Notes/2021-05-05-python-notes/#regex-notes","title":"Regex Notes","text":"<p>Remove single line comments // from code:</p> <ul> <li><code>\\/\\/.*$\\n</code> - Finds all single line comments starting with //.</li> <li><code>\\/\\/</code> - string has //</li> <li><code>.*</code> - then has anything after that</li> <li><code>$\\n</code> - then matches next line as well.</li> <li>Check and Validate on Regex101</li> </ul>"},{"location":"Tech%20Notes/2021-05-05-python-notes/#python-for-animation-and-modelling","title":"Python for Animation and Modelling","text":"<p>VPython GlowScript</p> <ul> <li>can be used to create objects and animate them</li> <li>VPython makes it unusually easy to write programs that generate navigable real-time 3D animations.</li> <li>https://www.glowscript.org/docs/VPythonDocs/videos.html</li> </ul> <p>Manim</p> <ul> <li>can animate equations and plots</li> <li>https://github.com/3b1b/manim</li> <li>https://www.youtube.com/watch?v=ENMyFGmq5OA</li> </ul> <p>Sage</p> <ul> <li>Allows equation animations and plotting</li> <li>https://www.sagemath.org/download-mac.html</li> </ul> <p>Povray</p> <ul> <li>The Persistence of Vision Raytracer is a high-quality, Free Software tool for creating stunning three-dimensional graphics. The source code is available for those wanting to do their own ports.</li> <li>http://www.povray.org/</li> </ul> <p>ImageMagick</p> <ul> <li>Create, edit, compose, or convert digital images.</li> <li>It can resize, flip, mirror, rotate, distort, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polygons, ellipses and B\u00e9zier curves.</li> </ul> <p>EdX</p> <ul> <li>https://learning.edx.org/course/course-v1:CornellX+ENGR2000X+1T2017/home</li> </ul>"},{"location":"Tech%20Notes/2021-08-24-web-development-notes/","title":"Web and App Development","text":"<p>Web development may need:</p> <ul> <li>OS and Infa - linux mostly, hosting/gcp/aws - machine connected to internet</li> <li>Web Server - Apache/NginX - connects domain to DIR/Process.</li> <li>Web Cache - optional - Squid</li> <li>Database - to store, mysql/sqlite/mongo/postgres</li> <li>Backend - lang &amp; framework - this will take requests and return response and can connect to DB.</li> <li>Frontend - is on the client side, JS, can have framework.</li> </ul> <p></p> <p>Web Servers / Proxy Servers</p> <ul> <li>Is a server application that acts as an intermediary between a client requesting a resource and the server providing that resource.</li> <li>Can help configrue DIR in case of static site and process in case of python/perl/php and use WSGI or CGI. Uses:</li> <li>Monitoring and filtering</li> <li>Filtering of encrypted data</li> <li>Bypassing filters and censorship - Geo restriction, firewall etc</li> <li>Logging</li> <li> <p>Improving performance</p> </li> <li> <p>Gunicorn, is a Web Server Gateway Interface (WSGI) server implementation that is commonly used to run Python web applications.</p> </li> <li> <p>Apache</p> </li> <li> <p>NginX</p> </li> <li> <p>more on webserver</p> </li> </ul> <p>Databases:</p> <ul> <li>mongo - nosql - JSON like document based</li> <li>mysql / PostgreSQL - sql</li> <li>neo4j - graph</li> </ul>"},{"location":"Tech%20Notes/2021-08-24-web-development-notes/#backend","title":"Backend","text":"<p>It is the server side part of the app. It provides services which clients can request or consume.</p> <p>Frameworks and languages</p> <ul> <li>Flask - Python</li> <li>Laravel - PHP</li> <li>Elastic.js - JS</li> </ul> <p>Auth is required for all request, so decouple and make a service of it. The gateway can take req and the authenticate it or reject it. Once autheticated it can send it to the correct service.</p> <p>APIs or Headless or RESTful backend can be build when we need to completely separate client from server.</p> <ul> <li>request payload is JSON data sent with req. In GET req we don't have to send payload we just send URL params.</li> <li>avoid actions in payload. like func name in payload, instead keep action in route</li> <li>avoid doing everything in one route, eg, if not exist then create else add, instead make the actions atomic to routes.</li> <li>for huge responses, like get_orders() returning all orders with all details, add pagination or fragmentation (in microservices)</li> </ul> <p>Load Balancing or Consistent Hashing</p> <ul> <li>when requests to a server increases we need to add new servers, then redirect requests to different servers. This can be done using hashing. Hash gives a random number to a request, then that number can be used to direct a request to a particular server, eg, hash_number mod servers is the server ID, 14%4=2, so req hased as 14 goes to server 2, if we have 4 servers.</li> <li>now when we add new server, say 5, then the mod operation changes, every req gets mod of 5, this makes huge operational difference, as in there is a shift in all requests, so all cache that we built becomes useless, to avoid this, we use consistent hashing.</li> <li>more - https://www.youtube.com/watch?v=K0Ta65OqQkY</li> </ul> <p>DjangoREST vs FastAPI, ???</p> <p>WebSockets vs HTTP - Unlike req-res architecture of HTTP, in web sockets server can also send updates to client, updates are sent immediately when they are available. WebSockets keeps a single, persistent connection open while eliminating latency problems that arise with HTTP request/response-based methods. Can be used in messaging and notifications. Protocol is:</p> <ul> <li>XMPP on TCP</li> </ul> <p>Microservices - ???</p> <p>Gateway - ???</p>"},{"location":"Tech%20Notes/2021-08-24-web-development-notes/#frontend","title":"Frontend","text":"<p>It makes the UI or client for an app. It can talk to API and do CRUD. It need to send authorization and access key/token to API.</p> <p>Must have features:</p> <ul> <li>Modular Layout - cards, templates</li> <li>Search</li> <li>Sort</li> <li>Filter</li> <li>Pagination</li> <li>Favourites</li> <li>Enhancements:</li> <li>Event Bubling / Capturing</li> <li>Debouncing</li> </ul> <p>Frameworks</p> <ul> <li>React</li> <li>Angular</li> <li>Vue</li> </ul> <p></p> <p>Object-Relational Mapping (ORM)is a technique that lets you query and manipulate data from a database using an object-oriented paradigm. It abstracts Object oriented code from database and hence we can switch DBs. eg, Eloquent in Laravel, SQLAlchemy is an open-source SQL toolkit and object-relational mapper for the Python programming language</p> <p>Headless is backend app with no frontend, it only has API endpoints.</p> <p>12-Factor Application - The Twelve-Factor App methodology is a methodology for building software-as-a-service applications. These best practices are designed to enable applications to be built with portability and resilience when deployed to the web. More - https://12factor.net/</p> <p>PWA - Progressive Web Apps</p> <p>Stacks:</p> <ul> <li>LAMP - Linux Apache MySql PHP</li> <li>MEAN - mongo express angular node</li> <li>MERN - mongo express react node</li> </ul> <p>What is a docker??</p> <p>FastAPI</p> <p>Flask Mega Tutorial</p> <p>Saleor - Python eCom open-source framework:</p> <ul> <li>headless ecom SDK</li> <li>can create category hierarchy</li> <li>can have attributes for products like size cost etc</li> </ul> <p>References:</p> <ul> <li>https://www.fullstackpython.com/</li> <li>https://yalantis.com/blog/tech-stack-for-web-app-development/</li> <li>https://testdriven.io/blog/fastapi-mongo/</li> </ul>"},{"location":"Tech%20Notes/2021-08-24-web-development-notes/#mobile-app-development","title":"Mobile App Development","text":"<p>Data is sourced from RESTAPI or can be local storage.</p> <p>Apps can be:</p> <ul> <li>WebApp/HTML5 App - it is nothing but a webpage, optmized for mobile experience. Frameworks: Bootstrap, jQuery Mobile, Onesen UI.</li> <li>HTML5 - Same as website but with Local Storage, Video Streaming, Drag and Drop.</li> <li>Progressive Web App - Website with Push Notifications, Offline Work, Splash Screen and Installable with icon.</li> <li>Hybrid - Native container, installable, running webapp in webview in native container app. PhoneGap/Cordova can build container that can run webApp.</li> <li>Native Cross Platform - One code base, multi platform, native like most experience. React Native, Flutter, Xamarin.</li> <li>Native - Pure iOS and Android app. Different codebase. Full functionality.</li> </ul> <p>More:</p> <ul> <li>https://www.mobiloud.com/blog/native-web-or-hybrid-apps</li> </ul>"},{"location":"Tech%20Notes/2022-05-15-tableau-notes/","title":"Tableau Notes","text":"<p>Tableau is a data analysis and visualization tool.</p>"},{"location":"Tech%20Notes/2022-05-15-tableau-notes/#date-calculations","title":"Date Calculations","text":"<p>Here are some basic common calculations that helo in making KPIs and easy working with dates to find YOYs and MOMs</p> <ul> <li>Month-Year to Business Date <code>DATEPARSE('yyyy-MM-dd',[Business Month]+'-01')</code></li> <li>Month-Year to Business Year - <code>Left([Business Month],4)</code></li> <li> <p>Max Date - <code>{MAX([Business Date])}</code></p> </li> <li> <p>Monthly</p> </li> </ul> <pre><code>IF ( DATEDIFF('month', [Business Date], [Max Date], 'monday') = 0  AND DAY([Business Date])&lt;=DAY([Max Date]))\nTHEN 'Current Month'\nELSEIF ( DATEDIFF('month', [Business Date], [Max Date], 'monday') = 1\n//AND\n//DAY([Business Date])&lt;=DAY([Max Date])\n)\nTHEN 'Last Month'\nELSEIF ( DATEDIFF('month', [Business Date], [Max Date], 'monday') = 12 // AND DAY([Business Date])&lt;=DAY([Max Date])\n)\nTHEN 'Last Year Month'\nEND\n</code></pre> <ul> <li>Rolling 13 months</li> </ul> <pre><code>[Business Date] &gt; DATEADD('month',-13,{MAX([Business Date])})\nand\n[Business Date] &lt;= {MAX([Business Date])}\n</code></pre> <ul> <li>yearly</li> </ul> <pre><code>IF ( DATEDIFF('year', [Business Date], [Max Date], 'monday') = 0   AND MONTH([Business Date])&lt;=MONTH([Max Date]))\nTHEN 'Current Year'\nELSEIF ( DATEDIFF('year', [Business Date], [Max Date], 'monday') = 1  AND ( (  MONTH([Business Date]) &lt;= MONTH([Max Date])  ) //OR \n//( \n// ( MONTH([Business Date])=MONTH([Max Date]) ) AND ( DAY([Business Date])&lt;=DAY([Max Date]) ) \n//)\n)\n)\nTHEN 'Last Year'\nEND\n</code></pre> <ul> <li> <p>CM <code>SUM(IIF([Monthly]=='Current Month',[Closed Customers],NULL))</code></p> </li> <li> <p>MOM</p> </li> </ul> <pre><code>(\nSUM(IIF ([Monthly] == 'Current Month',[Users],0)) - SUM(IIF ([Monthly] == 'Last Month',[Users],0) ) )\n/\nSUM(IIF ([Monthly] == 'Last Month',[Users],0))\n</code></pre> <ul> <li>MOM up <code>IF [Closed Customers MOM] &gt; 0 THEN \"\u25b2\" END</code></li> <li> <p>MOM Donw <code>IF [Closed Customers MOM] &lt;= 0 THEN \"\u25bc\" END</code></p> </li> <li> <p>YOY</p> </li> </ul> <pre><code>( SUM(IIF ([Yearly] == 'Current Year',[Closed Customers],0)) - SUM(IIF ([Yearly] == 'Last Year',[Closed Customers],0) ) )\n/\nSUM(IIF ([Yearly] == 'Last Year',[Closed Customers],0))\n</code></pre> <ul> <li>YOY Up <code>IF [Closed Customers YOY] &gt; 0 THEN \"\u25b2\" END</code></li> <li>YOY Donw <code>IF [Closed Customers YOY] &lt;= 0 THEN \"\u25bc\" END</code></li> <li>YTD <code>SUM(IIF([Yearly] == 'Current Year',[Closed Customers],0))</code></li> </ul> <p>KPI Format</p> <ul> <li>small arrows <code>0%\u202f \u2bc5; -0% \u2bc6; 0%\u2800\u2800;</code> \u2bc7 \u2bc8 \u2bc5 \u2bc6</li> <li>\u2b9c \u2b9e \u2b9d \u2b9f <code>0%\u202f \u2b9d; -0% \u2b9f; 0%\u2800\u2800;</code> \\(U+2800\\)</li> <li>\ud83e\udc44 \ud83e\udc46 \ud83e\udc45 \ud83e\udc47 <code>0%\u202f \ud83e\udc45; -0% \ud83e\udc47; 0%\u2800\u2800;</code></li> <li>more arrows - http://xahlee.info/comp/unicode_arrows.html</li> </ul> <p>TUG Austia - https://github.com/tableau/community-tableau-server-insights - readymade events</p> <p>Number Tweaks</p> <p>Number standardize between 0 and 1 per category for trend line colors</p> <pre><code>(\nSUM([Value ]) - WINDOW_MIN(SUM([Value ])) )\n/\n(\nWINDOW_MAX(SUM([Value ])) - WINDOW_MIN(SUM([Value ])) )\n</code></pre>"},{"location":"Tech%20Notes/2022-05-15-tableau-notes/#links","title":"Links","text":"<ul> <li>Data Structuring for Analysis - https://help.tableau.com/current/pro/desktop/en-us/data_structure_for_analysis.htm</li> </ul>"},{"location":"Tech%20Notes/material_mkdocs/","title":"Material for MkDocs","text":"<p>all about material theme for mkdocs</p>"},{"location":"Tech%20Notes/material_mkdocs/#admonitions","title":"Admonitions","text":"Collapsible <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> Collapsible Open <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>More: https://squidfunk.github.io/mkdocs-material/reference/admonitions/#collapsible-blocks</p>"},{"location":"Tech%20Notes/material_mkdocs/#code-blocks","title":"Code Blocks","text":"<p>Inline <code>code</code> and block.</p> Code Title: bubble_sort.py<pre><code>    def bubble_sort(items):\nfor i in range(len(items)):\nfor j in range(len(items) - 1 - i):\nif items[j] &gt; items[j + 1]:\nitems[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>More: https://squidfunk.github.io/mkdocs-material/reference/code-blocks/</p>"},{"location":"Tech%20Notes/material_mkdocs/#content-tabs","title":"Content Tabs","text":"CC++ <pre><code>#include &lt;stdio.h&gt;\nint main(void) {\nprintf(\"Hello world!\\n\");\nreturn 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\nint main(void) {\nstd::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\nreturn 0;\n}\n</code></pre> <p>More: https://squidfunk.github.io/mkdocs-material/reference/content-tabs/</p>"},{"location":"Tech%20Notes/material_mkdocs/#diagrams-mermaid","title":"Diagrams Mermaid","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> <p>more: https://squidfunk.github.io/mkdocs-material/reference/diagrams/</p>"},{"location":"Tech%20Notes/material_mkdocs/#mathjax","title":"MathJax","text":"<p>Inline \\(\\alpha\\) math.</p> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>More: https://squidfunk.github.io/mkdocs-material/reference/mathjax/</p>"},{"location":"Tech%20Notes/material_mkdocs/#lists-as-tasks","title":"Lists as Tasks","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> <p>More: https://squidfunk.github.io/mkdocs-material/reference/lists/</p>"},{"location":"blog/2018-05-02-Hello-Humans/","title":"Hello Humans!","text":"<p>Here is my first post on GitHub using Jekyll and I turn to my last in twenties. Phew..! life passes fast.. isn't it?</p> <p>Well, time ticks at it's own pace. So much to learn and so less time. World is running and so are we. Run.. chase your dreams.. rise and shine.. but remember, we are humans blessed enough to enjoy mother earth. Life is a journey, enjoy all ups and downs.</p> <p>Here, I basically write notes of what I learn and what I do. This helps me refer back and keep a log of how I did something. I was saving my notes locally and randomly so thought of putting them onto GitHub to centralize them and make them available to the world.</p> <p>A few posts here might not be very well document or in polished manner as they are quick notes to refer back. However, I do include the most essential part and try not to break the flow in understanding a concept.</p> <p>Thanks for landing here. A quote from Steve Jobs' speech at Harvard:</p> <p>Stay Hungry.. Stay Foolish.. ;)</p> <p>Cheers...!</p>"},{"location":"blog/2018-06-16-github-pages-jekyll/","title":"Github Pages and Jekyll Sites - Complete Setup","text":"<p>Github Pages are static sites that can be hosted on GitHub for free. Github Pages use Jekyll (a Ruby Gem) to build static site from markdown files.</p> <ul> <li>Do not remove this line (it will not be displayed) {:toc}</li> </ul>"},{"location":"blog/2018-06-16-github-pages-jekyll/#quickest-way-to-get-started","title":"Quickest way to get started","text":"<p>Use 'Jekyll Now', it is flat 30 seconds blog setup. Follow the steps below: - You can setup Jekyll on GitHub by forking Jekyll Now repository. - The readme.md in above repository is a very good tutorial that you can follow and setup Jekyll on your GitHub account. - Modify config files and github settings as stated in above readme. - your blog is live</p> <p>With this you can use your time on writing post rather than other geeky stuff, but if you need to setup everything or if it is required your can follow setting Jekyll locally below.</p> <p>Now that blog is working, we need to write posts.</p>"},{"location":"blog/2018-06-16-github-pages-jekyll/#publishing-posts-to-the-site","title":"Publishing Posts to the Site","text":"<p>Posts can be published in 3 ways:</p> <ol> <li> <p>Directly write on GitHub.com: This is fastest way and requires no setup. You can go to <code>_posts</code> folder on this repository and create new .md file.</p> </li> <li> <p>Local MD files You can use Sublime, atom or any other text editor on your local machine and the upload it to GitHub or use Git locally then commit and push to GitHub.</p> </li> <li> <p>Local Jekyll setup You can install Jekyll locally on your machine. This will require you to install Ruby as well. Then on localhost you can render your entire website (blog) and see changes. Then you can push it to GitHub.</p> </li> </ol>"},{"location":"blog/2018-06-16-github-pages-jekyll/#setting-up-jekyll-to-run-locally","title":"Setting up Jekyll to run locally","text":"<ul> <li>You need to have ruby, gem, gcc and g++ installed. else do <code>brew install gem</code> and all.</li> <li>Then you need to install <code>gem install bundler jekyll</code></li> <li>Next, <code>gem install github-pages</code> installs all gems required by github pages, all of the dependancies you\u2019ll need, like: kramdown, jemoji, and jekyll-sitemap</li> <li><code>jekyll new my_blog</code> creates scaffold for a new site. This is all you need to do.</li> <li><code>jekyll build</code> builds</li> <li><code>jekyll serve</code> serves the site to localhost:4000.</li> <li>Detailed article on installing jekyll, here.</li> <li>Tutorial with all steps, KBRoman.</li> <li>Advanced features: If you need to extend the functionality of Jekyll posts then advanced tutorial can be found at here.</li> </ul> <p>Issues: - If you see permission issue on Mac, run using <code>sudo</code>. This may occur as gem and ruby are already installed on mac but in Library folder which is not writable. - If you want to run locally already existing site, then create a new temp blog then copy 'Gemfile' and 'Gemfile.lock'. The site root should have these files. They are required to provide all gems that Jekyll requires for proper functionality.</p>"},{"location":"blog/2018-06-16-github-pages-jekyll/#github-site-for-your-projects","title":"Github Site for your Projects","text":"<p>Github can further be used to host your projects site. This is kind of a sub-site/sub-domain of main site.</p> <p>My site: <code>myname.github.io/</code></p> <p>Project Site: <code>myname.github.io/abc_project/</code></p> <p>All projects repository come under gh-pages branch and not master.</p> <p>Creating a sub site is same as creating a main site.</p>"},{"location":"blog/2018-06-16-github-pages-jekyll/#jekyll-notes","title":"Jekyll Notes","text":"<p>Jekyll is a Ruby library to make blog and pages site.</p> <p>_config.yml has all configuration variables.</p> <p>Posts are markdown files store under _posts folder</p> <p>Pages are markdown files in root location.</p> <p>_layouts have different .html files that define the layout for example: default, pages or posts. These can include other templates from _includes folder. They have {{ content }} which gets populated by file that uses this layout. </p> <p>For eg. 'default.html' can include 'meta.html'.</p> <p>'post.html' can use 'default.html' as layout. So all code in 'post.html' will populate {{ content }} in default.html</p> <p>some_post.md can use <code>post.html</code> as layout. So all markdown from this file will be populated to {{ content }} of 'post.html'.</p> <p>To list all categories in site</p> <p>Category returns two array items, first is category name and second is another array of posts.</p> <p>Categories in site: <pre><code>{\\% for category in site.categories \\%}\n- {{ category[0] }}\n{\\% endfor \\%}\n</code></pre></p> <p></p> <p>Related post: - How to add syntax highlighting to Jekyll Sites</p>"},{"location":"blog/2018-07-01-data-wrangling-in-python/","title":"Data Wrangling in Python using Pandas","text":"<p>Pandas is a package in Python that can be used for data manipulation.</p>"},{"location":"blog/2018-07-01-data-wrangling-in-python/#what-is-data-manipulation","title":"What is Data Manipulation?","text":"<p>Data manipulations can be organized around six key verbs:</p> <ul> <li>arrange: order dataframe by index or variable or sort the data</li> <li>select: choose a specific variable or set of variables or select columns in data</li> <li>filter: subset a dataframe according to condition(s) in a variable(s) or select rows in data</li> <li>mutate: transform dataframe by adding new variables or add a calculated column</li> <li>group_by: create a grouped dataframe </li> <li>summarize: reduce variable to summary variable (e.g. mean)</li> </ul> <p>Here, variable is a column in data set.</p> <p>We'll cover how to perform above operations on a dataset using Pandas.</p>"},{"location":"blog/2018-07-01-data-wrangling-in-python/#quickest-data-in-pandas","title":"Quickest data in pandas","text":"<pre><code>text = '''colA colB\nJan 239\nFeb 234\n'''\nfrom io import StringIO\nimport pandas as pd\npd.read_csv(StringIO(text),delimiter=' ')\n</code></pre>"},{"location":"blog/2018-07-01-data-wrangling-in-python/#filter","title":"Filter","text":"<p>We can filter data to get a set of rows from complete dataset. It is similar to <code>WHERE</code> clause in SQL.</p>"},{"location":"blog/2018-07-01-data-wrangling-in-python/#doing-same-stuff-using-r","title":"Doing same stuff using R","text":"<p>R is also an excellent programming language for data manipulation. <code>dplyr</code> is a package in R that can be used to perform above operations. </p> <p>An excellent article by Ben, The 5 verbs of dplyr, can provide you more details on this.</p> <p>Another article that compares R and Python can be found here.</p> <p>Comparison of Pandas with SQL</p> <p>Pandas docs excellent details with examples.</p>"},{"location":"blog/2019-06-14-syntax-highlight-jekyll/","title":"How to add syntax highlighting to Jekyll Sites","text":"<p>Jekyll supports syntax highlighting by default using gem <code>rouge</code>. It can highlight 100 different language.</p> <p>You need to add one line in <code>config.yml</code> </p> <pre><code>highlighter: rouge\n</code></pre> <p>and need to ensure that <code>rouge</code> gem is installed. If you have forked any Jekyll site then you can skip this step, else run: <pre><code>gem install rouge\n</code></pre></p> <p>Themes: There are many themes available for syntax highlighting. They can be previewed here. They can be downloaded from here.</p> <p>I personally prefer Github flavoured theme which I downloaded from here.</p> <p>Once you have decided the theme then you can replace the file <code>_syntax-highlighting.scss</code> file located in <code>_scss</code> directory. Every Jekyll site must have this file by default.</p> <p>Please see below some of the use cases.</p> <p>Ruby:</p> <pre><code>require 'redcarpet'\nmarkdown = Redcarpet.new(\"Hello World!\")\nputs markdown.to_html\n</code></pre> <p>Python:</p> <pre><code>import numpy as np\nimport pandas as pd\ndf = pd.read_csv('employee.csv')\ndf.head()\n</code></pre> <p>HTML</p> <pre><code>&lt;head&gt;\n&lt;body&gt;\n    Hello..!\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"draft/HDPCD%20-%20rough/","title":"HDPCD","text":""},{"location":"draft/HDPCD%20-%20rough/#sqoop-notes","title":"Sqoop Notes","text":""},{"location":"draft/HDPCD%20-%20rough/#import-data-from-a-table-in-a-relational-database-into-hdfs","title":"Import data from a table in a relational database into HDFS:","text":"<pre><code>sqoop import \\\n  --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\\n  --username=retail_dba \\\n  --password=hadoop \\\n  --table departments \\\n  --as-textfile \\\n  --target-dir=/user/root/departments\n</code></pre>"},{"location":"draft/HDPCD%20-%20rough/#import-the-results-of-a-query-from-a-relational-database-into-hdfs","title":"Import the results of a query from a relational database into HDFS","text":""},{"location":"draft/HDPCD%20-%20rough/#boundary-query-and-columns","title":"Boundary Query and columns","text":"<pre><code>sqoop import \\\n  --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\\n  --username=retail_dba \\\n  --password=hadoop \\\n  --table departments \\\n  --target-dir /user/root/departments \\\n  -m 2 \\\n  --boundary-query \"select 2, 8 from departments limit 1\" \\\n  --columns department_id,department_name\n</code></pre>"},{"location":"draft/HDPCD%20-%20rough/#query-and-split-by","title":"Query and split-by","text":"<pre><code>sqoop import \\\n  --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\\n  --username=retail_dba \\\n  --password=hadoop \\\n  --query=\"select * from orders join order_items on orders.order_id = order_items.order_item_order_id where \\$CONDITIONS\" \\\n  --target-dir /user/root/order_join \\\n  --split-by order_id \\\n  --num-mappers 4\n\n  sqoop import \\\n  --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\\n  --username=retail_dba \\\n  --password=hadoop \\\n  --table departments \\\n  --target-dir /apps/hive/warehouse/retail_ods.db/departments \\\n  --append \\\n  --fields-terminated-by '|' \\\n  --lines-terminated-by '\\n' \\\n  --num-mappers 4 \\\n  --outdir java_files\n\n  sqoop import \\\n  --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\\n  --username=retail_dba \\\n  --password=hadoop \\\n  --table departments \\\n  --fields-terminated-by '|' \\\n  --lines-terminated-by '\\n' \\\n  --hive-home /apps/hive/warehouse \\\n  --hive-import \\\n  --hive-table departments_test \\\n  --create-hive-table \\\n  --outdir java_files\n\nsqoop export --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_rpt_db\" \\\n  --username retail_dba \\\n  --password hadoop \\\n  --table departments_test \\\n  --export-dir /apps/hive/warehouse/departments_test \\\n\nsqoop eval --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\\n  --username retail_dba \\\n  --password hadoop \\\n  --query \"insert into departments values (8000, 'Testing Merge')\"\n\n sqoop merge --merge-key department_id \\\n  --new-data /user/root/sqoop_merge/departments_delta \\\n  --onto /user/root/sqoop_merge/departments \\\n  --target-dir /user/root/sqoop_merge/departments_stage \\\n  --class-name departments \\\n  --jar-file /tmp/sqoop-root/compile/5952d1747b799e7ca009cb03a9378efc/departments.jar\n</code></pre>"},{"location":"draft/HDPCD%20-%20rough/#flume","title":"FLUME","text":"<p>practice from video directly</p> <p># PIG</p> <pre><code>lines = LOAD '/user/root/dummyText.txt' AS (line:chararray); \nwords = FOREACH lines GENERATE FLATTEN(TOKENIZE(line)) as word;\ngrouped = GROUP words BY word;\nwordcount = FOREACH grouped GENERATE group, COUNT(words);\nDUMP wordcount;\n\nsqoop import-all-tables \\\n  -m 1 \\\n  --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\\n  --username=retail_dba \\\n  --password=hadoop \\\n  --warehouse-dir /user/root/sqoop_import \\\n--driver com.mysql.jdbc.Driver\n\nPHONE_NUM number, PLAN number, REC_DATE number, STAUS number, BALANCE number, IMEI number, REGION varchar(30)\n\norders = LOAD 'pig_demo.orders' USING org.apache.hive.hcatalog.pig.HCatLoader();\ngrouped = GROUP orders BY order_status;\norderstatusdistinct = FOREACH grouped {\n  odistinct = DISTINCT orders.order_status;\n  GENERATE FLATTEN(odistinct);\n};\nDUMP orderstatusdistinct;\n</code></pre>"},{"location":"draft/HDPCD%20-%20rough/#hive","title":"HIVE","text":"<pre><code>select * from categories into outfile '/tmp/categories01.psv' fields terminated by '|' lines terminated by '\\n';\nselect * from customers into outfile '/tmp/customers.psv' fields terminated by '|' lines terminated by '\\n';\nselect * from departments into outfile '/tmp/departments.psv' fields terminated by '|' lines terminated by '\\n';\nselect * from products into outfile '/tmp/products.psv' fields terminated by '|' lines terminated by '\\n';\n</code></pre> <p>Like this we can export mysql table to linux files;</p>"},{"location":"draft/HDPCD%20-%20rough/#load-data-from-local-file-system-to-hive-table","title":"Load data from local file system to hive table","text":"<pre><code>load data local inpath '/tmp/categories01.psv' overwrite into table categories;\nload data local inpath '/tmp/customers.psv' overwrite into table customers;\nload data local inpath '/tmp/departments.psv' overwrite into table departments;\nload data local inpath '/tmp/products.psv' overwrite into table products;\n</code></pre> <p>You can remove overwrite while appending data to underlying hive table</p> <p>order_item_id            | int(11)    | NO   | PRI | NULL    | auto_increment | | order_item_order_id      | int(11)    | NO   |     | NULL    |                | | order_item_product_id    | int(11)    | NO   |     | NULL    |                | | order_item_quantity      | tinyint(4) | NO   |     | NULL    |                | | order_item_subtotal      | float      | NO   |     | NULL    |                | | order_item_product_price </p> <p><pre><code>order_item_order_id   int,   order_item_product_id   int, order_item_quantity     int,\norder_item_subtotal     int,\norder_item_product_price int\n</code></pre> - stage is for loading data as it is. It has all tables loaded. - ods has partition and buckets. Currently orders and order_items .</p> <p>cards  deck_of_cards  deck_of_cards_external</p> <p>default</p> <p>pig_demo  categories  customers  departments  order_items  orders  products</p> <p>retail_edw --enterprise data warehousing  order_fact  products_dimension</p> <p>retail_ods --operational data store  categories  customers  departments  order_items  order_items_bucket  orders  orders_bucket  products  --all are populated except buckets</p> <p>retail_stage --avroSerDe, avro schema not found.  departments_delta  order_items_stage --missing on git  orders_demo    orders_stage</p> <p>xademo  call_detail_records  customer_details  recharge_details</p> <p>hive_demo --TRANSACTIONS</p>"},{"location":"draft/HDPCD%20-%20rough/#insert-overwrite-table-order_items-partition-order_month-select-oiorder_item_id-oiorder_item_order_id-oorder_date-oiorder_item_product_id-oiorder_item_quantity-oiorder_item_subtotal-oiorder_item_product_price-substroorder_date-1-7-order_month-from-retail_stageorder_items_stage-oi-join-retail_stageorders_stage-o-on-oiorder_item_order_id-oorder_id-create-table-order_items_stage-order_item_id-int-order_item_order_id-int-order_item_product_id-int-order_item_quantity-int-order_item_subtotal-int-order_item_product_price-int-row-format-delimited-fields-terminated-by-stored-as-textfile-select-order_status-count1-from-orders-where-order_date-2013-12-14-000000-group-by-order_status-order-by-order_status","title":"<pre><code>insert overwrite table order_items partition (order_month)\nselect oi.order_item_id, oi.order_item_order_id, o.order_date,\noi.order_item_product_id, oi.order_item_quantity, oi.order_item_subtotal,\noi.order_item_product_price, substr(o.order_date, 1, 7)\norder_month from retail_stage.order_items_stage oi join retail_stage.orders_stage o\non oi.order_item_order_id = o.order_id;\n\ncreate table order_items_stage ( \n order_item_id int,\n order_item_order_id   int,   \n order_item_product_id   int, \n order_item_quantity     int,\n order_item_subtotal     int,\n order_item_product_price int\n )\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\n\nSELECT order_status, count(1) FROM orders\nWHERE order_date = '2013-12-14 00:00:00'\nGROUP BY order_status\nORDER BY order_status;\n</code></pre>","text":"<p>END</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/","title":"ISB AMPBA Data Science Machine Learning AI DL Notes","text":"<p>These notes will eventually go to kaggle as a data science guide.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#k-fold-cross-validation-in-python","title":"K-Fold Cross Validation in Python","text":"<p>We can improve model's performance by changing parameters of model.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#personal-leadership-isb-final-classes","title":"Personal Leadership - ISB Final Classes","text":"<p>D.V.R. Seshadri, ISB</p> <p>i am gift to the world the way I am.</p> <p>you can be a leader of yourself, no need to be MD or CEO. Coder can be a leader.</p> <p>thought - get a thought in mind, swatch bharat</p> <p>strategic - make a strategy to execute, 11m toilets</p> <p>entrepreneurial - make people build</p> <p>people leadership - educate people to use app / toilet</p> <p>self leadership - personal leadership - most required for above leaderships.</p> <p>iq - intellectual quotient eq - emotional sq - spiritual - max for personal leadership, how do you face tsunami of life?</p> <p>What make you happy</p> <ul> <li>self satisfaction</li> <li>surrounded by friends and family</li> <li>connecting with nature</li> </ul> <p>References:</p> <ul> <li>neem karoli baba and the train story</li> <li>book - The Executive and the Elephant: A Leader's Guide for Building Inner Excellence Book by Richard L. Daft</li> <li>Oxford time management - https://www.ox.ac.uk/students/academic/guidance/skills/time</li> <li>Scientist's Search for Truth Paperback \u2013 1 December 2005 by Swami Virajeshwara (Author)</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#story-telling-st","title":"Story Telling ST","text":"<p>Emotions along with the functionality for a product. Emotions create a good start.. people listen when you say I am from ISB. ST is like swimming, you have to do and learn, can't learn by watching videos.</p> <p>Story retains in mind of people, we remember thirsty crow story but not pascals law.</p> <p>Take 1: business presentation - the problem, the solution, the architecture, the impact, thank you</p> <p>Take 2: Story - here is the story of marc, marc is manager in a retail store, he spends 50% time just arranging items, and with our solution now he is free and can spend time with customers. Story can also be around Owner, Manager, Consumer or between two stores?</p> <p>Status update can be boring when BAU update is give, like business as usual.</p> <p>Add emotion in story, like Suresh ki dukaan nhi chalti thi, wo dept me gya but using Our app he has no time to sit back and has earned a car and house.</p> <p>5 ways to begin a story</p> <ul> <li>start with quote and contextualize it, connect the quote with context, take a stand on it. A quote with picture makes a big impact.</li> <li>start with a set up question</li> <li>tell an anecdote - a short amusing or interesting story about a real incident or person. specially related to audience.</li> <li>share an interesting fact or data -</li> <li>imagine a world where -</li> </ul> <p>Presentation to story is facts to emotions. checklist:</p> <ul> <li>does it has emotions</li> <li>does it build a common groung</li> <li>does it engage a visual brain?</li> <li>it is truth , well told?</li> <li>Does it have a contrast? - achoring effect?</li> </ul> <p>Develop a niggle around it.</p> <p>References:</p> <ul> <li>Obama speaks to Israel - https://www.youtube.com/watch?v=Oxfw3ZfBx6I</li> <li>https://economictimes.indiatimes.com/tech/information-tech/sebi-bans-ex-infy-wipro-staffers-in-insider-trading-case/articleshow/86632276.cms?from=mdr#:~:text=Two%20former%20employees%20of%20IT,US%2Dbased%20investment%20advisor%20Vanguard.</li> <li>hacker asks 18m - https://www.bbc.com/news/technology-53214783</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#isb-t6-pricing-analytics","title":"ISB T6 Pricing Analytics","text":"<p>Prof. Abhinav Uppal</p> <p>Course Objectives</p> <p>Pricing is one of the most powerful tools available with businesses to maximize their profits. Businesses spend a lot of resources to enhance the value of their product offerings through innovation, advertising, promotions, sales force effort and working with their channel partners. Implementing an effective pricing strategy is critical for businesses to capture that value back into the business. However, managers often under-attend to the pricing of their products, failing to tap into this opportunity to boost their profits.</p> <p>The objective of this course is to help you develop a systematic framework for assessing and formulating effective pricing strategies for businesses across a variety of contexts. Pricing decisions require careful attention to economic, marketing, organizational and psychological factors. We will learn relevant concepts and methods, and explore new approaches that will help you make better pricing decisions while keeping these factors in mind.</p> <p>The course will use a combination of lectures, in-class case discussions and assignments that will help you understand and apply the concepts that we will develop in class. The list of topics covered is detailed in the course schedule.</p> <p>Learning Goals</p> <p>Critical and Integrative Thinking: Each student shall be able to identify key issues in a business setting, and develop a perspective that is supported with relevant information and integrative thinking, to draw and assess conclusions.</p> <p>Effective Oral Communication: Each student shall be able to communicate verbally in an organized, clear and persuasive manner, and be a responsive listener.</p> <p>Interpersonal Awareness and Working in Teams: Each student shall demonstrate an ability to work effectively in a team, exhibiting behavior that reflects an understanding of the importance of individual roles and tasks, and the ability to manage conflict and compromise, so that team goals are achieved.</p> <p>Course Materials and Reference Books</p> <ul> <li>Lecture notes, cases, problem sets and supplementary readings on LMS.</li> <li>Nagle, Hogan and Zale, The Strategy and Tactics of Pricing, Pearson.</li> <li>Raju and Zhang, Smart Pricing, Prentice Hall.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-1","title":"Session 1","text":"<p>company make product, we promote to enhace value, value gets back to business for salary and all.</p> <p>pricing is balancing act.</p> <p>important now becuase of new ideas in market.</p> <p>Top-line is revenue, bottom line is the profit.</p> <p>New startups want to grow top-line, its not onlyu about profit but also about revenue.</p> <p>Role of pricing, why important:</p> <p>impact on profit by price is max, compared to fixed cost, vol, varianle cost.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#role-of-cost-in-pricing-decisions","title":"Role of cost in pricing decisions","text":"<p>when i set price i set quantity, low means more buy, high means less buy. Willingness to pay is different. quantity change brings cost. even monopolist have to consider the cost besucase of relateion with quantitu.</p> <p>Marginal Revenue is the revenue gained by producing one additional unit of a product or service. Marginal Cost is the cost added by producing one additional unit of a product or service.</p> <p>not always sure of marginal revenue.</p> <p>costs:</p> <ul> <li>fixed w.r.t. quantity sold</li> </ul> <p>what ever is the cost we have to bring that cost out from market by pricing.</p> <p>when we are giving discount then also we make adv and that incurs cost.</p> <p>Book publisher eg:</p> <p>selling will reduce inventory cost, cost to store the books.</p> <p>it can bring in opportunity cost, like sell another book which can give more profit..</p> <p>consultant missing:</p> <ul> <li>consumer will wait for sale.</li> </ul> <p>Linking cost structure to pricing and profit</p> <ul> <li>price elasticity (of demand): how demand changes by price.</li> </ul> <p>PED = delta demand % / delta price %</p> <p>PED &gt; 1 elastic, else inelastic</p> <p>elasticity of revenue EOR:</p> <p>% chng in revenue  / % chng in price</p> <p>EOR = 1 + EOD</p> <p>in elastic demand revenue inc with proce ince.. else opposit,</p> <p>break even profit elasticity, = -1, EOR = 0/</p> <p>where price change will work:</p> <p>Break even Profit elasticiy = -1 / (current % margin * % price change)</p> <p>hence, price change = % delta Q / % delta P</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#value-supremem-example","title":"Value supremem example","text":"<p>sell chicken below whole sale, chicken sale is related to ther prodcuts, which have difffernet % margin,</p> <p>how much more chicken to sell?</p> <p>2.2 is ratio, make 2.2 times the quantity,</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#price-sensitivity","title":"price sensitivity","text":"<ul> <li>customer service.</li> </ul> <p>Price elasticity of demand = % delta Q/ % delta p. compute elsticity dematn</p> <p>by using regression, linear. get price elasticity number.</p> <p>using regression unit sales against price.</p> <p>eg, quantity = b0 + b1 price</p> <p>price coeff b1 is +ve, generally -ve. price up qnatity down.</p> <p>+ve it can be only fo rluxury good. price high then demand high. status changes fo prodcut.</p> <p>if we add week to analysis then, price coeff goes -ve</p> <p>we can detrend the data to improve model further.</p> <p>Let also see log log model, then find price elasticiy.</p> <p>Elasticity?  detrend data with time.</p> <p>linear demad =</p> \\[ q_i = a - bp_i + ct_i + e_i \\] <p>=&gt; POD = -bp/q</p> <p>in linear demand, price elsticity is not constant, change with price level.</p> <p>for last record, pod = -0.47 &lt;= -38 * 12.5/1000</p> <p>pod = -0.47, is less than 1 hence inelastic</p> <p>since in elastic , henc to increase revenue, increase price.</p> <p>by how much to inc, till you reach 1.</p> <p>log-log model</p> <p>log q = log a -b log q</p> <p>=&gt; POD = -b</p> <p>Analysis:</p> <ul> <li>hsitorically get contect,</li> <li>but real world prob needs to be handeled , be aware.</li> </ul> <p>-</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-2","title":"Session 2","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#effect-of-cost-change-on-price","title":"Effect of cost change on price","text":"<p>Google it we looked at brek even profit elsticity convept.</p> <p>what happens when cost changes for a firm?</p> <p>fuel amy go up, cost amy inc. what proportion change is passed on to customers, up or down.</p> <p>demand = p = 500 - 10q</p> <p>marginal revenue is kitna milega, marginal cost is kitna lagega, on chaging quantity.</p> <p>if marginal revenue = marginal cost, then profit is max.</p> <p>if MR &gt; MC, then we cna dec price to sell more.</p> <p>if MR &lt; MC, then inc price.</p> <p>COncave convex demand cureve had different changes on price change.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#quanitty-discounts","title":"Quanitty discounts","text":"<p>buy 2 get one free. big maggie price, more discount.</p> <p>the more you sell, the more margin you make</p> <p>sell more bring production cost down.</p> <p>double marginalization prob - google it.</p> <p>manufacturer gives quantity doscoutn to retailer.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#two-part-pricing","title":"Two part pricing","text":"<p>above cna be solved by this., tow pricing part, pfront free, then additino fee, which is usage fee. Like cab and telecom, fixed + Usage charges.</p> <p>not always to be used, the person who uses more can have more upfront fix price.</p> <p>Xerox prob:</p> <ul> <li>two groups, with different usgae and pricing, find optimal price for them, to two stage pricing?</li> </ul> <p>this tells why one can go after only premium segment.</p> <p>We can equate to find optimal , R and p, which is fixed cost and price of a copy.</p> <p>so by two part pricing i am bale to extract willngness to pay from two differnet segmaetns, hence mroe profit.</p> <p>if we have 3 segments, then we can have more var introduced, or increase part pricing, how muc surplus you cna extract. Single pricing, flat, like netflix also has benefit and varibale multi-part fee also has benefit, it all depends on cases.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-3-2hr","title":"Session 3 (2hr)","text":"<p>Product Line Pricing</p> <p>We hvae different type, iphones, oyos, cards, dropbox plans,</p> <p>Contemporaneous v s tempopral</p> <p>temporal price discrimination ,here consumers are separated into different groups with different demand elasticities by charging different price at different points in time.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#cambridge-software","title":"Cambridge software","text":"<p>produt line and pricing?</p> <p>Assumptions:</p> <ul> <li>student version can be bought by anyobne</li> </ul> <p>for different software versions, we can find:</p> <ul> <li>price for different segment</li> <li>then weather differetn segment will buy or not,</li> <li>the quantity, variable cost, sidtribution cost, fixed cost and finally profit.</li> <li>then in each version, we get max profit segment.</li> </ul> <p>max profit doesnot mean the version is profitable and should be developed.</p> <p>a segment may be willing to buy but weather we should develop that segment or not is diff que.</p> <p>Surplus = WTP - Cost, determines weather one will buy and will we produe that version.</p> <p>other approach is, depends on decision rule, marketing team can decide the decision to weather keep only corp or student, weather to keep big version only or studen tversion only.</p> <p>There may be strategic resons and competitiors to consider, so we sould do some what-if analysis and look at competitiors and then change our considerations to cover versions that our competitors could target.</p> <p>Data needed is, WTP of all segments across all versions, not that studen tonly for student version.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#max-willing-to-pay-how-to-measure","title":"Max Willing to Pay, how to measure?","text":"<p>1 identify attributes - research, market, domain language,</p> <p>the attributes we take are usally max and min, like max 5000 songs and min 50 songs.</p> <p>2 create profiles,</p> <p>different values for different attributes,</p> <p>3 code values, high level is 1 and low level is 0</p> <p>not samples are collection of ones and zeros.</p> <p>4 collect data</p> <p>create 16 profiles and ask for liking from people</p> <p>so ask for rank ordering paired comparisions, ratings.</p> <p>NOW, if we have 6 atts, we need at least  samples to say about each of these atts. so 7 are enough, beause it can cover all scenarios.</p> <p>many samples make it difficult to capture the data.</p> <p>eg, 16 profile and ask for rating,</p> <p>then run regression,</p> <p>betas are partworths.</p> <p>rating as a function of other attributes.</p> <p>step 7, beta_price is reduce in utility when price goes from 249 to 99.</p> <p>26 ratings go down if we inc price from 99 to 249</p> <p>this way we get price per utility, we can then create models/sampes</p> <p>and find total utility adn then mul by price per utility to get WTP.</p> <p>Step 8 now WTP across customers</p> <p>so we get ratings from different customers, we do get WTP for each customer, this can vary for different cust segments, then we can find WTP for different segments and do segment analysis,</p> <p>Step 9: WTP customer seg table.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-4-2hr","title":"Session 4 2hr","text":"<p>Temporal Pricing Strategies</p> <p>strategic price changes over time</p> <p>coupons, discounts</p> <p>skimming - higher to lower price, like electronics, apparels.</p> <p>penetration pricing - is minimum price o</p> <ul> <li> <p>network effect is that the more number of people on a service the more value it has, eg, dropbox, uber, paytm etc.</p> </li> <li> <p>we can use oenetration princing when n/w effect is +ve</p> </li> </ul> <p>Short-term price discount:</p> <ul> <li>price discount can be passed on to consumer only if it is in high demand compared ot competitior.</li> </ul> <p>Problem Set 3:</p> <ul> <li> <p>marginal cost = $2</p> </li> <li> <p>fixed cost = $25</p> </li> <li> <p>w = whole seller price = 19.5</p> </li> <li> <p>profit_retailer = (p-w) D(p) -25</p> </li> </ul> <p>= Profit_manufacturer = (w-2) D(p)</p> <p>now we have size changed to 20, 80</p> <p>now we ahve to give discount.</p> <p>we have retailer and manufacturer profits</p> <p>we canc alcualte price by makign profit of retailer 0.</p> <p>manufacture ahs to be congnizant of what retailer can  do,.</p> <p>we need to set price such that both manufacturer and retailer make profit. manufacturer to make max profit and followed by retailer.</p> <p>Let $c is coupon price.</p> <p>if coupon is redeemed by both segments, then it is worth it.</p> <p>Hence, by price discrimination and coupon strategy we can make manufacturer's profit high and retailer's low..</p> <p>Game Theory : you respond to what your competitors are doing.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#isb-t6-financial-analytics","title":"ISB T6 Financial Analytics","text":"<p>By: Ramana Sonti | Email: ramana_sonti@isb.edu</p> <p>Course objective: The objective of the course is to provide \u201cnew ways of thinking about risk, randomness and investments\u201d. Though the material is always set in the context of finance, almost all the content is directly relevant to any application setting.</p> <p>The course is structured also as an introduction to analytics in Finance. We will consider some of the most prominent quantitative finance problems. Each problem will be motivated and formulated along with a discussion of relevant theory.</p> <p>It will help students develop basic skills in financial modeling, problem solving and quantitative analysis of risk-return tradeo\ufb00s. Besides, it is a good primer on using data science tools to finance problems.</p> <p>We will mainly cover portfolio selection, the Capital Asset Pricing Model (CAPM), and option pricing. Time permitting, we will also look at wrangling and analyzing financial data.</p> <p>Suggested books</p> <ul> <li>[BKMM] \u201cInvestments\u201d by Bodie, Kane, Marcus, and Mohanty, 8th ed, 2009, McGraw Hill</li> <li>[DL] \u201cInvestment Science\u201d by David Luenberger; Oxford University Press, 2013. (or earlier editions)</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-1_1","title":"Session 1","text":"<p>Old type of finance analytics. we still use linear regression. not really need bayesian DL.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#risk-and-return","title":"Risk and return","text":"<p>more risk is more return, but in structured format,</p> <p>we dont spend today to save for tomorrow, that's investment.</p> <p>we get return, that is mean over time. so 22% return mean the return is distributed around 22%.</p> <p>we can find variance adn st dev based on the probabilities of return.</p> <p>st dev is kind of risk. it tells how much our return will fluctuate. so with same risk, more return si more happiness.</p> <p>as risk increases more and more return is demanded.</p> <p>casino is different, we know we will loose and go for fun.</p> <p>invsestment depends on only two things (expected return and risk)</p> <p>function is nothing but estimation of reality, it is mathematical model.</p> <p>it is all compenstion for the risk.</p> <p>when combined we find exp return and exp st dev</p> <p>Risky asset weight is how much we want to invest in risky asset. 1 means all in risky.</p> <p>for portfolio, st dev is linear func. exp return is linear in W. resiltant, is also linear. all possible portfolio combinations lie on the straight line.</p> <p>leverage means borrowing.</p> <p>que is which point on line should be picked.</p> <p>risk aversion - disliking risk. (A)</p> <p>now optimize this, $w^* $ is optimal weight in risky asset.</p> <p>aspirations meet poortunities:</p> <p>line is all possible combi of risk-free and risky investment, grey line.</p> <p>we are finding optimal point in this line.</p> <p>curve is indifference curve.</p> <p>we find points where curve cuts line. that is the points which are feasible and makes one happy.</p> <p>There is not just one risky asset, there are many.</p> <p>do we even know A?</p> <p>w* goes down, as A goes up, as age goes up.</p> <p>Let us say, A is constant.</p> <p>tomorrow, multiple risky assest. we will take n risky and boil it to one risky portfolio.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-2_1","title":"Session 2","text":"<p>We cnnnot exactly model or make a fucntion of utility, hapiness etc but people are trying.</p> <p>Two asset protfolio:</p> <ul> <li>if the correlation is low between two assets then the portfolio gives good return. but correlation is not in our hand.</li> <li>all possible portfolio options are on the curve.</li> </ul> <p>Three asset portfolio:</p> <ul> <li>if we add z, then the possible portfolios will be on the surface.</li> </ul> <p>Short selling is selling something you dont own. tht is like borrwo and sell it now. people do it to gain by using price fluctuations. Like if TV is going to be chaep in future, then sell it for 1lks now and later when it costs 50k then buy and give abck to lender.</p> <p>as the number of assets increase, the std dev becomes asymtotic (converges infinitely) at a value.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-3","title":"Session 3","text":"<p>Mental Accounts and Portfolio Optimization</p> \\[ P(r_{ref} &lt; 5\\%) \\leq 10\\% \\] <p>ask the client this question.</p> <p>bequest is heritasge</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-4","title":"Session 4","text":"<p>diversification is acceptable every where. what is compensation for risk.</p> <p>The Capital Asset Pricing Model (CAPM)</p> <ul> <li>gives compensation for systematic risk only.</li> </ul> \\[ E(R_{i}) = R_{f}+\\beta_{i}(E(R_{m})-R_{f}) \\] <p>\\(E(R_{i})\\)  = capital asset expected return</p> <p>\\(R_{f}\\) = risk-free rate of interest</p> <p>\\(\\beta_i\\) = sensitivity</p> <p>\\(E(R_{m})\\)  = expected return of the market</p> <p>this eq says if you invest in only one stock then you get compensation only of systematic invt of that stock.</p> <p>Infy up down is risk. Compensation is systematic portion, i.e., pandemic, gdp etc, this goes with whole market. They will affect infy with market, this is \\(\\beta\\). now much infy is related to these overall fluctuations is \\(\\beta\\).</p> <p>for eg, hotels are most affected by pandemic and economy, and gdp and recession.</p> <p>insurance should be separate from investment.</p> <p>Comparing portfolio std dev and beta:</p> <ul> <li>CAPM satisfies all assets, hence all points are on line.</li> <li>if P Q R have same return but diff st dev, then they have same beta.</li> <li>ONLY risk that matters is the SYSTEMATIC RISK.</li> </ul> <p>Uses of CAPM</p> <p>adjusted colisng price of stock is the true return, they adjust the price going backwards.</p> <p>risk = sys risk + co of risk</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-5","title":"Session 5","text":"<p>options,</p> <ul> <li>risk transfer instruments,</li> <li>call/puts</li> </ul> <p>option is always good but comes at a cost and they are very useful.</p> <p>eg, Insurane has premium,</p> <p>Option pricing:</p> <ul> <li>what price should an option have.</li> <li>what should be the premium price.</li> </ul> <p>The Black-Scholes Model.</p> <p>we cannot make prices normally distributed as norml dist is from - infinity to + infinty</p> <p>similarly return can max go to -100% and not beyond that.</p> <p>log-normal is something of which the log is a normal distribution.</p> <p>Volatilty Index: https://www.google.com/search?q=volatility+index&amp;oq=volatility+&amp;aqs=chrome.2.69i57j0i433l2j0l4j0i131i433j0l2.3861j0j7&amp;sourceid=chrome&amp;ie=UTF-8</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#isb-t6-application-of-artificial-intelligence","title":"ISB T6 Application of Artificial Intelligence","text":"<p>Instructor: Dr. Manish Gupta | Email id: manish_gupta@isb.edu | TA: Vishal Siram | Email id: aai_ampba@isb.edu</p> <p>Course Objective: The goal of this course is to introduce students to latest technologies which could be very relevant towards their AMPBA project as well as further work on data science in industry. Specifically, in this course, I will focus on topics like chatbots, recommendation systems, word embedding methods, and image/video analytics. While the first two topics do not involve any deep learning, word embedding methods, and image/video analytics will be further extensions of topics that were covered as part of the basic deep learning course. Specifically, word embedding methods are focused on advanced deep learning for NLP/text while image/video analytics will be focused on advanced deep learning for vision. We will discuss recently proposed methods and algorithms which have disrupted businesses significantly and have a strong potential.</p> <p>The objective of the course is to enable the students to</p> <ul> <li>How to build simple chatbots very easily. How to integrate a question-answer knowledge base with bots.</li> <li>Understand advanced architectures in deep learning and their business applications.</li> <li>How to harness deep learning techniques for image/video analytics use cases.</li> <li>What are the best ways to capture semantics of words.</li> <li>How to build basic recommendation systems with hands-on code.</li> <li>Prerequisites: Python, little bit of Tensorflow/Keras/PyTorch/CNTK, knowledge of basic neural architectures like CNNs and LSTMs.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-1-recommendation-systems","title":"Session-1 Recommendation Systems","text":"<p>optimization/ linear programming / quadratic programming is often required by logistic companies.</p> <p>collaborative filtering and K-neareest neighbours are  alcmost similar algos.</p> <p>idea: fake people army</p> <p>read more/: neural colaboraive filering.</p> <p>CF prob:</p> <ul> <li>cold start</li> <li>sparsity</li> <li>scaling is also an issue</li> </ul> <p>how to do map-red in CF.</p> <p>hybrid recommendation methods:</p> <ul> <li>for cold start - start with popular items, then switch to CF</li> <li>mixed: several type of recommendations</li> <li>cascade: funnel kind of, hig recall, low precesion. top is low precesion, bottom is hig preceison recommendations.</li> </ul> <p>Content recommendation videos</p> <ul> <li>receent - like old new people dont want</li> <li>diverse - not really imp in search but is good in recommendations</li> <li>relevance - user session activity matters,</li> <li>explaination - why recommended for a user, increases change of clicking.</li> </ul> <p>rating matrix - nobody rates youtube video for eg. If user liked shared commented etc. ANy interactions can help score a video, thus we can recommend it to users.</p> <p>Tag Recommendations:</p> <ul> <li>based on one tag, we can recommend related tags to users. eg, barcelona the recommend spain.</li> </ul> <p>-</p> <p>user get info from ads being shown on website, ads are shown by google ad exchange.</p> <p>Explaination Types:</p> <ul> <li>Nearest neigbour explaination,</li> <li>content based explaination - your read sports news, hence sports article is recommmended.</li> <li>social based - if friend reads the you will too,</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#evaluation-of-recommendation-systems","title":"Evaluation of recommendation systems","text":"<p>Offline:</p> <ul> <li> <p>imp to measure accuracy. check interaction on recommended things.</p> </li> <li> <p>novelty an dexploration , user searched / not but recommended.</p> </li> <li> <p>recommnedations is personalization but then it has bad impact as well as it limits content based on someone else's eye.</p> </li> <li> <p>widget should not be heavy to load.</p> </li> <li> <p>interaction should be quick, like quick look a post/ connect a friend with just one click,</p> </li> </ul> <p>References:</p> <ul> <li>https://recsys.acm.org/ Recommendation Research</li> </ul> <p>cat u.data|cut -f3|sort  - group by column 3 and give count.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-2-word-embeddings","title":"Session-2 Word Embeddings","text":"<p>\"In natural language processing, Word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.\" - Wiki</p> <p>NN has n/w and we need to feed sentense hence, word embeddings.</p> <p>1hot encoding</p> <ul> <li>one word embeddingis 1 at word place and else 0.</li> <li>sparse prob</li> <li>similarity cnanot be established.</li> <li>cosine similarity  = 0</li> </ul> <p>hence we use dense representation of words.</p> <ul> <li>they are short</li> <li>they are not sparse</li> <li>they carry meaning, similarity can be established</li> <li>how word2vec is done:</li> <li>they are pre-trained model</li> <li>pretrained by google, 3 billion words.</li> <li>it has phrases as well, new york</li> <li>build on 100 billion news dataset words</li> <li>vector size is 300 dims.</li> <li>they are trained using shallow NN.</li> <li>started in 2013, has just one hidden layer.</li> <li>it is quick to learn these networks because o fshallo, 1 hidden layer.</li> <li>CBOW, Skipgram?</li> <li>after entire learning we get weight matrix, which is vocal by 300.</li> <li>in skipgram we try to find related word to a word. it needs less data to learn framing.</li> <li>windosize is a paramenter, usually 5.</li> <li>we can enable stop words, stem words etc, these are all params.</li> <li>gensim is package for this.</li> <li>300 dim is for one word 'King'</li> <li>cosine similarity can be found b/w words</li> <li>we can visualize this to see similar words closed to each other on x-y plane.</li> <li> <p>it aut0 creates dictionary of similarity from any corpus.</p> </li> <li> <p>we can do math, like, male-female.</p> </li> </ul> <p>Word Vectors - GloVe</p> <ul> <li>improves similarity, words can be similar not only if they occur in same sentence, but other sentenses as well.</li> <li>trained on wikipedia,</li> <li> <p>much better quality than word2vec.</p> </li> <li> <p>word2vec used for sentiment analysis,</p> </li> <li> <p>outofvocabWord - word may be in test data but not in train, so we use fastText</p> </li> </ul> <p>FastText</p> <ul> <li>use embeddings for characters, word is made of parts, prefix, suffix, word root etc.</li> <li>combines skip-gram with sub-word model.</li> </ul> <p>there are many models, recent ones use transformers, deep models, \\~20 hidden layers.</p> <p>PYNB----mahabharat file</p> <p>ELMO (Embeddings form Language models)</p> <p>ELMO uses biLM (bidirectional language model)</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#transformers","title":"Transformers","text":"<p>good for 512 inputs only.</p> <p>BERT:</p> <ul> <li>transformer is broken into encoder and decoder, encoder is BERT, decoder is GPT (genreatlised pre trained transforms).</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-3-image-analytics-1","title":"Session-3 Image analytics 1","text":"<p>Inception - groups pooling and filter, calls it inception module and puts them in b/w the POOLs.</p> <p>CNNs, conv &gt; Relu &gt; pool</p> <p>Grouped Convolutions:</p> <p>Depth Wise CNNs:</p> <ul> <li>dividing imgae into groups and using groups</li> </ul> <p>PNASNet-5</p> <p>This was not decided but build using neural architecture search.</p> <p>we can do reserch or we cannot do grid search because of husge params and layers. But we do random seacrh.</p> <p>Genetics is followed by evolution. it is an iterative process. Chromosome is collection of genes.</p> <p>it has depth wise separable convolution.</p> <p>we only change last 4 layers of n/w to fine tune recognition.</p> <p>fastAI is on top of pytorch and keras.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-4-image-analytics-2","title":"Session-4 Image analytics 2","text":"<p>Object detection algos:</p> <ul> <li>R-CNN</li> <li>SSD</li> <li>YOLO</li> </ul> <p>object detection framework:</p> <ul> <li>regional detection</li> <li>what is a region,</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-5-video-analytics","title":"Session-5 Video analytics","text":"<p>Dialated CNN have 0 is between the filter. they are common for segmentation purpose.</p> <p>Deeplab is atrous + multi Scale + CFRs</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#crf-conditional-random-fields","title":"CRF - conditional random fields","text":"<p>HMMs are CRFs.</p> <p>Part of speech tagging, before DL, was done using multi class (36) classification.</p> <p>So likely after adjective we have noun. They helped in sequential data.</p> <p>CRFs are cheaper compared to DL and still work well.</p> <p>CRFs work by bringing joint speech tag. All words get prediction jointly. similarly we don't get label for eah pixed but grouped pixels.</p> <p>read more on google.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#deeplab","title":"DeepLab","text":"<p>in 3rd conv layer, you have a receptive field, but if we need to vary this, we change violation rate. for each filter, we change by adding 0, to make filter large, and this changing resolution we are looking in input image.</p> <p>Atrus special pyramid pooling (ASPP) is build after this.</p> <p>input - cnn with atrus - conv output mid level - upsmapling (bilinear interpolation or DL way of upsampling) - fine grain not taken care - use fully connected CRF to bring out final details.</p> <p>atrus with rate 1 is simple conv. rate 1 = no zero added. rate 2 = 1 zero added</p> <p>rate r = r-1 zeros b/w filter values.</p> <p>thus receptive field becomes larger.</p> <p>DeepLab used VGG-16 with 'fc6' layer, with fully convlutional n/w. they dialated conv layer. we do dialation here, and 'fc6' give good accuracy.</p> <p>multi scale because we can change the scale at which we process the image.</p> <p>we do max-pooling on top of it to get the final output.</p> <p>Rate = 6 maeans dialation rate. so has 5 zeros.</p> <p>CRF do joint labeling so we do interation to get sharper segmentaiton.</p> <p>Fully connected CRF give more crisp segmentation.</p> <p>More examples to follow.</p> <p>code as well to lookup.</p> <p>assignment will cover the notebook.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#part-1-assigment","title":"Part 1 Assigment","text":"<p>group 2-3, do on colab</p> <p>submit py only from colab notebook.</p> <p>use logistic regression for 5 classes.</p> <p>train word2vec then classify</p> <p>use test only for scoring.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#visual-question-answering","title":"Visual question answering","text":"<p>interesting becuse of multi-modal aspect. it is image + text. ans cna be binary or logns asnwer.</p> <p>we cna combine object detection with knowledge base like wiki to ans the quesions.</p> <p>we do this in closed domain. where vocal is small.</p> <p>Challenges in VQA</p> <ul> <li>labeled data is created</li> <li>combining modalities, CNN for image, LSTM for text.</li> <li>attension - what part of picture is being asked for?</li> </ul> <p>mostly binary ques have ans yes. that's how people ask. So the trained ata has biad prob.</p> <p>Applications are anything you imageine.</p> <p>Visual DataSet Visual7W:</p> <ul> <li>had abastract image to manage bias by adding images with ans 'no'</li> </ul> <p>VQA Models: LSTM Q + norm L.</p> <p>. . .</p> <p>Hard and Soft Attention:</p> <ul> <li>sum of all weight is 1.</li> <li>a block might be left out, so for this, they made the sum add up to 1.</li> </ul> <p>model-decoder model was changed to have alpha ti have sum equal to 1.</p> <p>using hard and soft we can have differntent caption for the images.</p> <p>insome case soft would be better and in some hard.</p> <p>more examples to look at.</p> <p>exam, May first week.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#isb-t6-digital-media-analytics","title":"ISB T6 Digital Media Analytics","text":"<p>Instructor: Prof. Madhu Viswanathan | Email: Madhu_Viswanathan@isb.edu | TA: Shreya Singireddy  </p> <p>Course Description: This course will provide a foundation to explore and study the ever-evolving and fast-paced field of digital and social media marketing. We will learn about how the digital economy works, online advertising, and its relationship to basic theories of advertising. While doing so, we will also investigate how to causally measure advertising effects using randomized clinical trials (RCTs) and field experiments.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-1-introduction-to-digital-marketing-and-web-analytics","title":"Session 1: Introduction to Digital Marketing and Web Analytics","text":"<p>traditional marketing -&gt; digital marketing</p> <p>things have changed, focus has shifted form prod centric to consumer centric.</p> <p>customer will tell what is annoying them but not what what the problem is. you will have to find the problem.</p> <p>dont start with product, start with problem,them make s/q to fit the eco-system.</p> <p>customer personas  - needs wants and so on.</p> <p>audience tribes - privacy is an issue. build segmentation of customers based on behaviour. eg, netflix.</p> <p>customer decision journey - how customer walks from awareness to conversion. how interacts iwth different channels, like sales. this is cust journey. the behaviour comes in.</p> <p>Digital Channels:</p> <ul> <li> <p>Search - google</p> </li> <li> <p>Display - advt. on apps and web.</p> </li> <li> <p>Social Media -</p> </li> <li> <p>mobile - people use a lot</p> </li> </ul> <p>digital helps measure things. makes desicaon data backed.</p> <p>Challenges - gdpr, ott laws.</p> <p>ar - ve have improved.</p> <p>evolution is there , be prepared for changes. () voo</p> <p>google changed our behaviour.</p> <p>locatoion based targeting, like airtel putting tents.</p> <p>STP: Segemtnatioj, targeting and positioning.</p> <p>zzzzzZZZZ</p> <p>Challenges today:</p> <ul> <li>lot of content</li> <li>lot of people, videos etc.</li> </ul> <p>make money from the content, only then content matters.</p> <p>newspapers are dying because marriage, jobs, real state have moved to their own apps.</p> <p>digital marketing:</p> <ul> <li>paid - web, social, influential, lead targeting</li> <li>owned - website, mobile, blog.</li> <li>earned - shares, mentions, reposts, reviews.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-2-display-advertisements","title":"Session 2: Display Advertisements","text":"<p>persona behaviour journey</p> <p>pyscology is how ppl think of prod and services.</p> <p>who is cust?</p> <ul> <li>many details like gneder, age, ocupation etc.</li> </ul> <p>who is prod for?</p> <ul> <li>individuals are different, stereotypes, caring, harsh etc. batchelors are different.</li> <li>is it married men of a kind?</li> </ul> <p>what is differene about the customer?</p> <ul> <li>stereotypes, forward moving</li> </ul> <p>Interact customers effectively and efficiently:</p> <ul> <li>we need to be strategic about digital.</li> <li>you need to know where to cut.</li> </ul> <p>Consumer deision Journey</p> <ul> <li>framework , current process, loyalty loop.</li> <li>old way to generate need was advertising.</li> <li>people do info search by frnds whoc have bought a car.</li> <li>people evaluate by brochures of cars.</li> <li>then make a choice.</li> <li>then people evaluate, is car good or bad.</li> </ul> <p>per================================== Consideration set is a set of prod a cust focuses at, eg, brezza, eco, duster.</p> <p>this is funnel model, keep decreasing the coices in the set.</p> <p>Now, in digital world, things have changed, ad is not really required. need is generated on its own. and people do google serch.</p> <p>80% cust are looking aroudn even after purchasing prob. bust returnt he prod.</p> <p>google - mement of thruth is hapamrning,</p> <p>zzzzZzzzZZZZ</p> <p>prospect of a customer: we neee ots6hh o</p> <p>all tribe hows a  hierarchy</p> <p>peopelc can be in multiple tribes.</p> <p>tribes can be like, cookig, driving, etc.</p> <p>you need to have attribution srategies, what channels matter, mail, social etc.</p> <p>listening is big succes.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#search-engine-marketing","title":"Search Engine Marketing","text":"<p>seo is all about ranking, amazon has stepped up as a prod search engine.</p> <p>Google algo for SEO:</p> <ul> <li>unique content</li> <li>penalty for duplicate content</li> <li>inbound link, other websites that link to your website.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-3-role-of-social-media","title":"Session 3: Role of social media","text":"<p>Social media analytics:</p> <ul> <li>content analysis</li> <li>influencial marketing</li> <li>how to find influencer</li> </ul> <p>Group assignment will be based on this, find influencer based on data.</p> <p>Sentiment of tweets for airlines;</p> <ul> <li>more number of tweets is more negative.</li> <li>people hardly post =ve tweets.</li> <li>this score/ranking was similar to ASCI (american customer satisfaction index)</li> </ul> <p>sentiment is just like themrmometer for fever, it tells symptoms and then we have to dig further if required.</p> <p>Sentiment Anlaysis on Warren Buffet's letter to subscribers:</p> <ul> <li>It's result showed that it was related to market events, like crash in 1987, .com y2k in 2001, 2002 9/11. This was alos simple sentiment that gave us insights.</li> </ul> <p>7 steps framework is used to find influencers.</p> <p>Influencers are linked to words, categories, markets. We have to find that stickiness index.</p> <p>Process of measuring give, customer influence value (CIV) and customer lifetime value (CLV).</p> <p>4 ideal chards:</p> <ul> <li>activeness: how engages.</li> <li>clout - reachabluity</li> <li>talkativeness - retweet, reshare</li> <li>likemindedness - similarity, like amitabh is more followed and people will buy it.</li> </ul> <p>Value of facebook like:</p> <ul> <li>run the experiment, identify customers, send invite to join fb page to a few customers. then look at the sales numbers. If there are postings again ,look at the likes. now we have group of people, those who ike and buy and those who don;t like but buy.</li> </ul> <p>fb page has no value of page.</p> <p>design is automatically done if we know the problem.</p> <p>eBay Search Ad effectiveness</p> <ul> <li>spent a lot on search ad.</li> <li>roi was 300%.</li> <li>paper: https://faculty.haas.berkeley.edu/stadelis/Tadelis.pdf</li> </ul> <p>ebay Ad test:-</p> <ul> <li>they stopped paid search and the sales was still same.</li> </ul> <p>for ebay it was a scenario but for some other brand paid search may work.</p> <p>experimentation is vital in digi world to figure out the ROI on ads, pages etc. Influence was lower as could be seen from data.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-4-introduction-to-ab-testing","title":"Session 4: Introduction to A/B testing","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-5-analytics-of-digital-attribution","title":"Session 5: Analytics of Digital Attribution","text":"<p>Correlation vs Causation</p> <ul> <li>cops vs crime is not causal, but correlated, high crime has hgih cops.</li> <li>cusality can be used to target audience</li> </ul> <p>Controlled Test</p> <p>-</p> <p>Experiments are good when we have specific questions.</p> <p>DMA Measurements - ROI of DM:</p> <ul> <li>organize the data, solves 75% probs</li> <li>experiment, turn off ad, try A/B testing</li> <li>try to find the best combo</li> <li>Apply simple stats, do regressions.</li> </ul> <p>Attribution Model:</p> <ul> <li>display first, all credit goes to display</li> <li>last click models, search will get all craadits.</li> </ul> <p>these are worst assesement.</p> <p>https://sudhir-voleti.shinyapps.io/CustomerAttribution/</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#isb-t5-deep-learning","title":"ISB T5 Deep Learning","text":"<p>Session 1:</p> <ul> <li>Autoencode is only unsipervised DL others are supervised.</li> <li>Transformers, Birt and BPT3 are new algos</li> </ul> <p>Deep Learning mimics the human brain. Neurons take input, mul by weights, then sum it up. and based on threshhold it outputs erither 0 or 1</p> <p>IRIS example, it has 4 features, hence 4 inputs suppose it outputs 1 if versicolor else 0. Them it sums all inputs multiplied by weights</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#the-artificial-neuron","title":"The Artificial Neuron","text":"<p>It has  a input vector, \\(\\overrightarrow{x}\\), it is a row of features, eg, iris row with SL PL SW PW. Input vector has x1, x2, ... , xM. All are multipied by weights w1, w2, ... , wm. These are then aggregated by function <code>f()</code> called INtegration Function and then this passes through another function call Activation Function <code>a()</code>, this give output <code>y</code>. a() has the thresholding logic.</p> <p>Integration Function, f(.) :</p> \\[ f(\\overrightarrow{x}) =  \\sum W_i x_i &lt;&gt;= \\theta \\] <p>This function aggregates signals from other neurons, boosts or supresses the inputs.</p> <p>Activation Function, a(.) :</p> <p>This function outputs y based on threshold. It can be</p> <ul> <li>step, it is either 0 or 1.</li> <li>ramp, for x = 0 to 1 it is slope</li> <li>sigmoid - used to have gradients in output, b/w 0and 1.</li> <li>Tanh - used wehn we need garadients and values between -1 and 1.</li> </ul> <p>The sudden change in function is a problem for derivative. Hence, we use sigmoid or tanh.</p> <p>Rectified Linear activation function or ReLU is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.</p> \\[ f(x)= max(0,x) \\] <p>A smooth approximation (softplus) to the rectifier is the analytic function: $ f(x)=log(1+exp(x)) $. Derivative of softplus is the logistic function.</p> <p>Perceptron algorithm:</p> <p>Iterative algo to learn weight vector. Update weights in proportion to the error contributed by inputs.</p> <ul> <li>Randomly initialize the weight vector</li> <li>repeat until error is less than threshold \\(\\gamma\\) or max iterations M.</li> </ul> <p>\\(t_i\\) is expected value, \\(y_i\\) is predicted value, \\(\\eta\\) is learning rate.</p> \\[ \\overrightarrow{w}_{n+1} = \\overrightarrow{w}_n + \\eta * (t_i - y_i) * \\overrightarrow{x}_i \\] <p>\\((t_i - y_i)\\) is error.</p> <p>As we correct the weights, it rotates the hyperplane and this separates the classes better. It controls how quickly the model is adapted to the problem.</p> <p>Learning Rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#learning-non-linear-patterns","title":"Learning Non-Linear Patterns","text":"<p>Changing Integration Function, it is similar to kernel trick in SVM, we use quadratic or spherical intergration function, then w is mul with \\(x^2\\) for example. We cna take data to higher dim using polynomial integration func, then a hyperplane can separate the classes and when we reduce it back we get non-linear classification.</p> <p>Multi-Layeered Perceptrons, MLPs, have hidden layers. Each hidden layer has its x, y line. Set of lines can form a triangle, eg, three lines. Then we hve two classes one in triangle and another outside triangle.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#back-propagation-algorithm","title":"Back Propagation Algorithm","text":"<p>We have functions at each layer, we pass back the output as input back to previous layers based on the error each node contributes.</p> <p>----;</p> <p>Session 2:</p> <p>Tensor flow and pytorch are libraries ti implement DL. Keras is wrapper on top of these libs.</p> <p>Use DL only if the problem is not solvable by ML, for eg, image classification when we have 1000 object to classify from.</p> <p>Layout accuracy, 3d position of objects, positional relationships in objects, come up with a network of objects. Humans have hierarcial apporach</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#cnn-and-imagenet","title":"CNN and ImageNet","text":"<p>ImageNet holds 1,281,167 images for training and 50,000 images for validation, organised in 1,000 categories.</p> <p>MLP can't be used for imgae because the number of parameter to estimate grows to billions. becaus it is fully connected,</p> <p>In CNN, we have convolution layers. not all nodes are cnneted to each other, the network has conv and pooling.</p> <p>CNN of image with a filer:</p> <p>We get convolved feature in each convolution layer,</p> <p>at each layer we process part of image, and we overlap in each iteration. Filter size, eg, 3x3, is also paramenter, skipping 2px is alos paramenter. This allows us to reduce the number of weights we need to train,</p> <p>weights are shared so edges may be more but weights are less.</p> <p>The weights will learn in back propogation.</p> <p>We have multiple convolution layers. the number of filters define that.</p> <p>padding is added to the image if stride and filter cannot aprse whole imgae, also to make the edge pixels count same number of itmes as the other pixels. count as in passed by the filters.</p> <p>we can do zero paddign or mirror padding.</p> <p>The number of weights is eaqal to number fo filters passign thirugh image and plus 1 bias which is shared by all the filters.</p> <p>if we have two filters, eg, [3x3]x2, then we have weights times 2.</p> <p>Number of parameer is the filter size and bias and number of passes.</p> <p>so for 5x5 filter, 10 passes, params are , (5x5x3 + 1) x 10, 1 is a bias.</p> <p>FOr number of output, we count by adding padding twice, - filter size + 1.</p> <p>-----;</p> <p>Session 3</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#pooling-layer","title":"Pooling Layer","text":"<p>has 0 weights. ha filter that moves and gives max of points. it downsamples the imgae.  pool is done on all channels. so for 224x224 image with 64 channels, if we do pooling , max pool with 2x2 filter and stride 2, we get downsampled 112x112x64 size imge.</p> <p>max, l2, avg pooling</p> <p>why pooling, Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.</p> <p>Convolution brings out pattern, pooling finds most imp feature by avg or max.</p> <p>Overfitting can be avoided by dropout.</p> <p>We can augment data by flip rotate and rescaling images. dog remains a dog if we rotate.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#rnn","title":"RNN","text":"<p>Neural understand only numbers, now we have text data. 30 million unique words.</p> <p>vector to show word, w1 has vector with 1 at its location others 0, but cant be done for 30m words.</p> <p>each word has vector of fixed size with any number in vector, like 100 numer. similar word have similar numbers.</p> <p>word2vect</p> <p>RNN for text based work. it learns sequences, in context, letters in words have dependencies,</p> <p>Language models store bigram, trigrma etx. what comes afger what, ngram models.</p> <p>in neural we do word to vector by different algos.</p> <p>eg, we have 3 words and ech is represented by vecotr of 100 numbers then we have 300 inputs and 100 output uin NN.</p> <p>The output number is the predicted word and we assign that to our list of words.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#transformers_1","title":"Transformers","text":"<p>Like LSTM, Transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously described/existing sequence-to-sequence models because it does not imply any Recurrent Networks (GRU, LSTM, etc.).</p> <p>An architecture with only attention-mechanisms without any RNN (Recurrent Neural Networks) can improve on the results in translation task and other tasks!</p> <p>Another improvement is, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</p> <p></p> <p>It has no RNN but knows relative position of elements.</p> <p>Reference:</p> <ul> <li>https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-4_1","title":"Session 4","text":"<p>RNN offer lot of flexibility:</p> <ul> <li>one to one</li> <li>one to many</li> <li>many to many</li> </ul> <p>RNN output is matched against original. Output is concatenation of final neurons. If the output does not match then it backpropogates.</p> <p>paperwithcode.com - to keep updated</p> <p>we hve weights availabe in public by pre-trained model.</p> <p>output of a CNN is embedding for the captioning RNN.</p> <p>So we have dataset, [image, captions]. we pass image through public CNN for getting embeddings. Now our dataset is [Image embeddings, Captions]. Now we train on this.</p> <p>Bidirectional RNN to make use of 4th word to predict 3rd word.. not onlt firward looking but backword as well.</p> <p>Named-entity recognition (NER) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc</p> <p>Vanishing Gradient Problem:</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#isb-t5-marketing-analytics","title":"ISB T5 Marketing Analytics","text":"<p>Marketing analytics is done on CRM database to market more effectively. It is done to optimize the costs associated with marketing products and costs for customer acquiring and retention.</p> <p>Outbound means company calling a customer, Inbound means customer reaching out to company for service or product.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-3_1","title":"Session 3","text":"<p>Ad Copy - we can optimize the ad copy with text keyword that has less bid rate and can still get good click thru. this can be done using text mining and a model can be built for the same.</p> <p>Since htese keywords are infrequent hence less users type to comeup with thtat we have to find thousands of unfrequent keyworkds</p> \\[ Clicks = \\alpha [1 - e^{-\\beta b}] \\] <p>\\(\\alpha\\) = Traffic x CTR for the #1 ad</p> <p>\\(\\beta\\) = inversely related to competition intensity</p> <p>b = bid amount</p> <p>Using this we can start finding the conversion rate for different keywords.</p> <p>If LTV and ConversionRate is high then we can go for max bid amount and be #1 on the ad page. Else, we will have to find an optimal point.</p> <p>If the budget is lower than the optimal value for keywords, then do optimization with function to be sum of profits and constraint to be less than max expenditure.</p> <p>----;</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#isb-ml-sl2","title":"ISB ML SL2","text":"<p>linear classifier is a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector (input matrix).</p> <p>Bias is gap (error) between actual and predicted value whereas Variance is the scatterness of these values.</p> <p>if we have less complexity and more error then it is HB-LV, as we increse the complexity and reduce training error, it has more validation error and LB-HV. To find an optimal point where the model fits both training and validation data with less error is called bias variance trade off.</p> <p>Linear Discriminant Analysis or LDA is method to find linear combination of features that characterize or seperates two or more classes. It is dimentionality reduction technique while retaining as much info as possible. It is not a classifier. LDA and PCA are both linear transformation.</p> <p>LDA vs PCA &gt;</p> <ul> <li>LDA : Supervised :: PCA : Unsupervised (ignores class labels)</li> <li>LDA finds max separability, PCA finds max variance within a class.</li> </ul> <p>Vector is element of vector space having magnitude and direction. Vector Space is place/space of combination of vectors. It can be 2D ro 3D. Mul vectors together goves third vector. Mul by number to scale vector (bigger/smaller). 2D has two points (x and Y), 3D has 3 and n dimention has n. Matrix is a vector space and is used to store vectors. n x m matric can have vector in row or column. In CS, the data that we have, if we store it in matrix form it forms a vector space. YouTube</p> <p>Perceptron is an algo for binary classification. Input is a matrix of numbers, output is weather this belongs to a class or not. hence, it is called linear classifier. It makes a hyperplane in space to classigy the objects.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#neural-network","title":"Neural Network","text":"<ul> <li>Youtube Playlist https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#bayesian-network","title":"Bayesian Network","text":"<p>A Bayesian network is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#svm-support-vector-machine","title":"SVM Support Vector Machine","text":"<p>support-vector machines are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis.</p> <p>the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equality constraints.</p> <p>The most significant benefit from solving the dual comes when you are using the \"Kernel Trick\" to classify data that is not linearly separable in the original feature space.</p> <p>agrangian function (Primal) , applying a lagrangian stationarity condition and substitute to get dual</p> <p>we can define a separating hyperplane in a systematic way by introducing slack variables  \ud835\udf09\ud835\udc56  and minimizing the total error, Slack variables are positive (or zero), local quantities that relax the stiff condition of linear separability, where each training point is seeing the same marginal hyperplane.</p> <p>Kernel Trick is to map the data in much higher space so that it becomes linearly separable. A Kernel Trick is a simple method where a Non Linear data is projected onto a higher dimension space so as to make it easier to classify the data where it could be linearly divided by a plane. This is mathematically achieved by Lagrangian formula using Lagrangian multipliers</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#ensemble-learning","title":"Ensemble Learning","text":"<p>ensemble methods use multiple learning algos to obtain better predictive performance than could be obtained from any of the constituent learning algos alone.</p> <p>An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model.</p> <p>Bagging is composed of two parts: aggregation and bootstrapping. Bootstrapping is a sampling method, where a sample is chosen out of a set, using the replacement method.</p> <p>Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data. Boosting is an iterative technique which adjusts the weight of an observation based on the last classification</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#references","title":"References","text":"<ul> <li>Calculus https://www.youtube.com/watch?v=lowavG2SXsQ</li> <li>K-Means Stat Quest https://www.youtube.com/watch?app=desktop&amp;v=4b5d3muPQmA</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#isb-fp","title":"ISB FP","text":"<p>Soundfile is a package to read and write files. Every SoundFile has a specific sample rate, data format and a set number of channels.</p> <ul> <li><code>sf.read('filename.wav')</code> returns data and samplerate.</li> <li>Sound file can also be opened as SoundFile Objects <code>f</code>.</li> <li><code>f.read()</code> gives data</li> <li><code>f.write()</code> writes the file</li> <li><code>f.samplerate</code> returns sample rate.</li> </ul> <p>librosa - A python package for music and audio analysis. <code>librosa.feature</code> is used to extract features from a wav file. These are usally 2D arrays.</p> <p>MFCC is a 2D matrix format with MFCC bands on the y-axis and time on the x-axis, representing the MFCC bands over time. To simplify things, what we're going to do is take the mean across each band over time.</p> <p>MLP Classifier - MLPClassifier stands for Multi-layer Perceptron classifier which in the name itself connects to a Neural Network. Unlike other classification algorithms such as Support Vectors or Naive Bayes Classifier, MLPClassifier relies on an underlying Neural Network to perform the task of classification.</p> <p>PLP or perceptual linear prediction is a feature</p> <p>Links:</p> <ul> <li>PyPi with basic steps</li> <li>Read The Docs</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#ml-supervised-learning","title":"ML Supervised Learning","text":"<ul> <li>by Sailesh Kumar</li> </ul> <p>Session 1:</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#inroduction-to-machine-learning-sl","title":"Inroduction to Machine Learning SL","text":"<p>FORMULATION is the Key Skill in Data Science. It is the art of converting busines prob to ML.  \u201cOur technology, our machines, is part of our humanity. We created them to extend ourselves, and that is what is unique about human beings!\u201d -- Ray Kurzweil. Everything is evolving, tools, products, tech, skills etc.</p> <p>Products are evolving, emergent intelligence - let the technology emerge. Then learn thru unsupervised learning. Don;t make it pre intelligent, but let it evolve. Dont define classes on its own but let ML find the patterns and clusters.</p> <p>Philosophy of AI, understanding the Nature of the Mind. ML Goal is to unravell the nature of mind.  What is INTELLIGENCE? Generalization: Ability to predict or assign a label to a \u201cnew\u201d observation based on the model built from past experience.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#driving-better-decisions","title":"Driving Better Decisions","text":"<p>ML is to learn how to make better decisions, from broadcaste to peronalized decisions. tata sky vs netflix, this will apply to health n agri. From batch sms to realtime, e.g. no sundays offers but event based poersonlized offers. From data rich to AI first. ML is continuous learning, continuous improvement. It has evolved from BI to ML to DS to AI and is still progressing.</p> <p>Data - Insights - Features - Models - Predictions - Decisions - Feedback - Data</p> <p>Here, same paradigm can solve many problems, it can be a classification, recommendation, outlier detection or retrival. Every ML Paradigm is an OPTIMIZATION Problem</p> <p>We work by taking sample from reality, eg customer buying, rides taken.</p> <ul> <li> <p>clustering algo, outlier detection algo/method.</p> </li> <li> <p>rediction paradigm, is cust about to churn, car to breakdoiwn based onbserved data.</p> </li> </ul> <p>pattern recognition - usl,</p> <p>recommndatin engn -</p> <p>resoning alogo - its sequence of actions, not onlu 1, chatbot, solving math ptob</p> <p>reinforcement - laern to rason, explore an exploit,</p> <p>deep learning - not everyhting is deep learning</p> <p>Architecture of Ai</p> <ul> <li>stimulus to state, then state to action? how we do this is SL/USL.</li> </ul> <p>AI for industrial IoT</p> <ul> <li> <p>DOmain knowlege is faaaaarr more imp that stats and python</p> </li> <li> <p>how does on think about AI,</p> </li> <li>metrics to be max/minimized</li> <li>domain knowledge</li> <li>stimulus - sensors data,</li> <li>state - featured vectors</li> <li>response - actions or predictions, classes etc</li> <li>eg, RETAIL, AGRICULTURE, EDU</li> </ul> <p>Democratizing AI for ALL</p> <ul> <li>making AI available</li> <li>lot of apps/models/apis/services are developed</li> <li> <p>AI market place for India, all upload their model, and anyone can consume via API. it coneects product thinkeers with AI</p> </li> <li> <p>Ariculture, now I can tell what is gown an dant what stage, so no wrong growth neither import, no wastegae</p> </li> </ul> <p>Super vises:</p> <ul> <li>regresion, prediction</li> <li>clas - time here,</li> <li>recommnedation - what are you going to do in future based on past, tell fav 4 movie and we will recommend based on that.</li> <li>retrival - entity to a query.</li> </ul> <p>Supervised learning paradigm:</p> <ul> <li>numerical prediction the regression</li> <li>categorical then classification, crop is  classification cz it is not based on recommendation of past dta</li> <li>recommendation - top 4 recommendation</li> <li>retrival -</li> </ul> <p>eg:</p> <ul> <li>ad position is retrival based on query u show ad</li> <li>credit card is classification</li> <li>youtube - recommendation, also retrival for your query in seatch box.</li> <li>google images, - retrival n then, gif/clipart/sketch based on classification it filters.</li> <li>connect/follow on linkedIn via recommendation, seach is retrival,</li> <li>product next - recommendation</li> <li>android prce, profit model</li> <li>medicine/tereatmment classification</li> <li>resumne for Job ID - retrival, based on JD or query we provide resume.</li> <li>where to sell new prods, like jio all in one media</li> <li>like barclays new product</li> <li>MRI cancer? classification, class has interprtation and prediction. curent data vs future</li> <li>Stock regression</li> <li>credit limit, regression, cz of number</li> <li>customer churn, classifier, give data, i'll interpret.  -stealing/shopping clss, interpretaiton</li> <li>objects - classification, detection(interpret) and recognition(predictin).</li> <li>iPads in mumbai - regression, demand forecasting</li> <li>Tweet - classification</li> <li>rhowuse  -regression, price forecasting</li> <li>song humming - retrival,</li> <li>shaadi - retrival, also can be recommendatuib vased ib your activity. this exercise is classification as we give a class label to every prob</li> </ul> <p>From data to decisions</p> <p>-</p> <p>Outlier detection paradigm</p> <p>-</p> <p>session 2:</p> <ul> <li>data nuance1 - fourier transform?</li> <li> <p>noise vs signal, every data has it, eg, some vibrations in aeroplane are noise in xyz coordinates, while take off and landing is signal. Same for voice data, ocean heights. we need to distinguish bw noise and signal</p> </li> <li> <p>covida data has noise, like some tests are not trust worthy. count is not the true representation. not i=enought tests, not testing in the right places, flase postve and true nregative sis alo noise, reporting err isnoise. so we take the data and create model, then model predictions remove noise.</p> </li> <li> <p>ML is used to find signal in noise.</p> </li> </ul> <p>Data Nuance 2:</p> <ul> <li> <p>nobody is buying the complete logical set of things.</p> </li> <li> <p>grammar data - there is mixture of projection, means different products might be purchased from different baskets, eg, keyboard mouse with hammer and screw driver</p> </li> </ul> <p>Data nuance 3:</p> <ul> <li>degree of variabliluty in images is very high e.g., due to pose, light etc.</li> <li>making system invariant to the variance is what we need. eg, making system invariant to different accent of speech</li> </ul> <p>DN 3 Invariance:</p> <ul> <li>ignore the variance in all kind of data. ML is art of invariance learning, what matters and what doesn't matter</li> </ul> <p>DN 4 |  Missing Data</p> <ul> <li>many reasons and we need to be logical in filling missing data. like mean, averange, median or model based, predict the missing values.</li> </ul> <p>DN 5 | Not so normal dist</p> <ul> <li>to use stats methods and math priciple like aucledian distnace, we need normal dist assumption and or uniform distn.</li> <li>else, we can take log, or other transformations.</li> </ul> <p>DN 6 | heterogenious mixture :</p> <ul> <li>once column may have different unit values, eg, blood test</li> <li>min max may not work, because of outliers. it may work some time and not other time.</li> </ul> <p>DN 7 | Multi-modality features:</p> <ul> <li>2D, 3D,  TIme-series,</li> <li>need to normalize, reduce dimentionality,</li> </ul> <p>Importance of labelled data-</p> <ul> <li>scaling can be done using cloud</li> <li>AIML can be done using tensor, SparkML etc libs</li> <li>big data can be done using spark, hadoop, kafka etc.</li> <li> <p>these doesn;t make u special, or super.</p> </li> <li> <p>to make a super powerful company you need labelled data, then you can make more accurate predictions, eg, google has more labelled data than bing. google images, maps, capcha.</p> </li> <li> <p>it has to be kept sage, no leakage</p> </li> </ul> <p>what are you trying to predict?</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#machine-learning-feature-engineering","title":"Machine Learning Feature Engineering","text":"<p>What variables are important for you?</p> <p>Insights + Domain = Feature</p> <ul> <li>Fitbit to cardiac</li> <li>Bank tranactions to CIBL score</li> </ul> <p>Two ways to do DS:</p> <ul> <li>simple features, complex model</li> <li>complex features, simple model</li> </ul> <p>Here, complex model with neurons have latency and if you need fast predictions then use complex features, that;s what Google does.</p> <p>While modellling we need to consider the limitations of models like we cannot do multiplication or division in some models then we might need to take log or other such tranasformations.</p> <p>Model the variables that matter:</p> <ul> <li>NOTE: Try to never use raw number in ML, it may have noise and other things, e.g., dont use balance for creadit score, normalize it with other facts like expense, frequency of expense, (total debts)/(total income) etc.</li> <li>remove exp nature by taking log, remove senstivity of varibales</li> </ul> <p>Model Deviations from Expected:</p> <ul> <li>Model to predict total_sales, demand forecasting model, then modelling just for a number like sales is not important, you need to model it;s deviation from mean, that tells weather we have met expectation or not.</li> </ul> <p>\"Bugs\" in Feature Engineering</p> <ul> <li>Its like when telling a lie, then explaination goes complex, like Johnny Jhonny, yes papa...</li> <li>So if model is getting complicated and then too not acheiving accuracy then there is possibilty of a feature bug that the model is trying to compensate, e.g., 0 for not found and first value in field or variable having garbage values.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-3_2","title":"Session 3","text":"<ul> <li>Feature engg is exploring all features and trasnforming them</li> <li>impoertance of dimentions, eg, 23675, here 2 is most imp because of units we have. it is 10s thousands, similarly we need to find imp eg color, size etc. if we remove the thing, eg, remove 2 and make 0, then max loss of information is most imp feature.</li> <li>now, if we check, is the number divisible by 2, then, last digit becomes most important.</li> <li>many features will be there, then many feature will be engineered. task is to determine most imp feature.</li> <li>which is better? - same feature, but different class, A B are class, dengu non dengu, if sigma is high it is worse. Seperation of mean should be high and variance within the class should overlapp minimum, variance within class should be low</li> </ul> <p>study, fisher distribution, selection of feature, projection of features</p> <p>We reduce the dimentions into 1D, we compute w, the weigths which can be multiplie with dimensions to maximise a values:</p> <ul> <li>PCA maximizes variation in data</li> <li>Fisher maximizes separation in classes</li> </ul> <p>We use fishers formula to compute the separation of classes nin numeric way. Which dimension separates the classes maximum out of 1000s columns/dimntions.</p> <p>We use another method to find which projection separates the classes the max, diff in the means.</p> <p>clustering is done, then we get casses and it becomes classification problem from clustering problem. In real world new classes keep emerging.</p> <p>There are both clssifiers, discriminative and descriptive. once defines withing and another differentiates the classes.</p> <p>now, classifier world, model:</p> <ul> <li>classifiers describe the class , such that it does not overlap each other</li> <li>discriminative classifiers dicriminate and we know the classes.</li> </ul> <p>Partitioning into pure region</p> <ul> <li>cannot be 100% boundary, because of noise</li> <li>complexity of classifier can only be linear for now.then which is the best fir line to distinguish the classes is the optimization. which linear classifier maximises the purity.</li> <li>medium decision boundary is having a polynimial, all curves are degree of polynomial</li> <li>100% accuracy on training data, is all pure region, is very complex.</li> <li>out of these, something in b/w is good, region should be as pure as possible.</li> </ul> <p>Toy dataset</p> <ul> <li>art, start with feature engg, color, shape, lights, sounds</li> <li>collect the data, get sales data, then see number of times it was sold/not</li> <li>art, create labelled data, check video of toys, take interview of sample kids</li> <li>we have categorical data, shape and color. We make matrix to find probabilies/region. This is classifying a toy, as in to which cell a toy belongs to.</li> <li>Now if we increase the features then the matrix increases hence we will use decision tree.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-4_2","title":"Session 4","text":"<p>Recap:</p> <p>We create contingency table from categorical variables nd then try to find the pure region which the goal of ML Classifier.</p> <ul> <li> <p>each row, column or cell is a region.</p> </li> <li> <p>purity is the probability of the majority class in a region</p> </li> <li> <p>when the data is noisy, 55-45 then we don not use majority class, we use GINI measure, we find the expected accuracy.</p> </li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#3rd-measure-of-purity","title":"3rd measure of purity","text":"<p>requires entropy of distribution, used by communication sattelites.</p> <p>Information Theory 101:</p> <ul> <li> <p>rare letter vs frequent letter and its importance. rare ahve more importance.</p> </li> <li> <p>entropy means chaos and uncertainity.  </p> </li> </ul> <p>purity is high then entorpy is low.</p> <p>uniform ditn has workst entorpy</p> <p>Accuracy can be mesured by</p> <ul> <li>purity</li> <li>entorpy, or</li> <li>ginin index.</li> </ul> <p>Decision tree works by dividing data by class then findinf the purity of region.</p> <p>Size X Purity for a region is score, it is basis of decision tree. As we get more and more deeper in a tree shape emerges and purity incereases.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#greedy-decision-tree","title":"Greedy decision tree","text":"<ul> <li> <p>if A turne dout ot be the best feature then we keep it on top of tree, then further look down to partition, all rows ahving 'a1' and remaining 25 fetures.</p> </li> <li> <p>we stop growing when we reach a pure partiiton, need not partition futher, they are pure above a parameter threshold., eg, c1 and e1.</p> </li> <li> <p>in case of numeric attributes, sort and do all possible partitions. then we get n-1 partitions.</p> </li> <li> <p>for numeric, we can only go horiontally or diagonally, then for diagnol we do feature engineering.</p> </li> <li> <p>logistic regression is good for numerical variable.</p> </li> <li> <p>so we cna combine ML techniques an not get stuck with one, this is wehere we can do programming in it and cnannot use library. for eg, embed logistic regression at a leaf of decision tree.</p> </li> <li> <p>facbook, for eg, ignore numerical var and use only cat var to make decision tree, then partitions on numberical var.</p> </li> </ul> <p>Recap:</p> <ul> <li>fisher discriminant SV tech, for feature selectiion, importance, projection</li> <li>rule based classifier, pre set of classifier, based on few features,</li> <li>learn rules based on data, birth of decision tree, hence birth of ML. DT is first algo of ML.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#k-nearest-neighbors","title":"K-Nearest Neighbors","text":"<ul> <li>carc - classification and regression cheat</li> <li>for 2 classes we use odd k</li> <li>as we inc k, the complexity decreases</li> <li>parameter is nothing, this is non-parametric classifier</li> <li>given the hyper param what are the params</li> <li>hyper is used to control the complexity</li> <li>param is the tree lenght or the partitions</li> <li>no training time</li> <li>scoring complexity depends on k</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#kernel-density-function","title":"Kernel density function","text":"<ul> <li>points are like magnets and thy have influence on the validation data. They influence distn is called 'Density Estimate'</li> <li>magnets can be +ve or -ve, then each influence is calculated, then the data belongs to the class whcoh has high influence.</li> <li>density function is influence on x by all other points.</li> <li>Thsi is Parsean Window.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#bayesian-classifiers","title":"Bayesian Classifiers","text":"<ul> <li>Based on Bayes Rule.</li> <li>P(Data|Class) is based on old data where you know the class or result of diagnosis.</li> <li>P(Class) is probability of such dicease occuring</li> <li>P(Data) is probability of such patients coming</li> <li>RHS is all labelled data,m that is history</li> <li>LHS is for new data that has yet to come.</li> <li>P(Class|Data) is for new data that has not been seen previously.</li> <li>Learinig on RHS , inference on LHS.</li> <li>Comes in interview think like a Bayesian</li> </ul> <p>Multivariate data:</p> <ul> <li>we can start simple by using similar covariance matrix for all dimentions</li> <li>then inc complexity by allowing different covariance</li> <li>then by different degree of freedom</li> <li>then by increasing dimesion freedom as well.</li> </ul> <p>;</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#fa-forecasting-analysis","title":"FA - Forecasting Analysis","text":"<p>Ninary process are only yes or no</p> <p>point process tells us only occurance.</p> <p>forecasting is based on past data.</p> <p>predicting might not be associated with time, but forecasting is,</p> <p>in timeseries, the value is correlated with the previous observations.</p> <p>deterministic vs stochastic</p> <ul> <li> <p>there are a lot of fluctuations in stochastic series</p> </li> <li> <p>temporal correlation is relation to the previous year, scatter pliot of values this year vs last year</p> </li> <li> <p>notation, T + h|T is how many periods into the future you are going to forecast.</p> </li> <li> <p>frequency of seasonality is important, it depends on data granularity and how often the seasonality repeats.</p> </li> <li> <p>cyclicality is not that it will occur at regular interval but it is random, like pandemic or recession.</p> </li> <li> <p>seasonality occurs at regular interval while cyclicality is random in time interval.</p> </li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#time-series-modelling","title":"Time Series Modelling","text":"<p>There are different methods of forecasting, but three terms need to be taken care of:</p> <ol> <li>Seasonality - Some months have peak output value compared to other months. eg, more travel during christmas time. we need to capture this seasonality while forecasting.</li> <li>Trend - Inc/Dec in behaviour over time,</li> <li>Unexpected Events or Irregularity or Noise - Dynamic changes that occur in market or organization that cannot be captured. eg, pendemic, recession.</li> </ol> <p>We can use algo and other methods to capture seasonality and trend but unexpected events are dynamic and difficult to capture.</p> <p>Stationary timeseries means that the mean and variance are same between two timestamps. We need to make timeseries stationay if it is not (maybe it is like standardizing the values). We can use rolling statistics to find moving average, on a window size, to make a TS stationary. Moving avg plot is smoother than the actual values. This makes TS stationary, we can also do it by using Exponential smoothing.</p> <p>Simple exponential smoothing $ yT = \u03b1 XT + \u03b1(1\u2212\u03b1) yT\u22121  $.</p> <p>Double exponential smoothing for capturing trend and seasonality $ Yt =  \u03b1 Xt + (1-\u03b1) (yt-1 + bt-1) $ where, $ bt = beta (Yt \u2013 Yt-1) +  (1-beta) * bt-1 $.</p> <p>In descriptrive modelling of timeseries data we model to determine its components like seasonal patterns, trends, relation to external factors etc. This is analysis of timeseries data, it tells the underlying causes and the why behind the data. We decompose the timeseries data into constitution components. The quality is determined by how well the model describes the data. The primary objective is to develop the mathematical model from the data that can provide plausible description from the sample data.</p> <p>In Predective modelling or forecasting we use this information to find future values. The future values can only be estimated from what has already happened.</p> <p>Components of Time Series:</p> <ul> <li>Level - baseline value of series if it was a stright line</li> <li>Trend - optimal linear inc/dec behaviour over time</li> <li>Seasonality - optional repeating patterns or cyclic behaviour over time.</li> <li>Noise - optional varability that cannot be explained by the model.</li> </ul> <p>These constituent components can be thought to combine in some way to provide the observed time series.</p> <p>TS can be additive or multiplicative. if these components add then additive, else multiplied then multiplicative.</p> \\[ y = level + trend + seasonality + noise \\] <p>this can be modelled with assumptions using traditional statistical methods.</p> <p>these components can help make forecast but not always.</p> <p>Data Prepration needs to be done to check NULLs or non-continuous data, to make date granularity on the level we are interested in. Then:</p> <ul> <li>Group metrics by granularity of date, like, roll up to daily/monthly etc, use group by with sum/mean.</li> <li>Make date column as index.</li> </ul> <p>Visualizing the data helps us see seasonality, trend.</p> <p>We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: trend, seasonality, and noise.</p> <pre><code>import statsmodels.api as sm\ndecomposition = sm.tsa.seasonal_decompose(df, model='additive')\nfig = decomposition.plot()\nplt.show()\n</code></pre> <p>This gives: observed - timeseries as is trend - how metric has gone up or down seasonal - cyclic up and down in series, and residual - left over after fitting a model, its actual - trend - seasonality.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#using-arima-autoregressive-integrated-moving-average","title":"Using ARIMA - Autoregressive Integrated Moving Average","text":"<p>ARIMA(p, d, q), these account for seasonality, trend, and noise in data</p> <p>We can find values of pdq by using machine learning algos and can improve it by using grid search. We can check for lowest AIC value, and can consider those parameters.</p> <p>Summary Table - <code>print(results.summary().tables[1])</code> to print the results. This gives stats output table with coefficient value, p value and t scores.</p> <p>Diagnostics - we can run model diagnostic to see how are the variations and can usnderstand if it is feasible to use this model. if the residuals are normally distributed it indicates that we can make use of model.</p> <p>Validations - we can validate to understand accuracy, the predicted values are compared with real values. <code>pred = results.get_prediction(start=pd.to_datetime('2017-01-01'), dynamic=False)</code>. We can plot this to have a better unserstanding of the results. The trend should align and make sense. Next, we can find MSE and RMSE using the predicted and actual values.</p> <p>Once we are happy with the model we can Forecast the values for the future dates.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#using-fb-prophet","title":"Using FB Prophet","text":"<p>Developed by Facebook in 2017, it also has advanced capabilities for modeling the effects of holidays on a time-series and implementing custom changepoints.</p> <p>Install:</p> <ul> <li><code>sudo conda install -c conda-forge fbprophet</code></li> <li> <p>in case of error, <code>sudo pip install pystan==2.18.0.0</code>, then above step.</p> </li> <li> <p><code>import fbprophet</code></p> </li> <li> <p>in case of error,</p> </li> <li>in file <code>/Users/vaibhavyadav/anaconda3/lib/python3.7/site-packages/fbprophet/hdays.py</code></li> <li>replace <code>from holidays import WEEKEND, HolidayBase, easter, rd</code>,</li> <li>with</li> </ul> <pre><code>from holidays import WEEKEND, HolidayBase\nfrom dateutil.easter import easter\nfrom dateutil.relativedelta import relativedelta as rd\n</code></pre> <p>References:</p> <ul> <li>https://machinelearningmastery.com/time-series-forecasting-with-prophet-in-python/</li> <li>https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b</li> <li>https://www.analyticsvidhya.com/blog/2021/07/time-series-forecasting-complete-tutorial-part-1/</li> </ul> <p>;</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#unsupervised-learning","title":"Unsupervised Learning","text":"<ul> <li>No labels in data but we try to find patterns within, eg:</li> <li>Clustering \u2013 grouping data, inferring labels from the data</li> <li>Rule-mining \u2013 interesting relation b/w varibales, finding patterns based on purchase information</li> <li>Recommendation-system \u2013 suggesting new items based on usage</li> <li>Dimension reduction \u2013 Inferring structure from the data</li> </ul> <p>USL is first arrow on data but underlyting assumptions shuld be validated in EDA, then followed by Epicyclic procss.</p> <p>Association Rule Mining</p> <ul> <li>how to combine things, market basket ananlysis</li> <li>txns could be a few days of purchase as well.</li> </ul> <p>Terminology:</p> <ul> <li>I is all items in store</li> <li>X is one basket/txn, called item</li> <li> <p>D is dataset, table having each row as txn.</p> </li> <li> <p>Association rule is, X =&gt; Y, if X then Y, X and Y have nothing in common X is called antecedent, LHS or body Y is called consequent, RHS or head</p> </li> </ul> <p>How to evalueate:</p> <ul> <li>Support: how often they occur tohether in store,</li> <li>sigma is count, txns in store</li> <li>s is pct, count/total</li> <li>Confidence, once you buy X how often you buy Y</li> <li>given X, X =&gt; Y, c = S(X =&gt; Y)/S(X)</li> <li>Lift, they occur vs they randomly occur.,</li> <li>s(X =&gt; Y) / s(X)*s(Y)</li> <li>how much X lifts prob of buying Y.</li> <li>lift = 0.4/0.6*0.6 = 1.11, thsi says that you expect X adn Y 30% of time but they occur 40% of the time, hence has more lift.</li> <li>leverage is diff of occured and random, lift is ratio, \ud835\udc3f\ud835\udc52\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc54\ud835\udc52 \ud835\udc4b\u2192\ud835\udc4c =\ud835\udc60 \ud835\udc4b\u2192\ud835\udc4c \u2212\ud835\udc60 \ud835\udc4b \u2217\ud835\udc60(\ud835\udc4c)</li> <li>conviction is ratio with diff from 1. \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc63\ud835\udc56\ud835\udc50\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b \ud835\udc4b \u2192 \ud835\udc4c = (1\u2212\ud835\udc60(\ud835\udc4c))/ ( 1\u2212\ud835\udc50(\ud835\udc4b\u2192\ud835\udc4c) )</li> </ul> <p>Rules are discovered using Apriori algo. It tries to improve the efficiency by managing the frequently occuring items. Phase 1 - Join, support, exhaustive search, 2^n itemsets, if an itemset is frequent, then all its subsets are also frequent Phase 2 - Prune, confidence</p> <p>Improving, Hash-based technique, When scanning for 1-itemsets, create a hash for 2- itemsets, and remove hash buckets that do not have enough support. Reduce the transactions, Prune the transactions that do not contain any k-itemset, when scanning for (k+1)-itemset. Partitioning, Divide the transactions into non-overlapping datasets, Find \u201clocal\u201d frequent patterns, Verify if these local frequent patterns have enough support in the entire data.</p> <p>Adv FP-Growth, Improves the efficiency of not scanning the database multiple times for the generation of candidate itemsets, Minimizing the number of frequent items considered, Multi-level or multidimensional pattern mining, Sequential and time-series patterns.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#session-2-recommendations","title":"Session 2: Recommendations","text":"<p>Collaborative filtering:</p> <ul> <li> <p>filter itmes/users based on similarity from collaboration</p> </li> <li> <p>user-based - find similar users to focal user based posts liked, both like car and pizza hence similar</p> </li> <li>item-based - find similar items based on users liking them, u1, u2 like a post then other posts liked by u1 are similar and can be recommended to u2</li> </ul> <p>\ud835\udc48 is the set of users: |U| = n.  \ud835\udc621,\ud835\udc622,...,\ud835\udc62\ud835\udc5b \ud835\udc3c is the set of items: |I| = \ud835\udc5d. \ud835\udc561,\ud835\udc562,...\ud835\udc56\ud835\udc5d</p> <ul> <li>Ratings data is matrix \ud835\udc5f, with \ud835\udc5b rows (users) and \ud835\udc5d (columns), r_jk is the rating of the user \ud835\udc62 for the item \ud835\udc56</li> </ul> <p>User-based CF: Step 1</p> <ul> <li>We need to define a distance metric between the focal user and other users.</li> <li>Two popular metrics</li> <li>Pearson correlation between ratings</li> <li>Cosine similarity between ratings</li> <li> <p>Once we decide on a metric, we can find the neighbors using k-nearest-neighbors (knn) approach.</p> </li> <li> <p>Correlation proximity</p> </li> <li>Ratings by user \ud835\udc621 on items \ud835\udc561,...,\ud835\udc56\ud835\udc5d</li> <li>r11....r1p avf r1_</li> <li>r21....r2p avg r2_   \ud835\udc36\ud835\udc5c\ud835\udc5f\ud835\udc5f \ud835\udc621,\ud835\udc622 = \u2211 (\ud835\udc5f1i \u2212 \ud835\udc5f1_) \u2217 (\ud835\udc5f2i \u2212 \ud835\udc5f2_) / sqrt( \u2211(\ud835\udc5f1i \u2212\ud835\udc5f1_)^2 ) \u2217 sqrt( \u2211(\ud835\udc5f2i \u2212\ud835\udc5f2_)^2 )</li> </ul> <p>C\ud835\udc5c\ud835\udc60\ud835\udc46\ud835\udc56\ud835\udc5a \ud835\udc621,\ud835\udc622 = \u2211\ud835\udc5f1i \u2217 \ud835\udc5f2i / (  sqrt(\u2211\ud835\udc5f1i^2) * sqrt(\u2211\ud835\udc5f2i^2) )</p> <p>Itembased CF use same metrics</p> <p>Disadvantages of Collaborative Filtering</p> <ul> <li>Coldstart - When a new user who has not rated any item - When a new item is introduced into the market</li> <li>Serendipity- There are no surprises in the recommendations.- This can be modeled into the collaborative filtering framework</li> <li>Sparsity - Most of the ratings matrix is empty!</li> <li>Biased data (rating stickiness) - Popularity bias - Unique tastes of users is lost. - Over time the data can be biased - Is the rating the actual rating or the recommendation has biased it?</li> <li>Privacy related issues</li> </ul> <p>Association rule can bundle items, Recommendation is often a single or a set of items with no apparent relationship.</p> <p>The idea in matrix factorization is discover these latent features that the users and items can be aligned with,    Latent factors It is a general concept that can describe a user and an item.</p> <p>Singular Value Decomposition (SVD)  Fill (impute) the empty cells with average rating  Non-negative Matrix Factorization (NMF)NMF can be applied when \ud835\udc45 has only non- negative values</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dangers of Dimensionality Reduction Reducing dimensionality can distort data (and, hence, data mining results) in misleading ways</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#outlier-and-anomaly-detection","title":"Outlier and Anomaly Detection","text":"<p>The set of data points that are considerably different than the remainder of the data</p> <p>Detection schemes - Graphical , Statistical/model-based, Distance/proximity - based.</p> <p>Graphical Approaches</p> <ul> <li>Simple examples</li> <li>Boxplot (1D)</li> <li>Scatter plot (2D)</li> <li>Spinning scatterplot (3D)</li> <li>Benefits</li> <li>Intuitive, use powers of human cognition</li> <li>Limitations</li> <li>Time-consuming</li> <li>Subjective</li> <li>Difficult to use with higher dimensionality</li> </ul> <p>Statistical Approaches - Statistical methods (also known as model-based methods) assume that the regular data follow some statistical model (a stochastic model) - The data not following the model are outliers - Lots of different models are available - Effectiveness of statistical methods highly depends on whether the assumption of statistical model holds in the real data - Many statistical techniques have been developed - E.g., parametric vs. non-parametric</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#data-is-represented-as-a-vector-of-features","title":"Data is represented as a vector of features","text":"<ul> <li>Three major approaches - Nearest-neighbor-based</li> <li>Density-based</li> <li>Clustering-based</li> </ul> <p>Nearest-Neighbor-Based Approach</p> <ul> <li>Simple idea:</li> <li>Compute the distance between every pair of data points and use the information about k nearest neighbors of each point</li> </ul> <p>Density-Based Approach</p> <ul> <li>Finds local outliers, i.e., by comparing data points to their local neighborhoods, instead of looking at the global data distribution</li> <li>Intuition: The density around an outlier object is significantly different from the density around its neighbors</li> <li>Method: Use the relative density of an object against its neighbors as the indicator of the degree of the object being outliers</li> </ul> <p>Density-Based Approach: Local Outlier Factor (LOF)</p> <ul> <li>Basic idea:</li> <li>For each object (data point), compute the density of its local neighborhood (defined by the k nearest neighbors)</li> <li>Compute local outlier factor (LOF) of a given object as the ratio between its local density and the local densities of its nearest neighbors</li> <li>Outliers are objects with largest LOF value</li> <li>A number of further variations and refinements have been proposed</li> </ul> <p>Clustering-Based Approaches</p> <ul> <li>Regular data belongs to large and dense clusters, whereas outliers belong to small or sparse clusters, or do not belong to any clusters</li> </ul> <p>Strengths</p> <ul> <li>Many clustering techniques available</li> <li>Work for many types of data</li> <li>Clusters can be regarded as summaries of the data</li> <li>Once the cluster are obtained, need only compare any object against the clusters to determine whether it is an outlier (fast)</li> <li>Weaknesses</li> <li>Effectiveness depends highly on the clustering method used \u2013 it may not be optimized for outlier detection</li> <li>High computational cost</li> <li>I.e., need to first find clusters</li> <li>There are some techniques that try to mitigate this cost</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#challenges-of-outlier-detection","title":"Challenges of Outlier Detection","text":"<ul> <li>Modeling regular objects and outliers properly</li> <li>Hard to enumerate all possible regular behaviors in an application - The border between regular and outlier objects is often a gray area - More complex outlier behavior: collective outliers, contextual outliers</li> <li>Application-specificity in outlier detection</li> <li>Choice of distance measure among objects and the model of relationship among objects are often application-dependent</li> <li>Handling noise in outlier detection</li> <li>Noise may distort normal objects, blurring the difference between regular objects and outliers</li> <li>Understandability</li> <li>Why these are outliers? (I.e., justification of the detection)</li> <li>E.g., poor data quality, measurement malfunctions, manual entry errors, correct but abnormal data</li> <li>Specify the degree of an outlier</li> <li>The unlikelihood of the object being generated by a normal mechanism</li> <li>Supervised outlier detection methods</li> <li>Can be used, if descriptions (labels) of anomalous data are available - Otherwise, it is typically an unsupervised (exploratory) learning task</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#visualizing-data-using-t-sne1","title":"Visualizing Data using t-SNE1","text":"<ul> <li>t-SNE is mainly a non-linear dimensionality reduction technique for visualization</li> <li>t-SNE stands for t-distributed Stochastic Neighbor Embedding</li> <li>The main idea is to project high-dimensional objects to a low-dimensional objects such that there is high probability that</li> <li>t-SNE minimizes the divergence between - a distribution of pairwise similarities in high- dimensional space</li> <li>a distribution of pairwise similarities in low- dimensional space</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#spatial-and-temporal-anomaly-detection","title":"Spatial and temporal anomaly detection","text":"<p>Report any observed value that is significantly above or below its expected value (e.g., using GESD). Time series data Spatially distributed data One simple case of model-based anomaly detection is when we are monitoring a single real-valued quantity over time and/or space</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#anomalous-pattern-detection","title":"Anomalous pattern detection","text":"<p>Main goal of pattern detection: to identify and characterize relevant subsets of a massive dataset, i.e. groups of records that differ from the rest of the data in an interesting way.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#subset-scanning","title":"Subset scanning","text":"<p>We can scan over subsets of the dataset in order to find those groups of records that correspond to a pattern.</p> <p>Step 2: Consider the highest scoring potential patterns (S, P) and decide whether each actually represents a pattern.</p> <p>Step 1: Compute score F(S, P) for each subset S = {xi} and for each pattern type P, where higher score means more likely to be a pattern.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#linear-time-subset-scanning","title":"Linear-time subset scanning","text":"<p>Given a score function F(S) which satisfies the linear-time subset scanning property, we can optimize F(S) over the exponentially many subsets of data records, while evaluating only O(N) regions instead of O(2N). Just sort the locations from highest to lowest priority according to some function, then search over groups consisting of the top-k highest priority locations (k = 1..N). The highest scoring subset will be one of these!</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#why-bayesian-networks","title":"Why Bayesian networks?","text":"<ul> <li>An easily interpretable graphical representation of the relationships between a set of variables.</li> <li>Bayes Nets can be specified manually or learned automatically from data, and enable computationally efficient probabilistic inferences.</li> <li>Many practical and successful applications in medicine, manufacturing, failure diagnosis:</li> <li>Diagnosis: infer Pr(problem type | symptoms)</li> <li>Prediction: infer probability distributions for values that are expensive or impossible to measure.</li> <li>Anomaly Detection: detect observations that are very unlikely (i.e. have low probabilities given the model).</li> <li>Active Learning: choose the most informative diagnostic test to perform given these observations.</li> </ul> <p>\u201cX and Y are conditionally independent given Z\u201d: Pr(X, Y | Z) = Pr(X | Z)*Pr(Y | Z) or Pr(X | Z) = Pr (X | Y, Z) and Pr(Y | Z) = Pr(Y | X, Z)</p> <p>Make a truth table listing all combinations of values of your variables. If there are M binary variables, the table has 2M rows.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#the-many-uses-of-bayes-nets","title":"The many uses of Bayes Nets","text":"<p>Bayes Nets provide a useful graphical representation of the probabilistic relationships between many variables. Automatic learning of Bayes net structure can be used for exploratory analysis of datasets with many attributes. We can often improve the performance of model-based classification by moving from Na\u00efve Bayes to Bayes Nets. We can also use Bayes Nets to detect anomalies, by finding points with low probabilities given the Bayes Net. Bayes Nets provide a compact structure which enables us to efficiently compute probability distributions for any unobserved variables given observations of other variables.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#ct3-fraud-analytics","title":"CT3: Fraud Analytics","text":"<p>Frauds: Fake insurance, wrong medical claims and invalid transactions.</p> <p>Money Laundering: Hide the origin of money by passing from complex transactions.</p> <p>Why ML here: It can detect fraud using behaviours in data.</p> <p>C5.0 is a classification algorithm. This was used to create a model which detected Money Laundering. It reduced number of transactions reported from 30% to 1%.</p> <p>Based on behavioural data ML model can detect anomalies. It shows a row of data that stands out, like, too many txns on online shopping.</p> <p>We use supervised learning, where based on each columns (attributes) we assign a class to the data. Data has to be labbeled.</p> <ul> <li> <p>Deep Neural Network (DNN) can be used for classification. Neural Networks has hidden layers where we use transform functions, DNN has 100s of hidden layers.</p> </li> <li> <p>Perceptron is the simplest neural network possible: a computational model of a single neuron. A perceptron consists of one or more inputs, a processor, and a single output. It defines decision boundary between classes by defining the hyper planes in higher dimensions.</p> </li> <li> <p>SVM or Support Vector Machines can be used for classification. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs. It finds several hyperplanes to minimize the classification error.</p> </li> <li> <p>K-Nearest Neighbours or KNN uses simplicity metrics to group samples into classed in featured space. New data is assigned to the class which is nearest to the class.</p> </li> <li> <p>Random Forests breaks data in many samples and makes decision tree from each. New data is passed from trees and then picks the majority vote to assign data to class. This outperforms others but has issues of overfitting.</p> </li> </ul> <p>Financials fraud data is generally skewed.</p> <p>Exploratory Data Visualization is used to view how our classes differentiate. Find patterns of differentiation in data.</p> <ul> <li> <p>Andrews Plot shows curves and usually classes are well seperated in the curves.</p> </li> <li> <p>Parallel Lines each attribute is vertical line, each row is ||el line, classes can be seen separated.</p> </li> </ul> <p>In summary:</p> <ul> <li>Class labels are needed for supervised learning</li> <li>Perceptron learning lays the foundation for many of the neural network architectures</li> <li>Financial fraud data is singularly characterized by the skewed nature of the data</li> <li>Overfitting in neural networks is a generic issue</li> <li>Very powerful algorithms exist for supervised learning</li> </ul> <p>t-SNE t-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data. It gives you a feel or intuition of how the data is arranged in a high-dimensional space. t-SNE is a powerful visualization tool that preserves topological structures in the higher dimensional space</p> <p>Chernoff Faces, each attribute is specific feature on face, can have upto 17 attributes.</p> <p>Plot Histo, box plot, andrew, parallel, t-SNE, chernoff, pair plot, pareto of eigen values, scatter of pca and visualize. t-SNE, reduces 6D to 2D.</p> <p>ML</p> <p>Dimentionality Reduction</p> <p>Examples Datasets</p> <p>Machine learning for Fraud Analytics</p> <p>Fraud Stats:</p> <ul> <li>$26b fraud</li> <li>CNP 82% reason</li> </ul> <p>Examples of Fraud:</p> <p>Potential for ML use:</p> <ul> <li>banks legacy systems use about 300 rules to approve a txn</li> <li>ML algos can use other attributes of user like IP, device, day of time etc to detect fraud.</li> <li>We will use supervised learning.</li> <li>80% time in EDA and DC.</li> </ul> <p>Anomaly Detection</p> <p>-</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#sa4-term-4","title":"SA4 - Term 4","text":"<p>Kaggle</p> <p>Linear Regression is relation in observations (\\(x_i\\) : set of variables) and outcome (\\(y\\) : variable of interest) which tells us how x variables explain y. It is a line which has shortest distance to all possible outcomes from observation.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#covariance-and-correlation","title":"Covariance and Correlation","text":"<p>Covariance is variation of data from the mean. The value depends on unit and is not standardized.</p> <p>To calculate, find variation of each point from mean and then the product of variations. Then sum these variations and divide by n-1:</p> \\[Cov(Y,X) = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x})\\] <p>Here, +/- tells us relation but value does not tell the strenght as it varies with the unit of measure. We standardise values to find strength, in correlation.</p> <p>Correlation is how variables are linearly related to each other. It is standardized and does not change with the unit of X. The sign tells us +/- relation and the value tells us the strength. \\(cor(Y,X) = 0\\) does not mean that they are not related, it only means that they are not linearly related as they may have non-linear relation, like, $ Y = 50 - X^2 $.</p> <p>To calculate divide the diff from mean by standard deviation:</p> \\[ Cor(Y,X) = \\frac{Cov(Y,X)}{S_xS_y} = \\frac{1}{n-1}\\sum_{i=1}^{n}\\left (\\frac{y_i - \\bar{y}}{S_y}  \\right )\\left (\\frac{x_i - \\bar{x}}{S_x}  \\right ) = \\frac{\\sum(y_i - \\bar{y})(x_i - \\bar{x})}{\\sqrt{\\sum(y_i - \\bar{y})^2\\sum(x_i - \\bar{x})^2}}  \\]"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#use-of-linear-regression","title":"Use of Linear Regression","text":"<ul> <li> <p>Relation: To understand relation between X and Y, e.g. is sales related to advertisement, location, color etc? If so how, +/-, less/more.</p> </li> <li> <p>Prediction: To make predictions about Y variable, once we train the model (equation) from test data we can then predict Y from new X varables.</p> </li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#steps-in-linear-regression","title":"Steps in Linear Regression","text":"<p>We often do, modelling, estimation, inference, prediction and evaluation. And them may be repeat the cycle with corrections.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#modelling-define-develop-a-regression-model","title":"Modelling (define): Develop a regression model","text":"<p>The initial step in doing linear regression is to model the data. In this, we identify the variables of interest that we think are of importance in expaining y. This is usually based on past experienece. The variables are not exhaustive. Simple linear regression is formulated by following equation.</p> \\[ Y = \\beta_0 + \\beta_1 X + \\varepsilon  \\] <p>\\(\\beta_k\\) are called regression coefficients. We estimate these using the sample data.</p> <p>\\(\\epsilon\\) is the random error. This occurs because we might be missing some important x variable or x and y variable might not be linearly related. This is difference of actual point from the fitted point. This is also measure of goodness of fit of the regression model.</p> <p>Modelling should take 95% of time, thinking about variables, x variables. The remaining steps below, estimation, inference and prediction, should take 5% of the time.</p> <p>For example, price/advertising cost/promotion costs</p> <ul> <li>e.g. \\(UnitsSold = \\beta_0 + \\beta_1 Price + \\beta_2 AdExp + \\beta_3 PromExp + \\epsilon\\)</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#estimation-fit-using-software-to-estimate-the-model","title":"Estimation (fit): Using software to estimate the model","text":"<p>Prior data is required for each X, it could be collected over time. Use excel/r/python to fit the model. Linear regression gives estimates of regression coefficients, or predicted weights, denoted by \\(b_0, b_1, ..., b_r\\). We use least squares method, min sum of vertical distances,</p> \\[ \\hat\\beta_1 = \\frac{Cov(Y,X)}{Var(X)} = Cor(Y,X)\\frac{S_y}{S_x} \\] <p>Here, we have an assumption that Y and X are linearly related hence the estimated coefficient we got needs to be checked against this assumption before drawing any conclusion..</p> <p>It gives us estimated regression function</p> \\[ y = f(X) = b_0 + b_1x_1 + ... + b_kx_k \\] <p>Now for each i'th observations, or i'th row, we can put in the function to get estimated or predicted response, \\(f(X_i)\\). This should be as close to actual reponse \\(y_i\\). The difference, \\(y_i - f(X_i)\\) is called residuals. Now our aim is always to predict the beta estimates such that it minimizes the residuals, or deviation from actual y.</p> <p>To get best predicted weights we minimize the sum of squared residuals (SSR). for all i observations.</p> \\[ SSR = \\sum_i(y_i - f(X_i))^2 \\] <p>For one x it is called simple linear regression, for more than one x, it is called multiple linear regression. In multiple linear regression with two x, it represents a regression plane in a three-dimensional space. The goal of regression is to determine the values of the weights \\(b_0, b_1 and b_k\\) such that this plane is as close as possible to the actual responses and yield the minimal SSR.</p> <p>We also get regression output.Also here, we should get expected sign of estimators, else there might be some problem, like, multi-colinearity.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#inference-interpreting-the-estimated-regression-model","title":"Inference: Interpreting the estimated regression model","text":"<p>Inference in unerstanding the estimators and other stats from linear regression summary output. \\(\\beta_1\\) is, when the \\(X_1\\) variable increases by one unit then the Y variable increases by \\(\\beta_1\\) units, all other variables in the model being kept at constant. So on for other betas.</p> <p>For eg, \\(SalesUnits = -25096.83 - 5055.27Price + 648.61AdExp + 1802.61PromExp\\). Here, \\(\\beta_0\\) is fixed cost of production, without making any unit.</p> <p>\\(\\beta_0\\) is the value of Y when all X are 0. All \\(x_i\\) are 0  needs to be relevant. It can be relevant when we talk about fixed cost of production. So units produced can be 0 but still b0 would be fixed cost. But in our sales model, we cannot infer that when price of product is 0, then b0 is number of untis sold. We need to make managerally relevant interpretation of \\(beta_0\\). To improve \\(\\beta_0\\) estimation we need to adjust data. We can change data by, say, x= x - mean of x. This will keep all coefficients same but will change \\(b_0\\). In this case all x will be 0 only if x is equal to mean of x. Hence, we are saying that, when our ad expediture is equal to average of ad expenditure in past two years then units sold is \\(b_0\\). Rather than saying that when ad expenditre is zero in the original case. When we cannot make x equal to 0, for eg, \\(Income = b_0 + b_1Age\\), here \\(b_0\\) is only to fit the beta to the model but we cannot make inference that it is the minimum income, no!</p> <p>Regression line through Origin means that model has no intercept or \\(b_0\\). The residuals do not necessarily add up to zero. SST != SSE + SSR and \\(R^2\\) identities are also not true.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#prediction-making-predictions-about-y-variable","title":"Prediction: Making predictions about Y variable","text":"<p>We can make predictions of Y value based on Xs, put value of Xs and beta estimators to get predicted Y value. For eg, create 3rd scenarios, how much to set price, how much to spend on ad and promotions, based on these scenarios we get Y that can help us decide on how to increase sales/profit. The farther the X is from the X-bar mean, the larger is the Std Err. So we should take precaution if predicting value farther from our observations.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#evaluate-determine-how-accurate-the-models-predictions-are","title":"Evaluate: Determine how accurate the model's predictions are","text":"<p>Quality of Fit or goodness of fit tells us how good our model predicts. Larger t, or smaller p, means strong linear relationship b/w x and y, use it to keep/remove observations in model. The scatter plot of Y and Ycap can also tell us the strength. The closer the stronger, this is same as Cor(Y,Ycap).</p> <p>Needs update.. We can also measure by computing,</p> <ul> <li>SST, total sum of squared deviations</li> <li>SSR, sum of squares due to regression</li> <li>SSE, smu of squared errors (residuals)</li> <li>They are related: SST = SSR + SSE</li> </ul> \\[R^2 = SSR/SST = 1 - SSE/SST = Cor(Y,X)^2 = Cor(Y,Yhat)^2 = R^2\\] <p>0 &lt; R^2 &lt; 1, SSE&lt;SST. Higher R^2 means strong linear relationship.</p> <p>Repeat steps, modelling, estimation, inference &amp; prediction, again to improve the model.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#standardizing","title":"Standardizing","text":"<p>We might need to standardize the variables when we want to compare them for their effect on expaining Y. Standardizing makes them unit free, hence we can compare them. they can be standardized by doing \\((x-\\mu)/\\sigma\\) for all X and Y. Then we can run the regression model and interpret the coefficients. Here, one std dev change in X changes Y by b1 std dev.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#missing-values","title":"Missing Values","text":"<p>We need to handle missing missing values in the data. THere are many techniques to do this. Delete if you have lots of data.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#assumptions","title":"Assumptions","text":"<p>There are a few assumptions that we have considered when we have estimated the coefficients. One of them is that all \\(\\sum \\hat{y} - y\\) or \\(\\epsilon\\) is normally distributed around 0. The more we deviate from this assumption, the bad would our model fit.</p> \\[\\epsilon \\sim N(0,\\sigma)\\] <p>Since we have this assumption and we have estimated betas, we can do a hypothesis test around this.</p> <p>Now, CLT says that sample mean has Normal distribution with mean is population mean, and std is pop sd / root n, $ \\bar{x} \\sim N(\\mu,\\sigma/\\sqrt{n}) $ . We actually never know the population mean. We estimate it from sample. Similarly we never know actual betas, we only estimate the bees from sample training data. Hence, all bees follow a norml distn with mean beta and std is std of beta.</p> \\[b_k \\sim N(\\beta_k, std_{b_k})\\] <p>Skipping lot of derivations, leads us to below equation that we can use for hypothesis testing.</p> \\[\\frac{b_0 - \\beta_0}{S_{b_0}} \\sim t_{n-k-1}\\] <p>Here, \\(S_{b_0}\\) is std err of \\(b_0\\), n is observations, k is number of x variables.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#hypothesis-tests","title":"Hypothesis Tests","text":"<p>Hypothesis test is done to handle the uncertainities of the estimates from sample. \\(b_0\\) is estimate of beta, hence it is not a true value and we need to  have a confidence interval around it.</p> <p>For eg, Toy example, for ad_expense impact on units sold, the coefficient estimate is 600 but sales people say it should be around 300, then we have to check. The estimate is from sample, hence estimate and is uncertain, it is a belief, and it can be accepted or rejected. Hence, Hypothesis test. We need to formulate a hypothesis test for this. If SME says 1000 dollar increase in ad increses units sold by 300 units. Then,</p> <p>Ho: beta2 = 300, this is claim made, hence mu Ha: beta2 != 300</p> <p>Method 1: \\(x = b_2 = 648.6 ; \\mu = \\beta_2 = 300 ; \\sigma = S_{b_2} = 209\\) from regression output. Now it get back to std value and t-distribution. Our estimated coeff, b2, follows a t-dist, its std value = (x-mu)/sigma, this is t-statistic. It is 1.67, or 1.67 std to the right of mu. Now on the graph, check if this lies in rejection region for a certain confidence level. Here, conf level is 95%, so alpha = 0.05, and it is two tail test, so we will use \\(\\alpha/2=0.025\\), deg of freedom is n-k-1, residuals. Use T.INV to get std value from probablity and deg of freedom, using <code>T.INV(0.025,20)</code> it gives cutoff value as -2.71. So if t-statistic is beyond +-2.71 then we are in rejection region, but as t-statistic does not lie in either region hence we fail to reject the NULL hypothesis. Thus, our data does not has enough evidence to not reject the NULL hypothesis. Hence, the claim may be true. Out data has much noise.</p> <p>Method 2: p-value is probability that the sample mean would be &gt;= sample mean, given NULL hypothesis (mean is somevalue) is true. Here, p-value is twice the value cutoff by t-statistic in two tail test. Use the t-statistic found above to calculate the p-value. if p-value is lower than alpha then we can reject the NULL hypothesis, p-value = 2X(1-T.DIST(1.67,20,TRUE)) = 0.11, since .11 &gt; 0.05 alpha. Hence, we fail to reject the NULL hypothesis. Remember: When p is low the NULL has to go.</p> <p>Method 3: Use confidence interval in regression output. If the claim in in CI, then we fail to reject the claim.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#regression-output","title":"Regression Output","text":"<p>Every statistical software nowadays produces regression summary or output. This can answer many questions related to variables.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#r-square","title":"R-Square","text":"<p>It tells how much percentage of Y is explained by the observations. Rsq may be low, then also model may have value as coefficients tell us the marginal impact. Rsq is imp but we should also consider the algebric signs of coefficients, their magniture should be plausible, they should be precisely estimated, and also check if you have missed any imp right-hand-side variable.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#p-values","title":"P-Values","text":"<p>The p-values in regression output are the values for a hypothesis test in which we say that the true value of the coefficient is 0, i.e., weather one unit change in x has 0 impact on y. By looking at them, we can tell which observation is significant in explaining my Y variable. Usually, if p-value if higher than 0.05 then the variable becomes insignificant in explaining Y.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#coefficients","title":"Coefficients","text":"<p>They are unit dependent, their magnitude cannot be used for comparision. They tell us the change in Y variable for one unit change in them. If you need to compare then stanrdize them.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#confidence-intervals","title":"Confidence Intervals","text":"<p>They tell us CI at a certain confidence percentage. They are CI for estimates of betas.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#anomalies-of-regression","title":"Anomalies of Regression","text":"<p>There are many problems, like</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#multi-colinearity","title":"Multi-Colinearity","text":"<p>We might get high Rsq which means model is good fit, but also high p-value which makes observations insignificant or unexpected signs of coefficients, this might be problem of multicollinearity. It occurs when there is a nearly linear relationship among a set of explanatory variables, i.e., they are highly correlated, usually more than 90%. To solve it do a pair-wise correlation and just drop one of the correlated variable.</p> <p>It might not be a problem when we only have to predict but not understand individual observation effect.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#focused-session-on-big-data-analytics","title":"Focused Session on Big Data Analytics","text":"<p>Find a polar bear in images from cameras in forest.</p> <ul> <li>use azure iot to get images from camera</li> <li><code>devices.json</code> has camera lat, long, key and id.</li> <li>create cloud-shell, this gives bash shell on azure.</li> <li>create a resource group (project) on azure., 'streaminglab-rg'</li> <li>create storage account 'mysamg1'</li> <li>create storage account keys to allow camera to store files to your account.</li> <li>create disk space(container) named photos(dir)</li> <li>create 'azure iot hub'AIH to recieve photos in realtime. AIH can listen to multiple IOT devices at the same time. 'myhubmg1'</li> <li>get connection string to the hub, to access the cameras.</li> <li>deploy 10 camera array to AIH, they will send camera snap in some interval.</li> <li>On laptop, use node.js for sending photos (simulated cam, else cam kernel will talk to AIH)</li> <li>npm install AIH</li> <li>create devices.json has {lat: \"\", long: \"\", key: \"\", device_id: \"\").</li> <li>create deploy.json</li> <li>node deploy.js, read devices.json, the register to AIH</li> <li>key is populated for each devices once it is registered on AIH.</li> <li>to store photos, you need <code>azure-storage</code> pkg, install using node.</li> <li>create <code>test.js</code> to upload an image to AIH with camera device id, lat, long, timestamp details.</li> <li>Create 'stream analytics job'SAJ under IOT, 'polar_bear_analytics', this will take input from AIH and send the image to 'Vision Analytics'</li> <li>stream, clean, not all photo to ml</li> <li>In SAJ we can query based on window, timestamps, camera or other dimentions of data.</li> <li>SAJ -&gt; Computer Vision APi (CVA), for this use cloud-functions</li> <li>go to function app, make func, (streaminglab-rg)</li> <li>create 'http trigger func'</li> <li>now cretae 'run.js' that simulates real time, it uploads in random time interval.</li> <li>start SAJ</li> <li>SAJ sends filtered data to clout function which at the moment logs to console.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#custom-vision","title":"Custom Vision","text":"<ul> <li>Create resource 'polar bear r1'</li> <li>kind: CognitiveServices</li> <li>the upload training images with tags.</li> <li>do quick training of the model.</li> <li>now publish this model to get a web-api.</li> <li>we can use this web-api in our cloud ml function, then save the perdiction result to sql server.</li> <li>create sql server, myservermg1,</li> <li>configure the server to allow zure services, create db and table.</li> <li>update the function with js code to read preds from web-api and store results in db.</li> </ul> <p>For counting, use regression model.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#ml-classes","title":"ML Classes","text":"<p>ML in practise:</p> <ul> <li>most of the time goes in ML model building</li> <li>Auto ML does not works always</li> <li>data -&gt; model -&gt; predict</li> <li>Learning -</li> <li>p-value tell if the model is correct. it is confidence</li> <li>missing values and outliers k liye seek domain guy/customer</li> <li>touch more on inference side.</li> <li>number has no value, inference has value in corp.</li> </ul> <p>K-means algorithm, hands on:</p> <ol> <li>decide number of cluster, k=2</li> <li>initialize random data points as c1(x1, y1), c2(x2, y2).</li> <li>now find eucladian distance (ED) between between all the points and all the centroids.</li> <li>ED = [x1-x2)^2 + (y1-y2)^2]^0.5</li> <li>now the point belongs to centroid to which they have minimum distance.</li> <li>now calculate new c1, c2 by finding average of all new points belonging to the cluster.</li> <li>repeat step 3 to 6, until you get same c1,c2 as in last iteration.</li> </ol> <p>K-Means Variations:</p> <ul> <li>logic and flow remains same, but we can change the way we calculate the distance, hamming, manhattan etc. This changes the centroid and the point belonging to it.</li> </ul> <p>Business Scenario \ud83d\udcbc :</p> <ul> <li>Client may say that no, point A belongs to other cluster</li> <li>Change clustering method/k value/etc</li> <li>so no clustering is best, it can only be acceptable, by client.</li> </ul> <p>Hierarchical Clustering:</p> <ul> <li>Find dist of each point with everyother point. 5C2.</li> <li>create a 5X5 matrix with values as distance bw points</li> <li>in dendogram, y-axis is ED between points and x-axis is points.</li> <li>This is agglomerative clustering.</li> <li>Now any cut on Y axis gives us the clusters. </li> </ul> <p>Uber Case Study:</p> <ul> <li>Kmeans clustering to find centroid as place from where all rides can be catered.</li> </ul>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#knowledge-graphs","title":"Knowledge Graphs","text":"<p>Patters:</p> <ul> <li>How different subjects are related, find the connections, eg, Trump Boeing, Binny Bansal and Infosys.</li> <li>if business prob requires connections then build graphs</li> <li>Subject is node, edge is relationship.</li> <li>we can analyse customer care text data, and get insights</li> </ul> <p>-</p> <p>only ISB above ^</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#ai-image-generator-notes","title":"AI image generator Notes","text":"<ul> <li>GAN is used to genrete original like images</li> <li>Deep Convolutional Generative Adverserial Networks (or DCGAN) are a deep learning architecture that generate outputs similar to the data in the training set.</li> <li>This person does not exist.com</li> <li>nVidea research lab, start with low resolution and keep training to make a full resolution image.</li> <li>two ai neural network work against each other to generate and eveluate the images.</li> <li>fake images, fake media</li> <li>computing power is still issue</li> <li>lip syncing image to audio</li> <li>dataGrid, deepFake AI models</li> </ul> <p>----;</p> <p>Advanced Management Programme in Business Analytics (AMPBA)</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#foundation-term","title":"Foundation Term","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#probability-and-statistics-using-r","title":"Probability and Statistics using R","text":"<p>In the VY notebook</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#data-analysis-using-python-and-introduction-to-databases","title":"Data Analysis using Python and Introduction to Databases","text":"<p>In Kaggle notebook, all python numpy basics and eda examples.</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#miscellaneous","title":"Miscellaneous","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#additional-sessions","title":"Additional Sessions","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#term-1","title":"Term 1","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#business-communication","title":"Business Communication","text":"<p>Mihir, notes in md in folder</p>"},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#big-data-management-1","title":"Big Data Management-1","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#data-visualization","title":"Data Visualization","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#statistical-analysis-1","title":"Statistical Analysis-1","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#data-collection","title":"Data Collection","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#term-2","title":"Term 2","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#foundational-project-1","title":"Foundational Project-1","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#big-data-management-2","title":"Big Data Management-2","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#statistical-analysis-2","title":"Statistical Analysis-2","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#text-analytics","title":"Text Analytics","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#optimization","title":"Optimization","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#term-3","title":"Term 3","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#statistical-analysis-3","title":"Statistical Analysis-3","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#machine-learning-unsupervised-learning","title":"Machine Learning (Unsupervised Learning","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#contemporary-topics-1","title":"Contemporary Topics-1","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#big-data-applications","title":"Big Data Applications","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#advanced-optimization-and-simulation","title":"Advanced Optimization and Simulation","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#term-4","title":"Term 4","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#foundational-project-2","title":"Foundational Project 2","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#statistical-analysis-4","title":"Statistical Analysis 4","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#machine-learning-unsupervised-learning-2","title":"Machine Learning - Unsupervised Learning 2","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#machine-learning-supervised-learning-1","title":"Machine Learning - Supervised Learning 1","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#forecasting-analytics","title":"Forecasting Analytics","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#term-5","title":"Term 5","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#contemporary-topics-2","title":"Contemporary Topics 2","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#deep-learning","title":"Deep Learning","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#supply-chain-analytics","title":"Supply Chain Analytics","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#marketing-analytics","title":"Marketing Analytics","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#machine-learning-supervised-learning-2","title":"Machine Learning (Supervised Learning 2)","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#term-6","title":"Term 6","text":""},{"location":"draft/ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/#contemporary-topics-3","title":"Contemporary Topics 3","text":""},{"location":"draft/ITVersity%20-%20HDPCD%2077/","title":"HDPCD","text":"<p>```sql create alias of IP address and give it a name in /etc/host</p> <ul> <li> <p>IPs and Ports: 8020  HDFS port 8888  Sandbox Home, Welcome screen 8080  Amabri, web interface to acces and monitor everything. 2222  SSH 4200  Web terminal 21000 Atlas 9995  zeppelin 15000 Falcon 6080  Ranger 50070 Hadoop information, name node to access files. 3306  mysql</p> </li> <li> <p>how to SSH</p> </li> </ul>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#usage","title":"Usage:","text":"<p>ssh @ -p ;"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#example","title":"Example:","text":"<p>ssh root@127.0.0.1 -p 2222   ssh root@sandbox.hortonworks.com -p 2222</p> <ul> <li>Input local file into HDFS using Hadoop shell</li> </ul> <p>ip:50070 - takes you to name node to access files in HDFS.</p> <p>id hive id hdfs shows the process ID for both the process</p> <ul> <li>HDFS directories</li> </ul> <p>to create directory under HDFS login as user when we ssh root@sandbox.hortonworks.com then it is logged in as a root.</p> <p>/root in sandbox is the home directory of linux not hdfs. In this dir we place data and other local files.</p> <p>Other users are each services. Like hcat,hive,maria_dev,oozie,root,solr,spark,unit,yarn.</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#make-dir","title":"make dir","text":"<p>sudo -u hdfs hadoop fs -mkdir /user/root</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#list-dir","title":"list dir","text":"<p>hadoop fs -ls /user here we have dir of all users in hdfs /apps/hive/warehouse - it has all hive managed tables and databases. /user/root - we created this for development purpose.</p> <ol> <li>** HDPCD - Copy files to HDFS</li> </ol>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#change-owner","title":"change owner","text":"<p>sudo -u hdfs hadoop fs -chown root:hdfs /user/root</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#by-default-it-is-above-only","title":"by default it is above only.","text":""},{"location":"draft/ITVersity%20-%20HDPCD%2077/#copying-the-data","title":"copying the data","text":"<p>hadoop fs -copyFromLocal /root/test /user/root</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#for-2nd-practice-we-are-using-hdfc-userhdpcd","title":"for 2nd practice we are using hdfc /user/hdpcd/","text":"<p>cd /etc/hadoop/conf core-site.xml</p> <p>fs.defaultFS parameter has information about the port number on which we connect to create the files. Like:</p> <pre><code>&lt;property&gt;                                                                                                             \n  &lt;name&gt;fs.defaultFS&lt;/name&gt;                                                                                            \n  &lt;value&gt;hdfs://sandbox.hortonworks.com:8020&lt;/value&gt;                                                                   \n  &lt;final&gt;true&lt;/final&gt;                                                                                                  \n&lt;/property&gt;\n</code></pre>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#imp-to-copy-file-to-a-particular-name-node-pass-ip-addressport-8020-to-copy-file-to-that-particular-name-node","title":"imp: to copy file to a particular name node, pass ip address:port 8020 to copy file to that particular name node","text":"<p>hadoop fs -ls http://192.xxx.xxx.121:8020/user/root</p> <p>e.g. hadoop fs -ls hdfs://sandbox.hortonworks.com/user/</p> <p>this will list the file on above name node.</p> <p>it takes connections only on port 8020. rest are refused.</p> <ol> <li>** Create new dir in HDFS:</li> </ol> <p>hadoop fs -mkdir /user/root/ hadoop fs -mkdir -p /user/root/dir1/dir2 #will create missing dir1 also. hadoop fs -ls -R /user/root # dispays sub directories as well</p> <p>hadoop fs -mkdir /dir/t{1..10} # creates dir t1,t2,...,t10 hadoop fs -mkdir /dir/t{1,2} # creates dir t1,t2</p> <p>*** SQOOP ***</p> <p>** 10. SQOOP Introduction (very important): </p> <p>https://raw.githubusercontent.com/dgadiraju/code/master/hadoop/edw/hdp/sqoop/sqoop_demo.txt #all commands</p> <p>default port number for mysql is 3306 else it will be specified. also dont use localhost. Use full IP or hostname.</p> <p>hostname -f #tells the hostname</p> <p>ls -lrt /usr/share/java/mysql-connector-java*.jar #this is connector to jdbc driver</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#find-all-jars","title":"find all jars","text":"<p>find / -name \"mysql*.jar\"</p> <p>wget -o /dir  #downloads the latest file <p>unlink and relink to new version of file downloaded.</p> <p>unlink path.jar</p> <p>ln -s path/jar link</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#run-sqoop-import","title":"Run sqoop import","text":"<p>replace jar in hadoop and hadoop yarn</p> <p>hadoop fs -ls /apps/hive/warehouse/retail_stage.db</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#lists-all-the-tables-imported-to-hive","title":"lists all the tables imported to HIVE","text":"<p>** 11 SQOOP: list and eval</p> <p>ps -ef | grep -i manager #tells weather node manager and resource manager are running or not</p> <p>ps -ef | grep -i node #should be upa and running</p> <p>ps -fu hdfs ps -fu yarn</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#these-also-show-running-tasks","title":"these also show running tasks","text":"<p>scp -P 2222 code/hdpcd/retail_db.sql root@sandbox.hortonworks.com:/root/data/retail_db.sql copies files from mac to remote vm via scp</p> <p>mysql&gt; create database retail_db;</p> <p>mysql&gt; create user retail_dba identified by 'hadoop';</p> <p>mysql&gt; grant all on retail_db.* to retail_dba;</p> <p>mysql&gt; flush privileges;</p> <p>mysql -u retail_dba -p</p> <p>mysql&gt; source /root/data/retail_db.sql;</p> <p>mysql&gt; show tables;              </p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#to-verify-the-results","title":"to verify the results","text":"<p>mysql&gt; select * from categories limit 10;</p> <p>hadoop fs -copyFromLocal /root/data/dummyText.txt /user/root</p> <p>mysql -u retail_dba -p #to check if mysql is running</p> <p>hadoop fs -mkdir /user/root/sqoop_import # just a dir to import</p> <p>sqoop help #show commands and how to use the same.</p> <p>to list databases use hostname only. while to list databases use connection param /database_name</p> <p>sqoop eval #used to run a query after --query \"my sql\"</p> <p>list-databases</p> <p>[root@sandbox data]# sqoop list-databases \\</p> <p>--connect \"jdbc:mysql://sandbox.hortonworks.com:3306\" \\ --username retail_dba \\ --password hadoop</p> <p>sqoop list-tables --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" --username retail_dba --password hadoop</p> <ul> <li>Instead of using the --table, --columns and --where    arguments, you can specify a SQL statement with the --query argument</li> </ul> <p>e.g. sqoop import    --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\"    --username retail_dba --password hadoop    --query \"select * from categories where category_department_id=8 and $CONDITIONS\"   --target-dir /user/hdpcd/d1/categories_query/    -m 1    --delete-target-dir</p> <ul> <li>in sqoop import, the target directory should not exist.</li> </ul> <p>sqoop import \\   --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\   --username=retail_dba \\   --password=hadoop \\   --table departments \\   --as-textfile \\   --target-dir=/user/root/departments</p> <p>sqoop eval \\</p> <p>--connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\ --username retail_dba \\ --password hadoop \\ --query \"select count(1) from order_items\"</p> <ul> <li> <p>in sqoop import-all-tables there is warehouse-dir and not target-dir.</p> </li> <li> <p>during import tablename.java file is created in dir from where we execute the import/import-all-tables statement.</p> </li> <li> <p>these are basically ORM java classes.</p> </li> <li> <p>when saving as avro data file. tablename.avro file is created form where we launched the import command. </p> </li> <li>It has table details and columnn names in JSON format.</li> </ul> <p>sqoop import-all-tables \\ -m 12 \\ --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\ --username=retail_dba \\ --password=hadoop \\ --as-avrodatafile \\ --warehouse-dir=/apps/hive/warehouse/retail_stage.db</p> <p>add driver parameter then above import will work --driver com.mysql.jdbc.Driver</p> <ul> <li>To create avro format externa table in hive. We need to use avro classes and avsc file created while importing data from sqoop to avro file format.   Copy the avsc file to hdfs dir and provide that in avro.schema.url</li> </ul> <p>hadoop fs -put departments.avsc /user/hdpcd/departments.avsc</p> <p>e.g.:</p> <p>CREATE EXTERNAL TABLE departments ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' LOCATION 'hdfs:///user/hdpcd/departments_avro' TBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/hdpcd/departments.avsc');</p> <p>eval command can be used to run query and stored procedures as well.</p> <p>** 12. SQOOP Import</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#sqoop-import-refer-sqoop-documentation-as-well","title":"sqoop import, refer sqoop documentation as well.","text":"<p>sqoop --help #use when u get stuck</p> <p>*imp -m is used as number of mappers in import-all-tables</p> <p>-m 12 #means 12 parallel threads will run together.</p> <p>--as-avrodatafile #saves in avro format. same as JSON</p> <p>--warehouse-dir #give directory where data needs to be saved</p> <p>boundary conditions are set in background to break data into number of buckets.(min,max) Then this data is inserted into db by usign where clause on data.  Then this data is processed using parallel threads passed in -m or --num-mappers</p> <p>the data is placed in import data location and we can see data using ls command. table is a folder and inside that folder we have different files which have table data in part/</p> <p>the parts are made by -m command. default is 4.</p> <p>so if we pass -m 12, the data will be stored in 12 parts.</p> <p>the data in file is saved as CSV as default delimiter is comma.</p> <p>to verify records inserted, use:</p> <p>hadoop fs -cat /dir/dw/table/part-m-*|wc -l</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#this-will-cat-all-parts-and-count-the-records","title":"this will cat all parts and count the records.","text":"<p>same can be verified using count(*) in :</p> <p>sqoop eval --query \"\" or mysql.</p> <p>** 13 -  SQOOP Import - hive import</p> <p>hadoop fs -ls /user/hive/warehouse </p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#this-is-where-all-data-of-hive-lives","title":"this is where all data of hive lives","text":"<p>.db extension directories represent database.</p> <p>hive #takes to hive CLI</p> <p>hive&gt; create database sqoop_import;</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#creates-database","title":"creates database","text":"<p>in hive dfs =&gt; hadoop fs</p> <p>from above command sqoop_import.db directory will be created under warehouse directory</p> <p>hive&gt;show databases; #shows databases.</p> <p>use db; # to show databases.</p> <p>if we dont specify the compression codec then it is compressed as gzip. it is default compression.</p> <p>--outdir java_files  creats plane old java pojo class. it is optional.</p> <p>describe formatted departments; shows detailed information, location.</p> <p>dfs -du -s -h /path</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#gives-size-of-directory-in-which-data-is-stored","title":"gives size of directory in which data is stored.","text":"<p>** 14. SQOOP import - add on</p> <p>--hive-database can be used to import to a particalar database.</p> <p>e.g.: sqoop import    --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\"    --username retail_dba --password hadoop --table orders    --hive-import --create-hive-table    --hive-database hdpcd_d1    --driver com.mysql.jdbc.Driver</p> <p>This also does the same work: sqoop import    --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\"    --username retail_dba --password hadoop --table orders    --hive-import --create-hive-table --hive-table hdpcd_d1.orders2    --driver com.mysql.jdbc.Driver</p> <p>** 15. sqoop import</p> <p>we have something called boundry query  it is used to load the data. It selects min and max from id (primary key)</p> <p>no of mappers provide how data needs to be splitted before loading. It creates as number of buckets.</p> <p>boundary-query is important as it can improve performance of import. The query may take time to compute min and max. so we can use boundry query.</p> <p>--table and --query cannot be used together.</p> <p>e.g. -- Boundary Query and columns sqoop import \\   --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\   --username=retail_dba \\   --password=hadoop \\   --table departments \\   --target-dir /user/root/departments \\   -m 2 \\   --boundary-query \"select 2, 8 from departments limit 1\" \\   --columns department_id,department_name</p> <p>to import table using sqoop import command and table is without primaty key then we have to use --split-by or give number of mappers as 1</p> <p>to use where clause we have to pass conditions in --where as argument.</p> <ul> <li>split by e.g.</li> </ul> <p>-- query and split-by sqoop import \\   --connect \"jdbc:mysql://sandbox.hortonworks.com:3306/retail_db\" \\   --username=retail_dba \\   --password=hadoop \\   --query=\"select * from orders join order_items on orders.order_id = order_items.order_item_order_id where $CONDITIONS\" \\   --target-dir /user/root/order_join \\   --split-by order_id \\   --num-mappers 4</p> <p>** 16 - sqoop import</p> <p>import from sqoop into hive, an particular table</p> <p>hive parameters can be used to import data into the table</p> <p>we have many parameters like table, dir, overwrite.  These are self explainatory.</p> <p>in case of error, we might need to clean up the staging directory where the hive imports the data.</p> <p>It is a dir in non hive warehouse fs. hdfs://sandbox.hortonworks.com:8020/user/root/departments</p> <ul> <li>to load to a particular database use:   --hive-table retail_ods2.departments \\</li> </ul> <p>--create-hive-table  can be used to create table at time of load.</p> <p>--where  it can be given as parameters. col  value <p>can be given in single/double quotes.</p> <p>--append simply appends to end of file.</p> <p>** 17  SQOOP Import - incremental import</p> <p>it is used to insert data which is left from being imported. it uses last value inserted to load remaining values.</p> <p>so one way to import incremental is to use where clause. like where id&gt;7; this is traditional way.</p> <p>sqoop way is: to use  --check-column #like id --incremental #append --last-value #7</p> <p>or when using --incremental as lastmodified then pass timestamp in #--last-value</p> <p>e.g.:   --check-column \"department_id\" \\   --incremental append \\   --last-value 7 \\</p> <ul> <li>SQOOP job: sqoop job --create sqoop_job \\   -- import \\   --connect ...</li> </ul> <p>other job related commands: sqoop job --list</p> <p>sqoop job --show sqoop_job</p> <p>sqoop job --exec sqoop_job</p> <p>** 18. sqoop export</p> <p>HDFS to mysql</p> <p>export appends to the table in mysql by pusing duplicate records as well.</p> <p>use simple export command with various arguments it imports into mysql table. we need to pass the direcctory from which data is to be picked.</p> <p>by default it inserts data and does not update the data. to update the data we need to use separate argument.</p> <p>so in export the files are divided into splits which is parallel processing of threads. these threads depends on number of mappers. so for 10gb file. if we have 128mb block then we will have roughly 80files. now if we have 4 mappers then each mapper will process 20 files in each thread.</p> <p>--update-key department_id \\   --update-mode allowinsert</p> <p>** 19. SQOOP export - merge/upset</p> <p>for adding data to mysql, we can create file in local fs and then move it to hdfs, then we have to use export command to bring those records to mysql. we can use update-key and update-mode to append the data.</p> <p>key can be only primary or unique key only. Composite key can be given by comma seperated. If we dont pass key then we may get duplicate records.</p> <p>mode can be updateonly or allowinsert.</p> <p>so old record will be updated and new record will be inserted.</p> <p>in updateonly mode new data is not inserted only matching key record gets updated. in allowinsert mode both update and insert occurs.</p> <p>*imp  If we dont have primary key in mysql table then the update will not happen, only new records will be inserted.</p> <p>*how it fails Insert data into existing table without passing --update-key parameter. the export fails. It fails because duplicare record is inserted for primary key.  If mapper fails it runs the same task 4 times.  this is built in default feature.  In this case it will fail all 4 times with same error duplicate entries for primary key.</p> <p>In other cases, when we dont have primary key, then if export fails then it mapper will commit in batches after say 1000 successfull insert and beyond that whatever fails is attempted to be inserted 4 times. So data may have 4 duplicate records for last batch. It might be the case when we are inserting null in not null column.</p> <p>so sqoop export provides option for staging table with command --clear-staging-table In this case data in staging table will be cleared and new records will be inserted in staging table. hence, target table will be secured.</p> <p>we have to give following parameters: --Merge sqoop merge    --merge-key department_id \\   --new-data /user/root/sqoop_merge/departments_delta \\   --onto /user/root/sqoop_merge/departments \\   --target-dir /user/root/sqoop_merge/departments_stage \\   --class-name departments \\   --jar-file  <p>** 20. sqoop import delimiters</p> <p>fields terminated by and line terminated by null-string, and  null-non-string</p> <p>so if we have data having ',' or other delimiters as value then we have to enclose the values. this can be done by using  --enclosed-by \\\" since double quotes is special charachter hence we have to escape it by back slash.</p> <p>Now all data in file will be enclosed like \"Fan Shop\"</p> <p>--escaped-by special characters in data can be escaped using this.</p> <p>--fields and lines we can terminate filds by say | and lines by say : and enclose by say double quotes.</p> <p>all 3 above are special characters and need to be escaped.</p> <p>--mysql-delimiters provides regular mysql delimiters.</p> <p>*Same arguments for import to HIVE.</p> <p>login to hive shell. use db; then</p> <p>describe formatted departments_test;</p> <p>this will show complete details of departments.</p> <p>when you have null data then take special care of delimiters.</p> <p>** 21.  SQOOP Export delimiters.</p> <p>-1 for numeric null and nvl for string null</p> <p>In hive null is \\u001 hive default terminated by \\001 these are octal numbers.</p> <p>input-null-string = nvl input-null-non-string = -1</p> <p>run export by above parameters</p> <p>null will be inserted in mysql table.</p> <p>** 22. Sqoop file formats</p> <p>to import the data in binary format import data in hdfs as a sequence file.</p> <p>avsc file format saves the data in somewhat json format.</p> <p>so delimiters and file formats depends on data we have in the file and  we should take care of null characters and data as well.</p> <p>** 23. Sqoop Job, Merge</p> <p>Sqoop job is a predefined job syntax which is created to run a sqoop command as and when required.</p> <p>there should be a space between import and --</p> <p>eg: sqoop job --create sqoop_job \\  -- import  -- connect  other such stuff</p> <p>**to see all jobs sqoop job --list shows a list of jobs</p> <p>sqoop job --exec to execute the job</p> <p>**sqoop Merge</p> <p>it merges data in hdfs not from db</p> <p>so it matches on an id and picks new record from the dump. we have to give a seperate new directory for merged results.</p> <p>a java file is created for each import command. we can to java_files directory to see all java files.</p> <p>when we run sqoop import we get jar file path in the logs. java class file name is same as that of table we are importing</p> <p>so on merge, new records are inserted and existing are updated with latest value.</p> <p>use any command with --help to find syntax and commands.</p> <p>** FLUME **</p> <ol> <li>** Flume Introduction how to play with memory channel is main thing to learn.</li> </ol> <p>** 25. flume configuration and starting an agent</p> <p>**starting an agent</p> <p>flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template</p> <p>cd /etc/flume/conf #this is where we have template file having sqquence generator.</p> <p>so we have 1st an agent named agent. it has 3 things: - sources, its name, type:seq, channel - channels, name, type:memory|file, capacity:100. - sinks, name, type:logger, channel</p> <p>so in example we use sequence generator to generate log kind of data that flume will read and bring in to sink/memory.</p> <p>the command we provide is like</p> <p>flume-ng agent -n agent -f /etc/conf/flume-conf.properties.template</p> <p>we can create our own flume conf file anywhere in and just provide the location to the -f parameter.</p> <p>HISTORY: cd /etc/flume/conf ls -lrt cat flume-conf.properties.template cd flume-ng agent -n agent -f /etc/flume/conf/flume-conf.properties.template </p> <p>** 26. FLUME - Memory channel Capacity</p> <p>Maily covers properties of memory.</p> <p>copy example conf file with parameters having details of net logger and web service that run on it then copy this and paste it in any directory on local machine</p> <p>mkdir flume  vi example.conf</p> <p>paste the contents from flume docs</p> <p>type bind port type capacity all are defined here</p> <p>now flume-ng agent -n a1 -f example.conf</p> <p>it will start flume agent.</p> <p>now telnet from another terminal</p> <p>telnet localhost 44444</p> <p>type what u wish to. . . . .</p> <p>this will be passed to flume agent.</p> <p>the data will be transfed form memory channel</p> <p>main focus should be on memory properties.</p> <p>kill telnet session.</p> <ul> <li>Memory: it has two properties:</li> <li>capacity and</li> <li>transaction capacity</li> </ul> <p>also,</p> <p>type,  keep-alive byteCapacityBufferPercentage byteCapacity</p> <p>may be asked to increase/decrease the capacity of various parameters</p> <p>saving data to hive/hdfs is not in scope of certi.</p> <p>memory can be cinfugured in conf file, mainly has two parameters capacity and transactionCapacity.</p> <p>Code:</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#use-a-channel-which-buffers-events-in-memory","title":"Use a channel which buffers events in memory","text":"<p>a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100</p> <p>*** PIG ***</p> <p>** 27. PIG Introduction</p> <p>pig commands, how to use. see if services are up and running. HDFS, YARN, MapReduce should be up and running</p> <p>PIG does not have any daemon so it will run if above 3 r running</p> <p>to run command u can goto pig grunt shell</p> <p>make a file in /user/root&gt;demo.txt</p> <p>bag=table tuple=row</p> <p>so when we write a pig script, behind the scene a map reduce job is genereated by api.</p> <p>this is a jar file is executed in background.</p> <p>take the job id from logs and goto ambari</p> <p>services&gt; yarn&gt; quick links&gt; resource manager using see last job that is run click on history to see details.</p> <p>if we do not write on grunt shell then we can make pig script file.</p> <ol> <li>** write and execute pig latin script.</li> </ol> <p>pig grunt&gt;exec pig_demo.pig</p> <p>this will run MapReduce job of pig.</p> <p>pig -help show all possible options to be used with pig</p> <p>** 29. PIG - Word Count</p> <p>describe shows data type of a bag.</p> <p>*explain shows MapReduce plan. how data will be read.</p> <p>we use tokenize to create bag of tuples. each touple is one word.</p> <p>words = FOREACH lines GENERATE FLATTEN (TOKENIZE(line)) as word;</p> <p>describe words; thos shows words as bag of tokens. explain words shows comma separated words.</p> <p>TOKENIZE breaks line into words and makes a bag of it FLATTEN makes words in seperate line.</p> <p>here word is alias.</p> <p>grouped = GROUP words by word;</p> <p>this will create a bag for each word.</p> <p>so key and its bag in which words are repeated.</p> <p>wordcount = FOREACH grouped GENERATE group, COUNT(words);</p> <p>this will print word and number of times for each word.</p> <p>** 30. pig execution life cycle</p> <p>all statements are validated. MapReduce job runs only when dump/illustrate/store is executed.</p> <p>** 31. LOAD data to pig relation woithout schema</p> <p>so we should have mysql installed with retail_db database in it.</p> <p>also this db should be imported to sqoop via hdfs.</p> <p>data can be imported to /user/root/sqoop_import/</p> <p>we have to use positional notation to work on relationless schema</p> <p>we can cast the colums as well</p> <p>dept_id = FOREACH departments GENERATE (int) $0;</p> <p>** 32. LOAD data to pig relation with schema</p> <p>u should have data in sqoop_import directory</p> <p>department  = LOAD 'dir/sqoop_import/departments' using PigStorage(',') AS (dept_id: int, dept_name: chararray);</p> <p>** 33. Load data from HIVE table.</p> <p>fot this we should have HIVe, database in it and tables in it,</p> <p>we have to use </p> <p>pig -useHCatalog</p> <p>this will take to grunt shell along with enbling HCatalog</p> <p>so it will use meta data of hive table and will pick data types from HCatalog.</p> <p>then when we describe the pig relation we see that all the fields are defined and have schema.</p> <p>** 34. Transform to match HIVE schema</p> <p>ok, so here we have two ways to load in pig from hive one is using: HCatalog, db.tablename</p> <p>second: giving location of hive table which is a file having data.</p> <p>in 1st case the schema is picked from HCatalog metastore</p> <p>and in 2nd case we have to define schema as (json)</p> <p>task is to match schema in both the case.</p> <p>** 35. Group the data on one or more PIG relation</p> <ul> <li>To get count of all rows group the data.</li> </ul> <p>so if we need count of all records then we can pass ALL.</p> <p>if we need count along with where a=b then we need to pass an expression to GROUP argument</p> <p>for null in pig use != '';</p> <p>** 36. Grouping one or more relation * Group by key</p> <p>we give a key to group the data and then we can count the data.</p> <p>** 37. remove null values from a relation</p> <p>so in filer we just have write by column is not null.</p> <p>** 38. Store data into a folder in HDFS</p> <p>cannot store in existing directory.</p> <p>verify if data is present in hdfs directories</p> <p>then load the data into pig.</p> <p>simply instead of dump use store/</p> <p>pig will create just one file in new directory</p> <p>PigStorage is used to provide the delimiters.</p> <p>BinStorage stores the data in binary format (machine readable format).</p> <p>always validate your results.</p> <p>STORE  INTO  USING JsonStorage('|'); this stores data in json format. <p>so data can be stored as text, binary or Json format and other format as well.</p> <p>** 39. Store the data from a Pig relation into a Hive table.</p> <p>goto hive shell create datanase pig_demo;</p> <p>use pig_demo;</p> <p>create tables in hive database;</p> <p>goto pig shell -useHCatalog. verify that data is in hdfs dir.</p> <p>LOAD path USING PigStorage(',') as ...;</p> <p>on STORE pass class of HCatalog.</p> <p>STORE alias INTO 'db.tbl' USING HCatalog class();</p> <p>Schema in pig has to match with column names and datatype in hive table.  Else the STORE will not work.</p> <ol> <li>**Sort the output of a Pig relation </li> </ol> <p>load data from hive into pig alias.</p> <p>then alias = ORDER alias by $1; sorts by column 2.</p> <p>alteast 2 MapReduce jobs for orderBy</p> <p>above is without schema and sorted using positional notation.</p> <ol> <li>**Apache Pig - Remove the duplicate tuples of a Pig relation (using distinct)</li> </ol> <p>alias = DISTINCT alias [PARTITION] [PARALLEL]</p> <p>[] are used to provide reducers and performance tuning.</p> <p>distinct only works on the relations and not the columns</p> <ol> <li>** Specify the number of MapReduce job for pig script</li> </ol> <p>PARTITION BY is not in scope, it is related to hash partitioning.</p> <p>PARALLEL is used to give number of reduce tasks</p> <p>to set default parallel value for all jobs</p> <p>set default_parallel 2;</p> <p>so for each job it will use parallel tasks as many as possible.</p> <p>if we provide parallel in transformational level then that many reducers will run.</p> <ol> <li>**JOIN Theory:</li> </ol> <p>IJ, OJ: LOJ, ROJ, FOJ</p> <ol> <li> <p>** Pig - Join two datasets using Pig - 01 Introduction</p> </li> <li> <p>** PIG - INNER JOIN</p> </li> </ol> <p>in exam validate after each step COUNT_STAR is a function in pig</p> <p>validation at each step by describe or explain formatted is imp.</p> <p>use pig HCatalog so that schema can be loaded.</p> <ol> <li>OUTER JOIN</li> </ol> <p>cannot use positional notation we have to define the schema.</p> <p>without schema, join is difficult.</p> <p>to access column of a table use ::</p> <ol> <li>**Replicated join using pig</li> </ol> <p>*replicated join is done when data we need to join is small enough to fit into the memory.</p> <p>it uses replicated cache and the smaller file is distributed across all jvms for mappers</p> <p>resouce manager is 8088.</p> <ol> <li>**run pig using tez. just use -x tez to run as tez engine</li> </ol> <p>in pig.properties</p> <p>exectype=mapreduece</p> <p>this is default execution engine</p> <ol> <li>** registering JAR, define alias of UDF and invoking UDFs</li> </ol> <p>@todo:</p> <p>jar -tvf to see classes in jar</p> <p>find / -name \"piggybank.jar\"</p> <p>then in pig shell</p> <p>REGISTER the jar</p> <p>we can make alias for fully qualified class name. this alias can be used in statement.</p> <p>*** HIVE ***</p> <ol> <li> <p>Data Analysis</p> </li> <li> <p>to create orc table, create a text table and then CTAS from text file stored as orc. e.g. create table sfo_weather stored as ORC as select * from sfo_weather_text;</p> </li> <li> <p>when we create an external table in hive then it has a location, the schema, delimiters should match the data at that location.    on mataching the table is itself populated with that data.</p> </li> <li> <p>data can be loded in hive in following way:</p> </li> <li>by using load data inpath:       it can be loaded from local,       from HDFS.       can be appended or overwritten.</li> <li>by using HCatalog in pig storage.</li> <li>by CTAS.</li> <li> <p>INS*F</p> </li> <li> <p>To load data into partition table select the partition columns as well. Also if only particular partition needs to be saved them we can define them in partition (a=b,c=d). then we need not select these columns. </p> </li> </ol> <p>E.G. LOAD DATA INPATH '/user/data/pv_2008\u201006\u201008_us.txt' INTO TABLE page_view PARTITION (date='2008\u201006\u201008', country='US')</p> <p>from sfo_weather s insert into table weather_partitioned_all  partition (year,month) select s.station_name,s.DayOfmonth, s.precipitation, s.temperature_min, s.temperature_max,s.year,s.month ;</p> <ol> <li>HIVE architecture:</li> </ol> <p>HIVE = HDFS, metastore, JARs which compile sql queries to MapReduce jobs</p> <p>so in hive the database is a dir and inside it are directories of tables, inside them are files/</p> <p>for hdfs paths onlt hadoop command can go and not linux commands.</p> <p>etc/hive/conf has hive-set.xml</p> <p>use this to see db used and name and password.</p> <p>db=mysql db_name=metastore</p> <p>this has all metadata it has information about the file copied to hdfs.</p> <p>it is casesensitive</p> <p>sudo find / -name \"hive.jar\"</p> <p>these convert sql query to MapReduce jobs amd submit to hadoop cluster.</p> <ol> <li>** Write and execute hive query 01 this has a lot to cover with join and everything.</li> </ol> <p>hive set fs.defaultFS; #tells name node</p> <p>dfs -ls /user/root # tells ki hive can connect to hdfs</p> <p>simply write the hive queries and it executes as a map reduce job.</p> <p>when we load data from hdfs, that is, load data inpath, then the directory is deleted after succesfull import.</p> <ol> <li>** Write and execute hive query 02</li> </ol> <p>queries on githubusercontent</p> <ul> <li>like insert into table, we have insert overwrite table.</li> <li> <p>the partition column in table cannot be a normal column in table. It is sepecified separately.   the values are also not stored in file but are picked from folder names.</p> </li> <li> <p>to insert into partition column we ins</p> </li> <li> <p>** HIVE Managed Table.</p> </li> </ul> <p>it is simple table created using query.</p> <ol> <li>** HIVE External table.</li> </ol> <p>to copy file form mac to sandbox use scp command.</p> <p>scp host guest</p> <p>then use put to copy file to hdfs</p> <p>In EXTERNAL table we can define a location where we can store a table.</p> <p>When we delete a table only metadata is deleted adn not the files stored/</p> <p>for managed table use describe formatted and see the directory is a default one.</p> <p>in external we hve to specify</p> <p>if table is to be used by pig hive spark and others then make external table for hive use only use managed table.</p> <ol> <li>**Create retail_db tables in hive</li> </ol> <p>in exam use STORED AS AVRO  un;ess they give custom input format and output format.</p> <ol> <li>**Create Hive Partitioned Table</li> </ol> <p>create table (...)  partitioned by (col datatype)</p> <p>Type of partitioning: -list -range -hash</p> <p>these are in hive but without terminology.</p> <p>in hive we have list partitioning as partition by  and hash paartitonjing as bucket.</p> <p>so when we partition we create one more colimn on which we partjioon and this can be an existing column or can be a new column as well.</p> <p>then for each distinf records inthat column that many folders are created inside a table folder;</p> <p>so the partion column is not stored in the data file. when we do select form tbale then value of the partiton column is picked from the folder name it self.</p> <p>we can either alter table to create a static partition in which er pass a vluer for a column or we cna create dtynamic partition in which we hive created partition on runtime itself.</p> <p>strict mode should be off is we use dnamic partition.</p> <ol> <li>** BUcketed table.</li> </ol> <p>CLUSTERED BY [col] SORTED BY [col] INTO n BUCKETS</p> <p>in partition we mention col names and data types both but in case of buciket we only mention the col names and not the data type;</p> <p>you can create a bucket and partition seperately but you in case of both you first have to create a partition and then create a bucket. after creating a bucker you cannot create a partition.</p> <p>advantage of bucketing is to divide sdata into equal parts. we cna use fast sampling queries onthat data.</p> <ol> <li>** CTAS AND ORC file formats.</li> </ol> <p>ctas = create table as select.</p> <p>for ORC</p> <p>create table ..  .... use STORED AS orc as select ..</p> <p>show create table orders_orc;</p>"},{"location":"draft/ITVersity%20-%20HDPCD%2077/#shows-complete-sql-to-create-table","title":"shows complete sql to create table.","text":"<ol> <li>**Specifying file format and delimiters</li> </ol> <p>formats can be file format like text, rcm orc, paraquet, avro also we cna use cutom input format and output format. then we have to give sede class file as well.</p> <p>so just have to pass stored as .</p> <p>delimiter row format fields terminated by..</p> <ol> <li>**Load data from local file system and HDFS to Hive table</li> </ol> <p>LOAD DATA LOCAL INPATH  load data from local file system, cent os</p> <p>LOAD DATA INPATH  loads data from hdfs.</p> <p>overwrite onto will delet else will append,</p> <p>load data inpath moves the data from hdfs hence it delets from hdfs.</p> <ol> <li>**Load data from hive select query (using insert) and compressed data</li> </ol> <p>insert overwrite table select * from..</p> <p>else the data will be appended..</p> <p>so COMPRESSED will compress the data in file.</p> <p>set hive compress value to true. by default it is false.</p> <ol> <li>** DML in HIVE. insert update delete.</li> </ol> <p>there are limitations. begin commit and rollback are not supported.  rest all is auto commit.</p> <p>only orc file format is supported in update and delete.</p> <p>hive.txn.manager should be set to DBTxnManager.</p> <p>tables must be bucketed.</p> <p>so for dml operations we should have 3 things in table structure. 1. clustered into buckets. 2. stored as orc 3. tblproperties ('transactional'='true')</p> <p>then only we can make a table ready for update and delete.</p> <p>for next insert it will again create new directory, having n buckets in each.</p> <p>for update also new directoy is added. over the time compession will take place and all files will be merged into one file.</p> <p>only soft delete happens that is data becomes unavailable and will be cleaned on compession.</p> <ol> <li>**Hive joins, execution engines (tez and mr) and explain/execution plan</li> </ol> <p>set hive.execution.engine=tez;</p> <p>this will make query run in tez engine.</p> <p>so for exam remember the parameter that needs to be changed.</p> <p>*execution plan: explain can be used to explain the execution of the query.</p> <p>practice more in joins from certi point of view.</p> <ol> <li>**Hive Sub Queries and Total Ordering (ORDER BY)</li> </ol> <p>cannot use sub query in IN clause. can only be used in from clause.</p> <p>subquery should have alias.</p> <p>*SORT by: this can be used when we donot have to order entire data.</p> <p>so basically in this use order by on a derived column having an alias. like order by count in group by query.</p> <ol> <li>**Practice Exam - Getting Started.</li> </ol> <p>** Setting up AWS **</p> <p>password of amazon account. used ICICI credit card ending 8009, CVV not asked.</p> <p>Download KeyValue pair for ssh and save it.</p> <p>connect vnc on public dns and using port 5901. password for vm is hadoop, when asked in vnc viewer.</p> <p>launch terminal run $ ./start-all-services.sh this will launch ambari.</p> <p>ambari has username and password ambari.</p> <p>wait while services are up and running.</p> <p>exam overvirew gives all this information, like launching and username, password.</p> <p>perform all taks using user horton, not root. veriy by whoami</p> <p>in exam directory we have exam task.</p> <p>about 10 questions, 2sqoop, 1hdfs, 3pig, 4hive. may b 1 flume.</p> <p>stop the instance if you need break. we can continue from where we left when we resume.</p> <ul> <li>Practical:</li> </ul> <p>Public DNS: ec2-54-201-130-145.us-west-2.compute.amazonaws.com Public IP:  54.201.130.145</p> <p>To SSH:     ssh -i ~/Downloads/hwx-practise-exam.pem ubuntu@ec2-54-201-130-145.us-west-2.compute.amazonaws.com</p> <p>then sudo su - horton</p> <p>then start working by starting script.</p> <ul> <li>To copy from aws vm:</li> </ul> <p>scp -i ~/Downloads/hwx-practise-exam.pem ubuntu@ec2-54-187-249-103.us-west-2.compute.amazonaws.com:/home/horton/solutions/a.txt ~/Downloads/datasets/sol/</p> <p>*** Mains ****</p> <p>Passport ID proof. Original and copy. 1600x900 monitor 15minutes before we can start the exam. so for 6-8 slot. 5:45 exam start. 5:30 everything up and running. Mobile DND on, 4G on.  5:00 home.  4.45 leave office.</p>"},{"location":"draft/JSPandServlets/","title":"JSPs and Servlets Notes","text":""},{"location":"draft/JSPandServlets/#setup","title":"Setup:","text":"<ul> <li>We use Tomcat as server. Download and browse it to new server.</li> <li>Runs at 8080 as localhost.</li> <li>makes a project in eclipse which are files and can be configured.</li> </ul>"},{"location":"draft/JSPandServlets/#simple-servlet","title":"Simple Servlet:","text":"<ul> <li>Create new web &gt; dynamic web project.</li> <li>keep everything default.</li> <li>WebContent has WEB-INF having web.xml file.</li> <li>It has configuration about default page and web app.</li> <li>javax.servlet.http.HttpServlet - super class for servlet.</li> <li></li> <li>to print from servet we use PrintWriter = response.getWriter();</li> </ul>"},{"location":"draft/JSPandServlets/#understanding-servlet","title":"Understanding Servlet:","text":"<ul> <li>Tomcat runs servlet by path mentioned in annotation.</li> <li>it does not run by javaClass.</li> <li>it run by servlet path.</li> <li> <p>it is defined by urlPatterns = {\"path\"}</p> </li> <li> <p>Say Tomcat has many web apps. </p> </li> <li>it creates request n response objects.</li> <li>from url it runs a particular servlet depending upon annotation or web.xml.</li> <li>then 2 object req n res are passed to SimpleServlet.</li> <li>it process and sends data to response object.</li> <li>goes through PrintWriter.</li> <li>then the page is rendered.</li> <li>req n res are two params in doGet() method.</li> </ul>"},{"location":"draft/JSPandServlets/#simple-xml-configuration","title":"Simple XML configuration:","text":"<ul> <li>in wed.xml we can define path.</li> <li>deployment descriptor. Instead of annotation we can write: <pre><code>    &lt;servlet&gt;   &lt;servlet-name&gt;xmlServlet&lt;/servlet-name&gt;\n&lt;servlet-class&gt;org.vby.XmlServlet&lt;/servlet-class&gt;\n&lt;/servlet&gt;\n&lt;servlet-mapping&gt;\n&lt;servlet-name&gt;xmlServlet&lt;/servlet-name&gt;\n&lt;url-pattern&gt;/xmlServletPath&lt;/url-pattern&gt;\n&lt;/servlet-mapping&gt;\n</code></pre></li> </ul> <p>Why web.xml? - annotation started from java v5. - dont have to compile or change the code.</p>"},{"location":"draft/JSPandServlets/#post-method-and-passing-parameters","title":"POST method and passing parameters.","text":"<ul> <li>response is the object we need to work on.</li> </ul>"},{"location":"draft/JSPandServlets/#http-is-stateless","title":"HTTP is stateless:","text":"<ul> <li>we can set session attributes and get them when needed.</li> <li>we can get session by:     <code>HttpSession session = request.getSession();</code></li> <li>then we can use session.get/set for values and variables.</li> </ul>"},{"location":"draft/JSPandServlets/#making-object-persistent-across-browsers","title":"Making object persistent across browsers.","text":"<ul> <li>sessions can save data but what if we want to save object.</li> <li>so that value needs to be saved across application.</li> <li> <p>shared across servlets and users/browsers.</p> </li> <li> <p>we can use context object for the same. </p> </li> <li><code>ServletContext context = request.getServletContext();</code></li> </ul> <p>END</p>"},{"location":"draft/Java%20Notes/","title":"Java Notes","text":""},{"location":"draft/Java%20Notes/#up-and-runnin-with-java","title":"Up and runnin with Java","text":"<p>To mainain data integrity, members are private and are accessed using methods.</p>"},{"location":"draft/Java%20Notes/#arraylist","title":"ArrayList:","text":"<pre><code>ArrayList&lt;class,..&gt; myVar = new ArrayList&lt;class,..&gt;();\n</code></pre> <p>JAVA: - j2ee and spring adds on - 8 lpa in india - 30 lpa in au/uk/us</p> <p>Main skills: - spring - web services  -SOAP, REST - hibernate - Multi threading, - da/ds - jms, junit, spring, hibernate, struts, j2ee, jpa, rest, soap</p>"},{"location":"draft/Java%20Notes/#lynda-oops","title":"Lynda - OOPS:","text":"<ul> <li>the method of making complex applications</li> <li>UML to visually represent the OOPs logic.</li> <li>put ideas into practice.</li> <li>analysis, design, programming &gt;&gt; desig, build and code/</li> <li>first this and then code.</li> <li>insted of waterfall we use agile methology.</li> <li>because the requirements change.</li> <li>an obejct has a data and logic which communicate among themselves.</li> <li>for object we need a class.</li> <li>each instance of object is its unique existance.</li> <li>pre written classes are into frameworks and libraraies.</li> <li>we have standard library.</li> <li>polymorphism, abstraction, inheritance and encapsulation.</li> </ul>"},{"location":"draft/Java%20Notes/#abstraction","title":"Abstraction","text":"<ul> <li>it is like showing the same outer appearance.</li> </ul>"},{"location":"draft/Java%20Notes/#encapsulation","title":"Encapsulation","text":"<ul> <li>Hide as much as possible.</li> <li>show only whats necessary.</li> <li>helps in dependencies and reusability.</li> <li></li> </ul>"},{"location":"draft/Java%20Notes/#inheritance","title":"Inheritance","text":"<ul> <li>use members of pre defined class.</li> <li>one can have only one parent and not multipe parent class.</li> <li>keyword: extends.</li> <li>super.doSomething() - thats how we call a method from the parent calss.</li> </ul>"},{"location":"draft/Java%20Notes/#polymorphism","title":"Polymorphism","text":"<ul> <li>differen behaviour for same thing.</li> </ul>"},{"location":"draft/Java%20Notes/#understanding-the-object-oriented-analysis-and-design-processes-","title":"Understanding the object-oriented analysis and design processes-","text":"<ol> <li>gather your requirements. </li> <li>describe the application.</li> <li>identify the most important objects.</li> <li>describe the interaction between those objects.</li> <li> <p>create a class diagram. </p> </li> <li> <p>the above should be done on paper and not tool.</p> </li> <li>it does not need to be done once but instead it is done over n over weeks n months.</li> </ol>"},{"location":"draft/Java%20Notes/#uml","title":"UML","text":"<ul> <li>these are used to represent the object, a class.</li> <li>its interactions with other objects.</li> <li>it should not be much emphsised but should be created to have big picture.</li> </ul>"},{"location":"draft/Java%20Notes/#domain-modelling-modelling-the-app","title":"Domain Modelling (Modelling the App)","text":"<ul> <li>In this we identify the classes/</li> <li>then we make relationships between them, the interactions.</li> <li>this is not database modelling. so no primarty, foreign keys or 1-1, 1-m relationships here.</li> <li>this is just for reference.</li> <li>determine the responsibilities of each object.</li> </ul>"},{"location":"draft/Java%20Notes/#uml-diagrmas","title":"UML diagrmas","text":"<ul> <li>they have attiburtes and functionality</li> <li>with data type, + - for public, private.</li> <li>Static, shared or classs-level variable is one that has same state across all instances of objects.</li> </ul>"},{"location":"draft/Java%20Notes/#abstract-class","title":"Abstract Class","text":"<ul> <li>this is just for sake of being inherited and is nvr instantiated.</li> <li>like bank account, inhertited by, SA, CA, LA.</li> <li>this can be made abstract.</li> <li>abstract class BankAccount { ... }</li> </ul>"},{"location":"draft/Java%20Notes/#interface","title":"Interface","text":"<ul> <li>this has just definitions and no functionality.</li> </ul> <pre><code>interface Printable {\n// message signature\nvoid print();\nvoid printToPdf (String fileName);\n}\n</code></pre> <ul> <li>these are like contract. fixed methods and names.</li> <li>not to miss something.</li> </ul> <p>to use: <pre><code>class MyClass implements Printable {...}\n// we can even as if an object is part of an interface.\nif ( myObj instanceOf Printable ) {\nmyObj.print();\n}\n</code></pre></p>"},{"location":"draft/Java%20Notes/#aggregation-and-composition","title":"Aggregation and Composition","text":"<ul> <li>this show uses is a part of etc.</li> </ul>"},{"location":"draft/Java%20Notes/#design-pattern-singelton","title":"Design Pattern - Singelton","text":"<ul> <li>it is class which has private constuctor</li> <li>so that this class cannot be instantiated from outside.</li> <li>we declare object as private static in class itself. __me</li> <li>we make public static getInstance() which instantiates and returns the object.</li> </ul>"},{"location":"draft/Java%20Notes/#design-pattern-memento","title":"Design Pattern - Memento","text":"<ul> <li>this is used when we want to undo a state of a object.</li> <li>there is a care taker class which asks the object to save the state.</li> <li>then the object returns its state to care taker.</li> <li>care taker stores the state and keeps it with it</li> <li>it can be used when we want to undo</li> <li>without breaking the encapsulation, going into the object and modifying it's state.</li> </ul>"},{"location":"draft/Java%20Notes/#desing-principles","title":"Desing Principles","text":"<ul> <li>there are design principles which make the code robust.</li> <li>we break the cide into peices to make it more stable and maintainable.</li> <li>we choose object that all have a particular responsibily and not one has all work to do.</li> </ul> <p>END</p>"},{"location":"draft/Lynda%20-%20Negotiating%20Your%20Job%20Offer/","title":"Negotiating Your Job Offer - Lynda","text":"<p>New job opportunity: - key responsibilities - growth opportunities - location and travel - Reputation of reporting manager</p> <p>Market Value: - Industry - Job Title - Location</p> <p>Must have in 2-3 years - learn, build brand - java + reporting + no SQL</p> <p>What are walkaways? - travel, - location? - job title?</p> <p>What are my trade-offs? - what can I give up in return.</p> <p>Receiving an offer: - Are you ready for TnC? - Say positively that I am excited. - When is it comfortable to review and discuss for offer? - Tell them that I am being interviewed with another company and have an offer of 7.5 lakhs - response time should not be more than a week.</p> <p>All what you can get: - Perks, - incentives - equity - facilities - Fringe benefits - insurance, retirement benefits</p> <p>How to Negotiate:</p> <p>What to avoid? - Be prepared, be intelligent - focus only on salary - Don;t make it personal, make it professional conversation - Negotiate on facts and figures.</p> <p>Salary negotiation myth: - last salary is low then it does not matter, market value matters - company can give depending on budget, not last salary.</p> <p>Negotiations: - Sign on bonus. - work from home. - Relocation assistance. - Be Prepared.</p> <p>Reimbushment for relocation? (how much can we claim?)</p> <p>Sign on bonus (CRR Time, I am expecting bonus)</p> <p>Down the line, when can I expect 1st increment?</p> <p>What position am I being hiered? Next role at what experince.</p> <p>Are there work from home options? if yes then how many?</p> <p>4375510481178017</p> <p>END</p>"},{"location":"draft/flume/","title":"example.conf: A single-node Flume configuration","text":""},{"location":"draft/flume/#name-the-components-on-this-agent","title":"Name the components on this agent","text":"<p>a1.sources = r1 a1.sinks = k1 a1.channels = c1</p>"},{"location":"draft/flume/#describeconfigure-the-source","title":"Describe/configure the source","text":"<p>a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444</p>"},{"location":"draft/flume/#describe-the-sink","title":"Describe the sink","text":"<p>a1.sinks.k1.type = logger</p>"},{"location":"draft/flume/#use-a-channel-which-buffers-events-in-memory","title":"Use a channel which buffers events in memory","text":"<p>a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100</p>"},{"location":"draft/flume/#bind-the-source-and-sink-to-the-channel","title":"Bind the source and sink to the channel","text":"<p>a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1</p>"},{"location":"draft/hive/","title":"HIVE","text":""},{"location":"draft/hive/#01hive_create_databasestxt","title":"01hive_create_databases.txt","text":"<pre><code>CREATE DATABASE IF NOT EXISTS cards;\nCREATE DATABASE IF NOT EXISTS retail_ods;\nCREATE DATABASE retail_edw;\nCREATE DATABASE retail_stage;\nCREATE DATABASE IF NOT EXISTS pig_demo;\n</code></pre>"},{"location":"draft/hive/#02hive_ddltxt","title":"02hive_ddl.txt","text":"<p>Create sample database</p> <pre><code>CREATE DATABASE IF NOT EXISTS cards;\nCREATE TABLE deck_of_cards (\nCOLOR string,\nSUIT string,\nPIP string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\n</code></pre> <p>Steps: 1. Download deckofcards.txt from github repository www.github.com/dgadiraju/data 2. <code>mkdir -p ~/demo/data/cards</code> 3. copy file deckofcards.txt to ~/demo/data/cards 4. Load data from local file system into hive table (append to existing table) 5. <code>LOAD DATA LOCAL INPATH '/root/demo/data/cards/deckofcards.txt' INTO TABLE deck_of_cards;</code></p> <p>Load data from HDFS into hive table (append data to existing table), file user /user/root/cards will be deleted LOAD DATA INPATH '/user/root/cards/deckofcards.txt' INTO TABLE deck_of_cards;</p> <p>Loads data from local file system (overwrite existing data)</p> <p><code>LOAD DATA LOCAL INPATH '/root/demo/data/cards/deckofcards.txt' OVERWRITE INTO TABLE deck_of_cards;</code></p> <pre><code>CREATE EXTERNAL TABLE deck_of_cards_external (\nCOLOR string,\nSUIT string,\nPIP string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE\nLOCATION '/apps/hive/warehouse/cards.db/deck_of_cards';\ncreate external table deckOfCardsExternal(\ncolor string,\nsuit string,\npip string\n)\nrow format delimited fields terminated by '|'\nstored as textfile\nlocation '/user/hdpcd/cards';\n</code></pre>"},{"location":"draft/hive/#create-ods-and-edw-database-for-retail_dbmysql","title":"Create ods and edw database for retail_db@mysql","text":"<pre><code>CREATE DATABASE IF NOT EXISTS retail_ods;\nCREATE DATABASE retail_edw;\nCREATE DATABASE retail_stage;\n</code></pre>"},{"location":"draft/hive/#hdpcd-define-a-hive-managed-table","title":"HDPCD - Define a Hive-managed table","text":"<pre><code>USE retail_stage;\nCREATE TABLE orders_demo (\norder_id int,\norder_date string,\norder_customer_id int,\norder_status string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ','\nSTORED AS TEXTFILE;\n</code></pre>"},{"location":"draft/hive/#hdpcd-define-a-hive-external-table","title":"HDPCD - Define a Hive external table","text":"<ul> <li>Download the data from github, unzip and copy deckofcards.txt </li> <li>Run on terminal on your PC/Mac to copy data to sandbox</li> <li>scp ./Documents/Training/GoogleDrive/Training/data/cards/deckofcards.txt root@sandbox.hortonworks.com:~</li> <li>On sandbox</li> </ul> <pre><code>hadoop fs -mkdir /user/root/cards\nhadoop fs -put deckofcards.txt /user/root/cards\nhadoop fs -ls /user/root/cards\n</code></pre> <p>launch hive by running \"hive\" <pre><code>CREATE DATABASE IF NOT EXISTS cards;\nUSE cards;\nCREATE EXTERNAL TABLE deck_of_cards_external\n(color string,\nsuit string,\npip string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nLOCATION '/user/root/cards';\n</code></pre></p>"},{"location":"draft/hive/#create-ods-tables-mostly-they-will-follow-same-structure-except-additional-audit-columns","title":"Create ods tables (mostly they will follow same structure, except additional audit columns)","text":"<pre><code>use retail_ods;\nCREATE TABLE categories (\ncategory_id int,\ncategory_department_id int,\ncategory_name string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\nCREATE TABLE customers (\ncustomer_id       int,\ncustomer_fname    string,\ncustomer_lname    string,\ncustomer_email    string,\ncustomer_password string,\ncustomer_street   string,\ncustomer_city     string,\ncustomer_state    string,\ncustomer_zipcode  string )\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\nCREATE TABLE departments (\ndepartment_id int,\ndepartment_name string\n)\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\n-- Partitioned \nCREATE TABLE orders (\norder_id int,\norder_date string,\norder_customer_id int,\norder_status string\n)\nPARTITIONED BY (order_month string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\nCREATE TABLE order_items (\norder_item_id int,\norder_item_order_id int,\norder_item_order_date string,\norder_item_product_id int,\norder_item_quantity smallint,\norder_item_subtotal float,\norder_item_product_price float\n)\nPARTITIONED BY (order_month string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\n-- Bucket\nCREATE TABLE orders_bucket (\norder_id int,\norder_date string,\norder_customer_id int,\norder_status string\n)\nCLUSTERED BY (order_id) INTO 16 BUCKETS\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\nCREATE TABLE order_items_bucket (\norder_item_id int,\norder_item_order_id int,\norder_item_order_date string,\norder_item_product_id int,\norder_item_quantity smallint,\norder_item_subtotal float,\norder_item_product_price float\n)\nCLUSTERED BY (order_item_order_id) INTO 16 BUCKETS\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\ncreate table orders_partitioned_bucketed (\norder_id int,\norder_date string,\norder_customer_id int,\norder_status string\n)\npartitioned by  (order_month string)\nclustered by (order_id) into 16 buckets\nrow format delimited fields terminated by '|'\nstored as textfile;\nCREATE TABLE products (\nproduct_id int, product_category_id int,\nproduct_name string,\nproduct_description string,\nproduct_price float,\nproduct_image string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\n-- Create edw tables (following dimension model)\nuse retail_edw;\nCREATE TABLE products_dimension (\nproduct_id int,\nproduct_name string,\nproduct_description string,\nproduct_price float,\nproduct_category_name string,\nproduct_department_name string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\nCREATE TABLE order_fact (\norder_item_order_id int,\norder_item_order_date string,\norder_item_product_id int,\norder_item_quantity smallint,\norder_item_subtotal float,\norder_item_product_price float\n)\nPARTITIONED BY (product_category_department string)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\n-- Create external tables for retail_stage\nuse retail_stage;\nCREATE EXTERNAL TABLE categories\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\nLOCATION 'hdfs:///apps/hive/warehouse/retail_stage.db/categories'\nTBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/root/retail_stage/sqoop_import_categories.avsc');\nCREATE EXTERNAL TABLE customers\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\nLOCATION 'hdfs:///apps/hive/warehouse/retail_stage.db/customers'\nTBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/root/retail_stage/customers.avsc');\nCREATE EXTERNAL TABLE departments\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\nLOCATION 'hdfs:///apps/hive/warehouse/retail_stage.db/departments'\nTBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/root/retail_stage/departments.avsc');\nCREATE EXTERNAL TABLE orders\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\nLOCATION 'hdfs:///apps/hive/warehouse/retail_stage.db/orders'\nTBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/root/retail_stage/orders.avsc');\nCREATE EXTERNAL TABLE order_items\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\nLOCATION 'hdfs:///apps/hive/warehouse/retail_stage.db/order_items'\nTBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/root/retail_stage/order_items.avsc');\nCREATE EXTERNAL TABLE products\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'\nSTORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'\nLOCATION 'hdfs:///apps/hive/warehouse/retail_stage.db/products'\nTBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/root/retail_stage/products.avsc');\nCREATE TABLE departments_delta (\ndepartment_id int,\ndepartment_name string,\nupdate_date string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\n</code></pre>"},{"location":"draft/hive/#03hive_loadtxt","title":"03hive_load.txt","text":"<p>Copy data from MySQL to local file system</p>"},{"location":"draft/hive/#enable-file_priv-to-retail_dba","title":"Enable file_priv to retail_dba","text":"<p>mysql -u root -p #if password enabled, else \"mysql -u root\" update mysql.user set file_priv = 'Y' where user = 'retail_dba'; commit; exit;</p>"},{"location":"draft/hive/#on-os-prompt-run","title":"On OS prompt, run","text":"<p>service mysqld restart mysql -u retail_dba -p #prompts for password and launches mysql CLI use retail_db;</p>"},{"location":"draft/hive/#make-sure-you-understand-table-structure-delimiter-partition-etc-run-mysql-export-command","title":"Make sure you understand table structure, delimiter, partition etc, run mysql export command","text":"<pre><code>select * from categories into outfile '/tmp/categories01.psv' fields terminated by '|' lines terminated by '\\n';\nselect * from customers into outfile '/tmp/customers.psv' fields terminated by '|' lines terminated by '\\n';\nselect * from departments into outfile '/tmp/departments.psv' fields terminated by '|' lines terminated by '\\n';\nselect * from products into outfile '/tmp/products.psv' fields terminated by '|' lines terminated by '\\n';\n</code></pre> <p>We cannot use orders and order_items directly as tables in hive database retail_ods are partitioned</p>"},{"location":"draft/hive/#load-data-from-local-file-system-to-hive-table","title":"Load data from local file system to hive table","text":"<pre><code>load data local inpath '/tmp/categories01.psv' overwrite into table categories;\nload data local inpath '/tmp/customers.psv' overwrite into table customers;\nload data local inpath '/tmp/departments.psv' overwrite into table departments;\nload data local inpath '/tmp/products.psv' overwrite into table products;\n</code></pre> <p>You can remove overwrite while appending data to underlying hive table</p>"},{"location":"draft/hive/#load-data-from-hdfs-to-hive-table","title":"Load data from HDFS to hive table","text":"<ul> <li>Prepare HDFS stage directory</li> <li>On command prompt (if you login as root) <pre><code>hadoop fs -mkdir /user/root/departments\nhadoop fs -put /tmp/departments.psv /user/root/departments\nhadoop fs -ls /user/root/departments\n</code></pre></li> </ul>"},{"location":"draft/hive/#launch-hive","title":"Launch hive","text":"<pre><code>hive\nuse retail_ods;\nload data inpath '/user/root/departments/*' overwrite into table departments;\nhadoop fs -ls /user/root/departments\n</code></pre> <p>You will not find files</p>"},{"location":"draft/hive/#04hive_inserttxt","title":"04hive_insert.txt","text":"<p>Prepare orders on mysql database</p> <p>On mysql <pre><code>select * from orders into outfile '/tmp/orders.psv' fields terminated by '|' lines terminated by '\\n';\n</code></pre></p> <p>Create orders_stage under hive database retail_stage <pre><code>hive\nuse retail_stage;\nCREATE TABLE orders_stage (\norder_id int,\norder_date string,\norder_customer_id int,\norder_status string\n)\nROW FORMAT DELIMITED FIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE;\nload data local inpath '/tmp/orders.psv' overwrite into table orders_stage;\n</code></pre></p>"},{"location":"draft/hive/#important","title":"Important","text":"<pre><code>insert overwrite table retail_ods.orders partition (order_month)\nselect order_id, order_date, order_customer_id, order_status,\nsubstr(order_date, 1, 7) order_month from retail_stage.orders_stage;\n</code></pre> <p>Now we have 2 tables retail_stage.order_items and retail_stage.orders</p> <p>We need to join these 2 and populate retail_ods.order_items table which have additional columns</p> <p>order_item_order_date and order_month</p> <p>Also table is partitioned by order_month</p> <pre><code>insert overwrite table order_items partition (order_month)\nselect oi.order_item_id, oi.order_item_order_id, o.order_date,\noi.order_item_product_id, oi.order_item_quantity, oi.order_item_subtotal,\noi.order_item_product_price, substr(o.order_date, 1, 7)\norder_month from retail_stage.order_items oi join retail_stage.orders_stage o\non oi.order_item_order_id = o.order_id;\n</code></pre>"},{"location":"draft/hive/#05hive_sqltxt","title":"05hive_sql.txt","text":""},{"location":"draft/hive/#using-all-hiveqlsql-clauses","title":"Using all HiveQL/SQL clauses","text":"<pre><code>-- Get number of orders by order_status for a given date '2013-12-14'\nSELECT order_status, count(1) FROM orders\nWHERE order_date = '2013-12-14 00:00:00.0'\nGROUP BY order_status\nORDER BY order_status;\n-- Get number of completed orders for each date before '2013-12-14 00:00:00.0'\nSELECT order_date, count(1) FROM orders\nWHERE order_date &lt;= '2013-12-14 00:00:00.0' AND order_status = 'COMPLETE'\nGROUP BY order_date\nORDER BY order_date;\n-- Get number of pending, review and onhold order for each date for the month of 2013 December\nSELECT order_date, count(1) FROM orders\nWHERE order_date LIKE '2013-12%' AND order_status IN ('PENDING', 'PENDING_PAYMENT', 'PAYMENT_REVIEW', 'ON_HOLD')\n-- order_date LIKE '2013-12%' AND (order_status = 'PENDING' or order_status = 'PENDING_PAYMENT'....)\nGROUP BY order_date\nORDER BY order_date;\n--Incorrect query\nSELECT order_date, count(1) FROM orders\nWHERE order_date BETWEEN '2013-12-01 00:00:00.0' AND '2013-12-31 00:00:00.0'\nAND order_status LIKE 'PENDING%' OR order_status IN ('PAYMENT_REVIEW', 'ON_HOLD')\nGROUP BY order_date\nORDER BY order_date;\n--Correct alternative query\nSELECT order_date, count(1) FROM orders\nWHERE order_date BETWEEN '2013-12-01 00:00:00.0' AND '2013-12-31 00:00:00.0'\nAND (order_status LIKE 'PENDING%' OR order_status IN ('PAYMENT_REVIEW', 'ON_HOLD'))\nGROUP BY order_date\nORDER BY order_date;\n</code></pre>"},{"location":"draft/hive/#06hive_transactionstxt","title":"06hive_transactions.txt","text":"<pre><code>-- Creating database (if not exists)\nCREATE DATABASE IF NOT EXISTS hive_demo;\n-- set hive.txn.manager by default it uses DummyTxnManager\nset hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n</code></pre> <p>Also make sure these properties are set - hive.support.concurrency \u2013 true - hive.enforce.bucketing \u2013 true (Not required as of Hive 2.0) - hive.exec.dynamic.partition.mode \u2013 nonstrict - hive.compactor.initiator.on \u2013 true (for exactly one instance of the Thrift metastore service) - hive.compactor.worker.threads \u2013 a positive number on at least one instance of the Thrift metastore service</p>"},{"location":"draft/hive/#creating-table","title":"Creating table","text":"<ul> <li>Make sure table is bucketed, file format is orc and (this is compultion)</li> <li>transactional is set to true under tblproperties <pre><code>create table hive_transactions (i int, j string)\nclustered by (i) into 4 buckets\nstored as orc\ntblproperties ('transactional'='true');\n-- Inserting data\ninsert into table hive_transactions values (1, 'itversity');\ninsert into table hive_transactions values (2, 'itversity');\n--or insert into table hive_transactions (i, j) values (1, 'itversity');\n-- Updating data\nupdate hive_transactions set j = 'IT Versity' where i = 2;\n-- Deleting data\ndelete hive_transactions where i = 1;\n</code></pre></li> </ul> <p>END HIVE</p>"},{"location":"draft/mongo%20University%20m101j%20exam%20solutions/","title":"M101 | Exam Solutions | Mongo University","text":"<pre><code>db.profile.find({\n\"ns\" : \"school2.students.indexes\",\n}).sort({\"millis\":-1});\ndb.posts.aggregate([\n{\n\"$unwind\" : \"$comments\"\n},\n{\n\"$group\" : {\n\"$_id\" : \"$comments.author\",\ncount : { $sum : 1 }\n}\n},\n{\n\"$project\" : {\n_id:0,\n\"author\" : \"$_id\",\n\"Count\" : \"$count\"\n}\n},\n{\n\"$sort\" : {\n\"$count\" : -1\n}\n}\n]).pretty();\ndb.posts.aggregate([\n{\n\"$unwind\" : \"$comments\"\n},\n{\n\"$project\" : {\n\"commentAuthor\" : \"$comments.author\"\n}\n}\n]).pretty();\ndb.posts.aggregate([\n{\n\"$unwind\" : \"$comments\"\n},\n{\n\"$project\" : {\n\"commentAuthor\" : \"$comments.author\"\n}\n},\n{\n\"$group\" : {\n\"_id\" : \"$commentAuthor\",\n\"Total\" : { \"$sum\" : 1 }\n}\n},\n{\n\"$sort\" : {\"Total\" : 1 }\n}\n]).pretty();\ndb.zips.aggregate([\n{\n\"$match\" : {\n\"state\" : {\"$in\" : ['CT' , 'NJ'] },\n\"pop\"   : {\"$gt\" : 25000}\n}\n},\n{\n\"$project\" : {\n_id : 0,\npop : 1,\n\"state\" : 1\n}\n},\n{\n$group : {\n_id : null,\naverage : {\"$avg\" : \"$pop\" }\n}\n}\n]).pretty();\n// above is incorrect.\ndb.zips.aggregate([ {\n$match: {\nstate: {$in: ['CA', 'NY']}\n}\n}, {$group: { _id: {state: \"$state\", city: \"$city\"}, pop: {$sum: \"$pop\"} } }, {$match: {\npop: {$gt: 25000}\n} }, {$group: { _id: null, pop: {$avg: \"$pop\"} } } ]);\ndb.grades.aggregate([\n{\n$unwind : \"$scores\"\n},\n{\n$match : { \"scores.type\" : { $in : [\"homework\", \"exam\"] } }\n},\n{\n$group : {\n_id : { student_id : \"$student_id\", class_id : \"$class_id\" },\navg : {$avg:\"$scores.score\"}\n}\n},\n{\n$group : {\n_id : \"$_id.class_id\",\nclass_avg : {$avg : \"$avg\" }\n}\n},\n{$sort : {\nclass_avg : 1\n}\n}\n]).pretty();\ndb.zips.aggregate([\n{$project: {\nfirst_char: {$substr : [\"$city\",0,1]},\npop:1\n}    },\n{\n$match : {\nfirst_char : { $regex : \"^[0-9]{1,1}$\" }\n}\n},\n{\n$group : {\n_id : null,\ntotal : {$sum : \"$pop\"}\n}\n}\n])\n</code></pre>"},{"location":"draft/mongo%20University%20m101j%20exam%20solutions/#final-exam","title":"FINAL EXAM","text":"<p><code>mongorestore -c messages -d enron &lt;</code></p> <pre><code>db.corpus.find(\n{\n\"headers.From\" : \"andrew.fastow@enron.com\",\n\"headers.To\" : {$in : [\"jeff.skilling@enron.com\"] }\n}\n).count();\ndb.corpus.aggregate([\n{\n$unwind : \"$headers.To\"\n},\n{\n$group : {\n_id : {from : \"$headers.From\", myId: _id},\nto : {$addToSet : \"$headers.To\"}\n}\n},\n{\n$unwind : \"$to\"\n},\n{\n$group : {\n_id : {\nfrom : \"$_id.from\",\nto : \"$to\"\n},\ntotal : {\n$sum : 1\n}\n}\n},\n{\n$sort : {\ntotal : -1\n}\n}\n]).pretty();\ndb.corpus.aggregate([\n{\n$unwind : \"$headers.To\"\n},\n{\n$project : {\n_id : 1,\nfrom : \"$headers.From\",\nto: \"$headers.To\"\n}\n},\n{\n$group : {\n_id : {\nfrom : \"$from\",\nid : \"$_id\"\n},\nto : {$addToSet : \"$to\"}\n}\n},\n{\n$project : {\nfrom : \"$_id.from\",\nto : \"$to\",\n_id : \"$_id.id\"\n}\n},\n{\n$unwind : \"$to\"\n},\n{\n$group : {\n_id : {\nfrom : \"$from\",\nto : \"$to\"\n},\ntotal : {$sum : 1}\n}\n},\n{\n$sort : {\ntotal : -1\n}\n}\n]).pretty();\ndb.messages.update(\n{\n\"headers.Message-ID\" : \"&lt;8147308.1075851042335.JavaMail.evans@thyme&gt;\"\n},\n{\n$addToSet : { \"headers.To\" : \"mrpotatohead@10gen.com\" }\n}\n);\ndb.messages.findOne({\"headers.Message-ID\" : \"&lt;8147308.1075851042335.JavaMail.evans@thyme&gt;\"});\n// Calvin Harris - Drinking From the Bottle (Feat. Tinie Tempah).mp3\ndb.posts.updateOne(\n{\n\"permalink\" : \"mxwnnnqaflufnqwlekfd\"\n},\n{\n$inc : { \"comments.1.num_likes\" : 1 }\n}\n);\ndb.posts.findOne(\n{\n\"permalink\" : \"mxwnnnqaflufnqwlekfd\"\n}).pretty();\n</code></pre> <p>END</p>"},{"location":"draft/mongodb-notes/","title":"Mongo DB - Notes","text":""},{"location":"draft/mongodb-notes/#setup","title":"Setup","text":"<p><code>~/code/mogodb</code> - it is base for project setup and is imported to eclipse. - added new softwares are /usr/local/mogodb, apache-maven. - spark and front end framework, FreeMarker, is added as jar to project.</p> <ul> <li>Non relational JSON db</li> <li>Does not save in table, instead stored as JSON document.</li> <li>It does not support joins and sql.</li> <li>It does not support transactions.</li> <li> <p>It supports indexes and secondary indexes.</p> </li> <li> <p>MongoD is the process which runs the mongoDB.</p> </li> <li>Shell is js shell which connects via TCP to mongoD.</li> </ul>"},{"location":"draft/mongodb-notes/#java-driver-and-web-framework","title":"Java Driver and Web Framework:","text":"<ul> <li> <p>SparkJava and FreeMarker are framework which well use to interact with mongoDB.</p> </li> <li> <p>SparkJava is micro web fw to setup routes.</p> </li> <li> <p>FreeMarker is used for HTML views.</p> </li> <li> <p>runs under JVM</p> </li> <li> <p>all this talks to mongoDB via mongoDB java driver.</p> </li> <li> <p>@todo: Quick Introduction to the Mongo Shell - watch at home.</p> </li> <li> <p>mongod is server daemon and mongo is shell that connects to it .</p> </li> <li>data is saved in /data/db by default</li> </ul>"},{"location":"draft/mongodb-notes/#following-notes-are-from-2nd-revision","title":"Following notes are from 2nd revision","text":""},{"location":"draft/mongodb-notes/#using-findone-function","title":"Using findOne function:","text":"<ul> <li>It takes first argument as document in which we have key and value.</li> <li> <p>this acts as key = column, value = value like we give in where clause.</p> </li> <li> <p>the second argument to findOne is the document having keys as columns we want to see and value as true to see and false to skip.</p> </li> <li> <p>to use gr and lt we use a sub document in argument.</p> </li> <li> <p>eg. db.score.find ( { score : { $gt : 35 } } )</p> </li> <li> <p>db.score.find ( { score : { $gt : 35 , $lte : 60} , type : \"English\" } )</p> </li> <li> <p>where score &gt; 35 and score &lt;= 60 and type = \"English\";</p> </li> </ul>"},{"location":"draft/mongodb-notes/#introduction-to-find","title":"Introduction to find()","text":"<ul> <li>the result is returned in form of batches. say 20.</li> <li>to page through type it.</li> <li>cursor on server is open. it is closed in say 10 mins on the server.</li> <li>.pretty() makes result more readable.</li> </ul>"},{"location":"draft/mongodb-notes/#querying-using-field-selection","title":"Querying using field selection","text":"<ul> <li>the 1st arg is like where clause. filter the result on matching conditions.</li> <li> <p>it accepts doc and that is matched for results.</p> </li> <li> <p>@Note: We can give key without quotes but the value has to be in quotes when writing a document.</p> </li> <li> <p>the 2nd arg takes what we want to see in results. by default the _id column is selected. it is analogous to select clause in rdbms.</p> </li> <li> <p>e.g.</p> </li> <li> <p>db.scores.find({student:19,type:\"essay\"} , {score:true,_id:false})</p> </li> </ul>"},{"location":"draft/mongodb-notes/#querying-using-gt-and-lt","title":"Querying using $gt and $lt","text":"<ul> <li>e.g.</li> <li> <p>db.scores.find({ score: { $gt : 95  }  })</p> </li> <li> <p>db.scores.find({ score: { $gt : 95 , $lte: 96 }  })</p> </li> <li> <p>db.scores.find({ score: { $gt : 95 , $lte: 96 } , type:\"essay\"  })</p> </li> </ul>"},{"location":"draft/mongodb-notes/#inequalities-on-string","title":"Inequalities on String","text":"<ul> <li>We can use lt, gt, lte, gte but this gives result based on UTF sorting of alphabets.</li> <li> <p>eg. { name : { $lte : \"D\" } } # it may produce absurd results as well.</p> </li> <li> <p>We have $type in which we match the datatype of a field. It accepts a number based on BSON type.</p> </li> <li> <p>e.g: name : { $type : 2 } - this gives names having string value</p> </li> <li> <p>We have $regex which accepts regular expressions.</p> </li> <li> <p>We have $exists, it accepts a key name. It returns only that document which has this key in it.</p> </li> <li>e.g: db.emp.find ( { profession : { $exists : true } } )</li> <li> <p>Nested content is not matched, only top level array element is matched.</p> </li> <li> <p>$all is is like a sub set of elements. all should be there but in any order</p> </li> <li> <p>$in is like any of it should match.</p> </li> <li> <p>We can use gt and lt in string Comparisons.     e.g.     &gt; db.people.find()     { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 }     { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 }     { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" }     { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" }     { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Vishakha\" }     { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" }     { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" }     { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" }     { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" }     { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" }     &gt; db.people.find({name:{\\(lt:\"D\"}})     { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" }     { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" }     { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" }     &gt; db.people.find({name:{\\)gt:\"D\",$lt:\"M\"}})     { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" }     { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" }     { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" }</p> </li> <li> <p>In mongodb the name field can have numeric value as well. But with above query we will get only the result we got. name:42 for eg will not be retrieved.</p> </li> <li>hence mongodb Comparison operator donot cross the data type boundary.</li> </ul>"},{"location":"draft/mongodb-notes/#using-regex-exists-and-type","title":"Using $regex, $exists and $type","text":"<ul> <li>exists is used to find if a certain filed is present in the document.</li> <li>type is used to find doc having value of particular type say string or number. the type code can be found from bson.org.</li> <li> <p>regex is used to match regular expressions.</p> <p>e.g.</p> <p>db.people.find({age:{\\(exists:false}}) { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" } { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" } { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Vishakha\" } { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" } { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" } { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" } { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" } { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" } { \"_id\" : ObjectId(\"57bfa78fdaf90e8c76d19cd0\"), \"name\" : 42 } db.people.find({age:{\\)exists:true}}) { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 } { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 } db.people.find({name:{$type:2}}) //2 is code for string { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 } { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 } { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" } { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" } { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Vishakha\" } { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" } { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" } { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" } { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" } { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" } db.people.find({name:{ $regex: \"a\"  }}) // all that have a somewhere { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 } { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 } { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" } { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" } { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Vishakha\" } { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" } { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" } db.people.find({name:{ \\(regex: \"e\\)\"  }}) //ends with e db.people.find({name:{ \\(regex: \"a\\)\"  }}) //ends with a { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Vishakha\" }</p> <p>db.people.find({name:{ $regex: \"^A\"  }}) //begins with A { \"_id\" : ObjectId(\"57bfa5aadaf90e8c76d19ccc\"), \"name\" : \"Ankit\" } { \"_id\" : ObjectId(\"57bfa5bcdaf90e8c76d19cce\"), \"name\" : \"Anuj\" } { \"_id\" : ObjectId(\"57bfa5c0daf90e8c76d19ccf\"), \"name\" : \"Arjun\" }</p> </li> </ul>"},{"location":"draft/mongodb-notes/#using-or","title":"Using $or","text":"<ul> <li> <p>$or takes in an array of documents and combines them with or conditions.</p> </li> <li> <p>when the value is array in document and we specify it to match in find, then only the outer array is looked in.</p> </li> <li> <p>no recursion occurs or inner depth arrays are matched.</p> </li> <li> <p>MongoDB has no sql language. Instead it has functions that have arguments passes to them.</p> <p>e.g.</p> <p>db.people.find({ \\(or : [ {name:{\\)regex:\"e\"}} , { age : { $exists:true } } ] }); { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 } { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 } { \"_id\" : ObjectId(\"57bfa5a3daf90e8c76d19ccb\"), \"name\" : \"Jaspreet\" } { \"_id\" : ObjectId(\"57bfa5b1daf90e8c76d19ccd\"), \"name\" : \"Hardeep\" }</p> </li> </ul>"},{"location":"draft/mongodb-notes/#using-and","title":"Using $and","text":"<ul> <li> <p>Same as we use $or.     e.g.     &gt; db.people.find({ \\(and : [ {name:{\\)regex:\"a\"}} , { name : { $gt:\"K\" } } ] });     { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"vaibhav\", \"age\" : 30 }     { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 }     { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"rahul\" }     { \"_id\" : ObjectId(\"57bfa594daf90e8c76d19cc9\"), \"name\" : \"Kashish\" }     { \"_id\" : ObjectId(\"57bfa59bdaf90e8c76d19cca\"), \"name\" : \"Vishakha\" }</p> <p>db.people.find( {name:{$regex:\"a\"}, name : { $gt:\"K\" } } );</p> </li> <li> <p>This query also gives same result as the above one. And this more efficient as well.</p> </li> </ul>"},{"location":"draft/mongodb-notes/#querying-inside-arrays","title":"Querying inside Arrays:","text":"<ul> <li> <p>MongoDB has polymorphic find. It also evaluates for matching array elements.     e.g.     &gt; db.accounts.find().pretty();     { \"_id\" : ObjectId(\"57bff4973df8f8d7306e7918\") }     {            \"_id\" : ObjectId(\"57bff4d93df8f8d7306e7919\"),            \"name\" : \"vaibhav\",            \"favorites\" : [                    \"ice cream\",                    \"beer\"            ]     }     {            \"_id\" : ObjectId(\"57bff5043df8f8d7306e791a\"),            \"name\" : \"Neeraj\",            \"favorites\" : [                    \"beer\",                    \"Spring Roll\"            ]     }</p> <p>db.accounts.find({favorites:\"beer\"}); { \"_id\" : ObjectId(\"57bff4d93df8f8d7306e7919\"), \"name\" : \"vaibhav\", \"favorites\" : [ \"ice cream\", \"beer\" ] } { \"_id\" : ObjectId(\"57bff5043df8f8d7306e791a\"), \"name\" : \"Neeraj\", \"favorites\" : [ \"beer\", \"Spring Roll\" ] } </p> </li> <li> <p>Here nested contents are not matched. Only first level is looked into.</p> </li> </ul>"},{"location":"draft/mongodb-notes/#using-in-and-all","title":"Using $in and $all","text":"<ul> <li> <p>The \\(all operator matches all the elements that are specified with elements present inside the array.     e.g.     db.accounts.find({favorites : {\\)all : [ \"beer\", \"cheeze\" ]} })</p> </li> <li> <p>The $in operator is used to filter results by having value in the values specified in \\(in array.     e.g.     &gt; db.accounts.find({name: {\\)in: [\"Sahiba\",\"Neeraj\"]}});     { \"_id\" : ObjectId(\"57bff5043df8f8d7306e791a\"), \"name\" : \"Neeraj\", \"favorites\" :     [ \"beer\", \"Spring Roll\" ] }     { \"_id\" : ObjectId(\"57c017fc3df8f8d7306e791b\"), \"name\" : \"Sahiba\", \"favorites\" :     [ \"beer\", \"Momo\", \"cheeze\" ] }     &gt;</p> </li> </ul>"},{"location":"draft/mongodb-notes/#queries-with-dot-notation","title":"Queries with dot notation","text":"<ul> <li>to match nested documents, if we specify document in nested way then it is mactched exactly.</li> <li> <p>we cannot match one key value. Not even in reversed order.</p> <p>{     \"_id\" : ObjectId(\"57c06919daf90e8c76d19cd2\"),     \"name\" : \"Rahul\",     \"email\" : {         \"work\" : \"rahul@info.com\",         \"personal\" : \"rgw@live.in\"     } } - to find with email we have to pass exact email doc in find clause. - Even the order of work and personal needs to be same.</p> </li> <li> <p>To query one part of doc, </p> <p>db.users.find({\"email.work\":\"rahul@info.com\"}); { \"_id\" : ObjectId(\"57c06919daf90e8c76d19cd2\"), \"name\" : \"Rahul\", \"email\" : { \"work\" : \"rahul@info.com\", \"personal\" : \"rgw@live.in\" } } </p> </li> <li> <p>if we use . notation then we can match on value.</p> </li> <li> <p>e.g. email : { work: \"abc\", personal: \"xyz\"}</p> </li> <li> <p>then email.work : \"abc\" - this fetches and gives result, while</p> </li> <li> <p>email : { work: \"abc\" } - this will not match as personal is also there in doc and byte by byte match will fail.</p> </li> <li> <p>Suppose a simple e-commerce product catalog called catalog with documents that look like this:     { product : \"Super Duper-o-phonic\",      price : 100000000000,      reviews : [ { user : \"fred\", comment : \"Great!\" , rating : 5 },                  { user : \"tom\" , comment : \"I agree with Fred, somewhat!\" , rating : 4 } ],      ... }</p> </li> <li> <p>Write a query that finds all products that cost more than 10,000 and that have a rating of 5 or better.</p> </li> <li> <p>Ans:     db.catalog.find(        {            \"price\" : {\"\\(gt\" : 10000},            \"reviews.rating\" : {\"\\)gte\" : 5}        }     );</p> </li> </ul>"},{"location":"draft/mongodb-notes/#querying-cursors","title":"Querying, Cursors :","text":"<ul> <li>cur = db.sb.find();</li> <li>cur.next(); - returns next doc</li> <li>cur.hasNext(); - true if there is next doc</li> <li>cur.sort({name:-1}) - sorts by descending name order</li> <li>cur.limit(5) - limits result set to 5, database only returns 5 docs.</li> <li>cur.sort({name:-1}).limit(5) - can be combined this way.</li> <li>cur.sort({name:-1}).limit(5).skip(2) - this sorts, skips 2 and shows 3 results. this sequence is followed by db engine,</li> <li> <p>the sort, skip and limit is sent to db and performed on server, not on cursor in the memory.</p> </li> <li> <p>limit and sort are processed on server side and not in memory on client side.</p> </li> </ul> <p>Q. When can you change the behavior of a cursor, by applying a sort, skip, or limit to it? A. This can be done at any point before the first document is called and before you''ve checked to see if it is empty.</p>"},{"location":"draft/mongodb-notes/#counting-results","title":"Counting Results","text":"<pre><code>db.abc.count();\ndb.abc.count({age : 34}); - arguments accepted are same as find();\n</code></pre>"},{"location":"draft/mongodb-notes/#updates","title":"Updates","text":"<ul> <li>its accepts two arguments, one is analogous to where clause like we pass in find command.</li> <li>The second argument is what we want to replace in the found doc. All the key:value in second arg replaces all the existing key:value in document except the _id field.</li> <li>It basically replaces wholosole document but it is dangerous.</li> <li>it replaces completely.     e.g.     &gt; db.people.update({name:\"Rahul\"},{name:\"Rahul\",age:32});     WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })</li> </ul>"},{"location":"draft/mongodb-notes/#set-unset-inc","title":"Set, Unset, Inc","text":"<ul> <li>We can use update with $set to change only a particular key:value.</li> <li> <p>e.g. db.people.update({name:Vaibhav},{$set : {  key:value, age:30 }})</p> </li> <li> <p>if the field does not exist then it will be added.</p> </li> <li> <p>db.people.update({ name:\"Vaibhav\"}, { $inc : { \"age\" : 1 }}); - this increases age by 1, if it age does not exist then it is crated as age:1.</p> </li> <li> <p>e.g.     &gt; db.people.update({name:\"vaibhav\"},{$set : { \"name\":\"Vaibhav\" } });     WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })</p> </li> </ul>"},{"location":"draft/mongodb-notes/#unset-command","title":"$unset Command","text":"<ul> <li> <p>it is used to remove a particular key value from a document.     e.g. db.people.update({key:value},{$unset,{key:1}});     the second key value is removed from document.</p> <p>e.g.</p> <p>db.people.update({name:\"Rahul\"},{$unset:{age:1}}) WriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 }) db.people.find(); { \"_id\" : ObjectId(\"57befca2daf90e8c76d1910e\"), \"name\" : \"Vaibhav\", \"age\" : 30 } { \"_id\" : ObjectId(\"57befdbddaf90e8c76d1910f\"), \"name\" : \"neeraj\", \"age\" : 31 } { \"_id\" : ObjectId(\"57bfa58cdaf90e8c76d19cc8\"), \"name\" : \"Rahul\" }</p> <p>db.people.update({name:\"Rahul\"},{$unset:{age:-1}}) this also removes the age from the document.</p> </li> </ul>"},{"location":"draft/mongodb-notes/#using-push-pop-pull-pullall-addtoset","title":"Using $push, $pop, $pull, $pullAll, $addToSet","text":"<ul> <li>a:[1,2,3,4,5] - is and array key value</li> <li>.update({find},{$set:{\"a.2\":5}}) - updates third value in array to 5.</li> <li>$push:{a:10} - add 10 in array.</li> <li>$pop:{a:1} - removes right most element.</li> <li>$pop:{a:-1} - removes left most element.</li> <li>$pushAll:{a:[3,54,23,5]} - pushes 4 more element to the array. It duplicates element even if it exists.</li> <li>same way we have pull and pullAll operators.</li> <li>$addToSet - add element to array if it does not exist. It does not dupicate a value.</li> </ul>"},{"location":"draft/mongodb-notes/#upserts","title":"Upserts","text":"<ul> <li>Updates if record exists else inserts. So if matching values are found then it updates. else it inserts.</li> <li>e.g. db.people.update({name:\"Rahul\"}, {$set: {age:28}} , { upsert : true } );</li> <li>The last argument tells shell to insert if name:Rahul is not in the collection.</li> <li> <p>If the matching condition is not enought to find a particular result then also mongodb will insert the new doc.     e.g.     &gt; db.people.update({age:{$gt:50}},{name:\"Sunny\"},{upsert:true})     WriteResult({         \"nMatched\" : 0,         \"nUpserted\" : 1,         \"nModified\" : 0,         \"_id\" : ObjectId(\"57c169edc1fc9c52ee7d6103\")     })</p> <p>This insers a new document to the collection: { \"_id\" : ObjectId(\"57c169edc1fc9c52ee7d6103\"), \"name\" : \"Sunny\" }</p> </li> <li> <p>it can be used when we need to mix data from someother collection and we are not sure if the doc exists or not.</p> </li> </ul>"},{"location":"draft/mongodb-notes/#multi-update","title":"Multi-Update","text":"<ul> <li>by default update command only update a single record even if first argument provides more than one result.</li> <li>to update multiple rows provide 3rd argument as { multi : true }</li> <li> <p>e.g. db.people.update( {}, {$set : { title: \"Dr\" }} , { multi : true } ) # this matches every doc because of find {}.</p> </li> <li> <p>this happens because MongoDB provides single thread to write operation.</p> </li> <li>if multiple operations are performed on single document then it is mutually shared and one process updates and waits for other to </li> <li> <p>update and so on.</p> </li> <li> <p>In multi-update there is pause and yield mechanism that follow. The multi update updates say 4 doc and then  pause to allow other write operation to occur on the document. then again picks the doc and updates other 4 doc.</p> </li> <li>it allows other readers and writers to operator.</li> <li>mongodb does not allow isolated transaction while these multi updates are occurring.</li> </ul>"},{"location":"draft/mongodb-notes/#remove","title":"Remove","text":"<ul> <li>first argument is same as find, the document should be passed. Must pass a doc, else all doc will be removed.</li> <li> <p>{name: {$gt : \"M\"}} - all after m are removed.</p> </li> <li> <p>e.g.</p> </li> <li> <p>db.people.remove({name:\"Anuj\"});</p> </li> <li> <p>WriteResult({ \"nRemoved\" : 1 })</p> </li> <li> <p>blank removes all the records one by one.</p> </li> <li>contrary to drop.</li> <li>db.coll.drop();</li> <li> <p>which removes all doc at </p> </li> <li> <p>drop is faster and also removes other data associated with the collection.</p> </li> <li> <p>by remove the indexes are not removed, while drop removes the indexes as well.</p> </li> <li>also in remove a write or read operation may see a collection with half removed records. this does not happen in case of drop.</li> <li>also in here one doc is not half removed. it is isolated and atomic to a particular read write operation.</li> </ul>"},{"location":"draft/mongodb-notes/#java-driver","title":"Java driver","text":"<ul> <li>Add dependecy in pom.xml, it adds necessary jars</li> <li>create db connection client,</li> <li>db connection handle,</li> <li>provide db name.</li> </ul>"},{"location":"draft/mongodb-notes/#java-driver-representing-documents","title":"Java Driver: Representing Documents","text":"<ul> <li>We can represent docs using BSON Document class.</li> <li>we can append to the object as many doc as we want.</li> <li>it accepts different data types as well.</li> <li> <p>Document class has helper functions like getString(), getInteger() that convert the object to particular data tyoe and then return the value.</p> </li> <li> <p>$or takes in an array of documents and combines them with or conditions.</p> </li> <li> <p>when the value is array in document and we specify it to match in find, then only the outer array is looked in.</p> </li> <li> <p>no recursion occurs or inner depth arrays are matched.</p> </li> <li> <p>projection is used to include and exclude coloumns. <pre><code>    col.find(doc)\n.projection(doc)\n.sort(doc)\n.skip(20)\n.limit(10)\n.into(doc);\n</code></pre></p> </li> <li>each of above doc can be replaced by builders.</li> </ul>"},{"location":"draft/mongodb-notes/#update-and-replace","title":"Update and replace","text":"<ul> <li>replaceOne, update is used to update the information.</li> </ul>"},{"location":"draft/mongodb-notes/#delete","title":"delete","text":"<ul> <li>deleteOne()</li> <li>deleteMany()</li> <li>find takes filter as document to provide where clause functionality.</li> </ul>"},{"location":"draft/mongodb-notes/#blog-internals","title":"Blog, internals","text":"<ul> <li>DAO is Data Access Object</li> <li>it is java class to access data of various objects. like user, session, blog etc.</li> </ul>"},{"location":"draft/mongodb-notes/#session-management","title":"session management","text":"<ul> <li>get signup</li> <li>result is singup page</li> <li>post details</li> <li>valid then writes to users, sessions table table, store cookie, redirects to welcome page</li> <li> <p>sessions table holds new session.</p> </li> <li> <p>the id value in cookie in browser is same what we store in session collection.</p> </li> </ul> <p>week 2 ends</p>"},{"location":"draft/mongodb-notes/#week-3-begins-schema-design","title":"Week 3 begins: Schema Design","text":""},{"location":"draft/mongodb-notes/#schema-design","title":"Schema Design","text":"<ul> <li>can keep in 3rd normal form.</li> <li>but in mongo keep in application read form.</li> <li>organise to suit application data access pattern.</li> <li>imp fatc</li> <li>pre joins/embed</li> <li>no join</li> <li>no constraints</li> <li>atomic operations but no transaction</li> <li>no declared schema, but has a similar struct in a collection,</li> </ul>"},{"location":"draft/mongodb-notes/#transaction-in-mongodb","title":"Transaction in MongoDB:","text":"<ul> <li>we have atomicity possible in MongoDB</li> <li>we can achieve Transaction by making the whole Transaction in one doc only, instead of in multiple docs using join.</li> <li>MongoDB will make sure that doc is locked and is seen only after an update.</li> </ul>"},{"location":"draft/mongodb-notes/#one-to-one","title":"One to One:","text":"<ul> <li>eny one can embed in other,</li> <li>should avoid embedding if doc size grows more than 16mb</li> <li>or to avoid bulk sizing.</li> </ul>"},{"location":"draft/mongodb-notes/#one-to-many","title":"One to many:","text":"<ul> <li>there should be two collections with link in id.</li> <li>in case of, \"One to Few\", the few ones should be embedded into the one.</li> </ul>"},{"location":"draft/mongodb-notes/#many-to-many","title":"Many to Many:","text":"<ul> <li>have to embedd into each other as array of IDs,</li> <li>for some reasons can be embedded as well.</li> </ul>"},{"location":"draft/mongodb-notes/#benefits-of-embedding","title":"Benefits of embedding:","text":"<ul> <li>High latency and high bandwidth of disk.</li> <li>so if disk spins once then we can quickly get data.</li> <li>but for one spin it takes time.</li> </ul>"},{"location":"draft/mongodb-notes/#trees","title":"Trees:","text":"<ul> <li>To represent trees, add ancestors, complete list with hierarchies.</li> </ul> <p>week 3 ends</p>"},{"location":"draft/mongodb-notes/#week-4-performace","title":"Week 4 - Performace","text":"<p>Performace can be increased by storage engine. from 3.0 we have pluggable storage engines.</p>"},{"location":"draft/mongodb-notes/#storage-engines-introduction","title":"Storage Engines: Introduction","text":"<p>Engine allows mongodb to talk to disks: - Mongo has pluggabel storage engines form 3.0; - it decides what ot keep in mem and wht in disk. Since disks are slow. - it doesnt effect communication bw servers. </p> <p>MMAP: - Asks to read data form memory in a page. If not found then data is brought from disk. - multile reader single writer lock - collection le=vl locking is done. - only one write in 1 collection - multile writers can happen in diff coll - in place update occurs but if size excceeds then collection is moved to a differnt place. - so power of two sizes is used in whcih a collection is kept in mem with more size as required to grow doc. - os makes decision for managing in mem and in disk, db does not deciede. - the minimum record space in MongoDB 3.0 is 32 bytes. - MMAPv1 automatically allocates power-of-two-sized documents when new documents are inserted  - MMAPv1 is built on top of the mmap system call that maps files into memory</p> <p>WiredTiger: - It is faster - document lvl concurrency - lock free, optimistic concurrency - two writes cannot happen 2gthr. - 100gb data file is brought in mem in pages. - WT manages what can be in mem and wht can be in disk, - in mem not compressed, in disk it is compressed. - compresses data of doc and indexes. - no inplace update. - writes the data to new place. and  frees old one. - marks old as unused and creates new. - this allows doc lvl concurrency. - overall faster. <code>mongod -dbpath WT -storageEngine WiredTiger</code></p> <ul> <li>new dir is required to change engine bcz it cannot read other eni=gine memory. <code>mongod -dbpath new_dir -storageEngine WiredTiger</code></li> </ul>"},{"location":"draft/mongodb-notes/#indexes","title":"indexes","text":"<ul> <li>when a collection is stored in a disk it is stored in random order.</li> <li>to find all like name = some. the scan all doc. can be million.</li> <li> <p>this determines speed and performace.</p> </li> <li> <p>index is ordered set of thing,</p> </li> <li> <p>may be alphabetically sorted. it has pointer to physical location.</p> </li> <li> <p>it uses binaray search and it will take log(2) to provide the doc.</p> </li> <li> <p>index can be for combinarion. like naem, hair color.</p> </li> <li> <p>then index entry is combinarion of cols.</p> </li> <li> <p>it can be used for just.</p> </li> <li>name</li> <li>name, hairColor,</li> <li> <p>name, hairColor, DOB</p> </li> <li> <p>but not </p> </li> <li>hairColor</li> <li> <p>or DOB.</p> </li> <li> <p>if we change anything on doc. then bTree is updated. so writes will be slower.</p> </li> <li> <p>reads will be much faster.</p> </li> <li> <p>insert all data and then create index.</p> </li> <li> <p>index uses disc space too. so cant be crated for all keys,</p> </li> <li> <p>so 10 million reocrds can be indexed for faster reads and redice disc IO.</p> </li> </ul>"},{"location":"draft/mongodb-notes/#creating-indexes","title":"Creating Indexes.","text":"<ul> <li> <p>10 million students with score of 4 exams.</p> </li> <li> <p>explain commands tells what db is doing when we query.</p> </li> <li> <p>db.students.explain.find({student_id:5});</p> </li> <li> <p>in winning plan:</p> <ul> <li>doing a collection scan/</li> <li>findOne is faster,</li> </ul> </li> <li> <p>db.students.createIndex({student_id:1}); - student id ascending.</p> </li> <li> <p>it takes a while to create an index.</p> </li> <li> <p>now the queries are nice and fast.</p> </li> <li> <p>now on explain the winning plan shows that</p> </li> <li> <p>it uses indexName student_id.</p> </li> <li> <p>db.students.explain(true).find({student_id:5});</p> </li> <li>this executes and tells docs scanned as well.</li> <li> <p>it gives execution stages.</p> </li> <li> <p>compound index:</p> <ul> <li>db.students.explain.find({student_id:1, class_id:-1});</li> <li>this sorts the index as stated.</li> </ul> </li> </ul>"},{"location":"draft/mongodb-notes/#discovering-and-deleting-indexes","title":"Discovering and deleting indexes.","text":"<ul> <li>db.students.getIndexes(); - lists indexes</li> <li>db.students.dropIndex({student_id:1}); - deletes index.</li> </ul>"},{"location":"draft/mongodb-notes/#multikey-indexes","title":"multiKey Indexes","text":"<ul> <li>these are created on arrays.</li> <li>if we have arrays liek tags.</li> <li>then index can be created for all elements in array.</li> <li>we cant combine two arrays index.</li> <li>arrays cna be combined with a single value column.</li> </ul>"},{"location":"draft/mongodb-notes/#dot-notaton-and-multi-key-index","title":"Dot Notaton and multi key index.","text":"<ul> <li> <p>to reach indide an array or embedded doc.</p> </li> <li> <p>to create index on array elemnt.</p> </li> <li> <p>db.students.createIndex({'scores.score':1});</p> </li> <li>this takes a long time and created an index.</li> <li> <p>it is multi key index.</p> </li> <li> <p>find scores.score $gt 99 with explain</p> </li> <li> <p>show s winningplan with index being used.</p> </li> <li> <p>find ppl with exam score above 99.</p> </li> <li> <p>db.students.find({'scores': {\\(elemMatch: {type:'exam', score:{\\)gt:99.9}}}});</p> </li> <li> <p>it matches for element with match at least one.</p> </li> <li> <p>but if we run it with explain the we see that in winnig plan:</p> </li> <li> <p>it finds score 99.8 to inf </p> </li> <li> <p>then finds docs with type exam, so it examined all document.</p> </li> <li> <p>so all docs were examined.</p> </li> <li> <p>we can make a mistake by using AND operator because it doesnot gurantee the correct result. </p> </li> <li>explain output tell more.</li> <li>next stage in winning plan better explains.</li> </ul>"},{"location":"draft/mongodb-notes/#index-creation-options-unique","title":"Index Creation Options, Unique","text":"<ul> <li> <p>no two docs can have same key if it is indexed.</p> </li> <li> <p>we can make a unique index.</p> </li> <li> <p>db.stuff.createIndex({thing:1}, {unique:true});</p> </li> <li> <p>this makes things column unique.</p> </li> <li> <p>and if the data has duplicate values then we cannot create the unique index. we get an error.</p> </li> <li> <p>we get duplicate key error if we inset dulplicate value.</p> </li> </ul>"},{"location":"draft/mongodb-notes/#sparse-indexes","title":"Sparse Indexes","text":"<ul> <li>ti can be used when index key is missing in doc.</li> <li>so id we have c in few docs.</li> <li> <p>then  unique index will be created on thise that have c value and rest will be left inseted of having null causing unique to break.</p> </li> <li> <p>so to create a unique index on phone numbers we can make sparse index.</p> </li> <li> <p>db.employees.createIndex({cell:1},{unique:true, sparse:true});</p> </li> <li> <p>this will make an index on and will neglect nulls.</p> </li> <li> <p>check using getIndexes.</p> </li> </ul>"},{"location":"draft/mongodb-notes/#background-index-creation","title":"Background Index Creation","text":"<ul> <li>foreground is fast</li> <li>blocks all readers and writes.</li> <li> <p>so cannot do on prod server.</p> </li> <li> <p>in Background </p> </li> <li>it is slower</li> <li> <p>dont block r/w.</p> </li> <li> <p>to work on replica set working set, take one out and create index on one and then rotate around to create on all. without performace bounty.</p> </li> <li> <p>db.students.createIndex({'scores.score':1}, {background:true});</p> </li> <li>this will not create a r/w lock in collection.</li> </ul>"},{"location":"draft/mongodb-notes/#using-explain","title":"Using Explain:","text":"<ul> <li>it does not bring data to clientt].</li> <li> <p>it is used to see whats gonna happen.</p> </li> <li> <p>from 3.0 explain changed</p> </li> <li>explain returns and explainable object.</li> <li>we can see find, updatem remove, aggregate but not</li> <li> <p>insert.</p> </li> <li> <p>db.foo.explain().find()</p> <ul> <li> <ul> <li> <ul> <li>.update() - etc..</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>we can see things like docs scanned ,  n returned</p> </li> <li>indexes used etc.</li> </ul>"},{"location":"draft/mongodb-notes/#explain-verbosity","title":"Explain verbosity","text":"<ul> <li>query Planner is default.</li> <li>we have execution stats and </li> <li> <p>allPlans Execution.</p> </li> <li> <p>these are level of output.</p> </li> <li> <p>var exp =  db.example.explain(\"executionStats\");</p> </li> <li> <p>exp.find({a: 17, b:57});</p> </li> <li> <p>it gives winning plan, query planner.</p> </li> <li>in executionstats we get </li> <li>nReturned, </li> <li>executionTimeMillis.</li> <li> <p>in each stage we ger doc returned,</p> </li> <li> <p>index is bound to min and max.</p> </li> <li> <p>index showuld be created to be used.</p> </li> <li> <p>all query should use atleast one index.</p> </li> <li> <p>so the index which is never used is waste</p> </li> <li>and query which is not using atleast one index should be optimized.</li> </ul>"},{"location":"draft/mongodb-notes/#covered-queries","title":"Covered queries:","text":"<ul> <li> <p>query which can be looked from index only without examining the documents is called covered query.</p> </li> <li> <p>so if we have i,j,k as keys and we have index on them as well.</p> </li> <li>and if we find (  { i:25, j:87} )</li> <li>then it will use index adn also use the docs.</li> <li>docs are used because we need _id filed as well/</li> <li>which is not in index and is picked from the docs.</li> <li> <p>so avoid fetching _id and get faster results.</p> </li> <li> <p>also, we need to project exactly what we need and what we don;t</p> </li> <li>if we say _id:0</li> <li> <p>then too the docs will be scanned.</p> </li> <li> <p>mongodb examines docs</p> </li> <li>we should specify which column we need and also make _id:0 to not show ID.</li> <li>then mongodb looks lesser docs. it may only look indexes</li> </ul>"},{"location":"draft/mongodb-notes/#when-is-an-index-used-choosing-an-index","title":"When is an index used, choosing an index:","text":"<ul> <li>cooses index.</li> <li>created query plans for selected.</li> <li>then the fastest plan is picked.</li> <li>winning is :</li> <li>returned all, or</li> <li>returned a threshhold sorted.</li> </ul>"},{"location":"draft/mongodb-notes/#index-sizes","title":"Index Sizes","text":"<ul> <li>we should pay attension on size of indexes</li> <li>index should fit into memory (ram) for better performace</li> <li>index size depends on storage engines</li> <li>In fact, the index size can be considerably smaller (at the cost of some CPU space) in WiredTiger with --wiredTigerIndexPrefixCompression enabled.</li> <li>size can be seen using stat command.</li> <li>e.g. db.students.stats();</li> <li>db.students.totalIndexSize();</li> </ul>"},{"location":"draft/mongodb-notes/#number-of-index-entries-cardinality","title":"Number of Index Entries, Cardinality","text":"<ul> <li>It depends on the type of values on which index is being created.</li> <li>regular index is 1:1</li> <li>sparse index, having nulls or other values may have values less than docs</li> <li>multikey index, is one that can be on array or orther collection. it has values more than index. significantly larger.</li> <li>so on update the entire tag collection need to be built again on the disk.</li> </ul> <p>--following is not revised.</p>"},{"location":"draft/mongodb-notes/#geospatial-index","title":"Geospatial Index","text":"<ul> <li>These index 2d and 3d location indexes.</li> <li>e.g.</li> <li>'location':[x,y]</li> <li>ensureIndex({\"location\":'2d',type:1});</li> <li>find({location:{$near:[x,y]}});</li> </ul>"},{"location":"draft/mongodb-notes/#geospatial-spherical-index","title":"Geospatial Spherical Index:","text":"<ul> <li>this index is called 2dsphere index.</li> <li> <p>it is used to represent the earth lat and longitudes.</p> </li> <li> <p>e.g. <pre><code>db.places.find({\nlocation:{\n$near: {\n$geometry: {\ntype: \"Point\",\n                coordiantes :[-125.256, 37.1521]\n},\n            $maxDistance:2000\n        }\n}\n}).pretty();\n</code></pre></p> </li> </ul>"},{"location":"draft/mongodb-notes/#text-index","title":"Text Index","text":"<ul> <li>these are used to create index on text.</li> <li>if we normally pass a word in find then it will match that word completely in the document. if whole set of words match a value.</li> <li>but if we make a text index on that key. then we can find using the index find.</li> </ul> <pre><code>db.coll.createIndex({\"words\":'text'});\ndb.coll.find({$text:{$search:'dog'}});\n</code></pre> <ul> <li>it searches in logical or operator.</li> <li> <p>that is it matches single words in the index.</p> </li> <li> <p>we can rank them as well using score</p> </li> <li> <p>e.g.  <pre><code>db.coll.find({$text:{$search:\"some text here\"}}, {score:{$meta: 'textScore'}}).sort({score:{$meta:'textScore'}});\n</code></pre></p> </li> </ul> <p>-- following is revised.</p>"},{"location":"draft/mongodb-notes/#efficiency-of-indexes-used","title":"Efficiency of indexes used.","text":"<ul> <li>Goal is to make r/w faster.</li> <li>selectivity - minimize records c=scanned.</li> <li>sorts - how sorts are handeled.</li> <li> <p>Examinig and making indexes fast: <pre><code>db.students.find({student_id:{$gt:500000}, class_id:54}).sort({student_id:1}).hint({class_id:1}).explain(\"executionstats\");\n</code></pre></p> </li> <li> <p>so in the above case 20k docs were returned but 80k docs were examined.</p> </li> <li> <p>to inc the efficiency we can porvide hint() to mongodb.</p> </li> <li> <p>we can pass index name to it so that it can use the index.</p> </li> <li>passing the hint() after find() reduced the nScanned and made result faster.</li> </ul> <pre><code>db.grades.find({\n\"score\":{$gte:65}\n}).sort({\"score\":1});\n</code></pre> <ul> <li>we should try to eliminate as much part of collection as possible and then further fetch the docs.</li> <li>so index should also be created in a way that first it helps in eliminating the max part and then it should sort or filter other values.</li> </ul>"},{"location":"draft/mongodb-notes/#logging-slow-queries","title":"Logging slow queries.","text":"<ul> <li>to debug we can profile whats slow in our app.</li> <li>mongo logs slow queries abive 100 ms </li> <li>it comes on the mongoD log screen.</li> </ul>"},{"location":"draft/mongodb-notes/#profiling","title":"profiling","text":"<ul> <li>writes to system profile.<ul> <li>0 off.</li> <li>1 log slow queries.</li> <li>2 log all queries.</li> </ul> </li> <li> <p>logs to system.profile.</p> </li> <li> <p>2 for debugging and see traffic.</p> </li> <li> <p>mongod --profile 1 --slowms 2</p> </li> <li>it will log slow queries taking more than 2 ms.</li> </ul> <p><code>db.system.profile.find({ns:/school.students}).sort({ts:1}).pretty();</code></p> <ul> <li>this gives all quries logged sorted by timestamp.</li> </ul> <p><code>db.system.profile.find({millis:{$gt:1}}).sort({ts:1}).pretty();</code></p> <ul> <li>this gives all queries taking more than a ms.</li> </ul> <pre><code>db.getProfilingLevel()\ndb.setProfilingLevel(1,4) // level and ms.\n</code></pre>"},{"location":"draft/mongodb-notes/#mongostat-command","title":"MongoStat command.","text":"<ul> <li>like ioStat command in unix.</li> <li> <p>gives 1sec info about ins, upd etc.</p> </li> <li> <p>:~ mongostat</p> </li> <li> <p>this is run on cmd. this gives all stats.</p> <p>e.g. <pre><code>&gt;mongostat\ninsert query update delete getmore command % dirty % used flushes vsize   res qr|qw ar|aw netIn netOut conn                      time\n*0    *0     *0     *0       0     1|0     0.0    0.0       0  2.5G 24.0M   0|0   0|0   79b    17k    1 2016-08-29T16:13:25+05:30\n    *0    *0     *0     *0       0     1|0     0.0    0.0       0  2.5G 24.0M   0|0   0|0   79b    17k    1 2016-08-29T16:13:26+05:30\n    *0    *0     *0     *0       0     1|0     0.0    0.0       0  2.5G 24.0M   0|0   0|0   79b    17k    1 2016-08-29T16:13:27+05:30\n    *0    *0     *0     *0       0     1|0     0.0    0.0       0  2.5G 24.0M   0|0   0|0   79b    17k    1 2016-08-29T16:13:28+05:30\n</code></pre></p> </li> </ul> <p>these are pretty obvi.</p>"},{"location":"draft/mongodb-notes/#mongotop","title":"mongotop","text":"<ul> <li>it is same as unix top command.</li> <li> <p>it gives a high level view of where mongo is spending its time.</p> </li> <li> <p>it tell db on which time is spent most. it tells read time and write time.</p> </li> </ul>"},{"location":"draft/mongodb-notes/#sharding","title":"Sharding","text":"<ul> <li>Splitting large collection into multiple servers.</li> <li>when we cannot get performace from a single server then we can shard.</li> <li>mongos is mongo shard that connects to shards.</li> <li>mongod has replicas.</li> <li>app talks to router, mongos.</li> <li> <p>mongos talks to mongod on each server.</p> </li> <li> <p>when u can;t get performace from one server.</p> </li> <li> <p>insert must include shard key. should be aware fo shard key for collection</p> </li> <li> <p>for update if there is no shard key then the request will be broadcasted.</p> </li> <li> <p>multi update is broadcasted.</p> </li> <li> <p>update, remove and find are broadcasted to all shard.</p> </li> </ul> <p>*## Week 5 **</p>"},{"location":"draft/mongodb-notes/#simple-aggregation-example","title":"Simple Aggregation Example","text":"<ul> <li>Same as below.</li> </ul>"},{"location":"draft/mongodb-notes/#aggregation-pipeline","title":"Aggregation pipeline","text":"<p>Same as we pass in unix thru pipe.</p> <pre><code>$project    - reshape   - 1:1\n$match      - filter    - n:1\n$group      - aggregate - n:1\n$sort       - sort      - 1:1\n$skip       - skips     - n:1\n$limit      - limits    - n:1\n$unwind     - denormalize 1:n\n$out        - output    - 1:1\n</code></pre> <p>we will using all these.</p>"},{"location":"draft/mongodb-notes/#simple-agg-example-exlained","title":"Simple agg example exlained","text":"<pre><code>    db.products.aggregate([\n{$group:\n            {\n_id : \"$manufacturer\",\n                nProd: { $sum : 1 }\n}\n}\n]);\n</code></pre> <p>This will group by manufacturer and find number of products.</p>"},{"location":"draft/mongodb-notes/#compound-grouping","title":"Compound Grouping","text":"<p>group by more than a key. <pre><code>    db.products.aggregate([\n{$group:\n            {\n_id : {\n\"manufacturer\" : \"$manufacturer\",\n                    \"category\" : \"$category\" } ,\n                nProd: { $sum : 1 }\n}\n}\n]);\n</code></pre></p> <p>Note: the _id in mongoDB can be a document as well.</p>"},{"location":"draft/mongodb-notes/#aggregation-expressions-overview","title":"Aggregation expressions overview:","text":"<ul> <li>$sum - sum or count</li> <li>$avg</li> <li>$min</li> <li>$max</li> <li>$push - builds arrays.</li> <li>$addToSet - does not duplicates.</li> <li>$first - sort the doc and then find first from group</li> <li>$last - same but last.</li> </ul>"},{"location":"draft/mongodb-notes/#sum","title":"sum","text":"<pre><code>    db.products.aggregate([\n{$group:\n            {\n_id : { \"maker\" : \"$manufacturer\" },\n                Sum_Prices : { $sum : \"$price\" }\n}\n}\n]);\n</code></pre>"},{"location":"draft/mongodb-notes/#avg","title":"avg","text":"<pre><code>    db.zips.aggregate([\n{ $group : {\n\"_id\" : \"$state\",\n                \"avg_pop\" : { \"$avg\" : \"$pop\" }\n}\n}\n]);\n</code></pre> <p>this groups by state and finds avg of population.</p>"},{"location":"draft/mongodb-notes/#addtoset","title":"addToSet","text":"<pre><code>    db.zips.aggregate([{\n$group:{\n\"_id\":\"$city\",\n            \"postal_codes\":{\"$addToSet\":\"$_id\"}\n}\n}]);\n</code></pre> <ul> <li>ids are postalcodes, what it does is, it adds all postalcodes to new</li> <li>key postal_codes and groups by city.</li> </ul>"},{"location":"draft/mongodb-notes/#max","title":"max","text":"<pre><code>    db.zips.aggregate([{\n$group:{\n\"_id\":\"$state\",\n            \"max_pop\":{\"$max\":\"$pop\"}\n}\n}]);\n</code></pre> <ul> <li>this groups by state and finds maximum population.</li> </ul>"},{"location":"draft/mongodb-notes/#group-stages","title":"Group stages:","text":"<p>Pass result through pipe line.</p> <pre><code>    db.fun.aggregate([\n{$group:{\n_id:{a:\"$a\", b:\"$b\"},\n                c:{$max:\"$c\"}\n}\n},\n                {$group:{\n_id:\"$_id.a\",\n                    c:{$min:\"$c\"}\n}\n}\n])\n</code></pre>"},{"location":"draft/mongodb-notes/#projections","title":"Projections:","text":"<ul> <li>we can:<ul> <li>remove key</li> <li>add new keys</li> <li>reshape keys</li> <li>use func:<ul> <li>$toUpper</li> <li>$toLower</li> <li>$add</li> <li>$multiply</li> </ul> </li> </ul> </li> </ul> <p>-we can show results as we want to by some operations on the values of keys. <pre><code>    db.products.aggregate([\n{$project:\n            {\n_id:0,\n                'maker':{ $toLower: \"$manufacturer\" },\n                'details' : {\n'category' : \"$category\" ,\n                                'price' : { \"$multiply\" : [\"$price\",10] }\n},\n                'item' : '$name'\n}\n}\n]);\n// below in wrong query:\n    db.zips.aggregate([{\n$project:{\n_id:0,\n            city:{$toLower:$city}, //wrong! $city has to be in \"\" . for _id as well\n            pop:1,\n            state:1,\n            zip:$_id\n}\n}]);\n// correct:\n\ndb.zips.aggregate([{\n$project:{\n_id:0,\n            city:{$toLower:\"$city\"},\n            pop:1,\n            state:1,\n            zip:\"$_id\"\n}\n}]);\n</code></pre></p>"},{"location":"draft/mongodb-notes/#match","title":"Match:","text":"<p>filter, if matches doc is pushed to pipeline. pre aggregate the filter.</p> <p>e.g. filter state CA, and then do the stuff. <pre><code>    db.zips.aggregate([\n{$match:{\nstate:\"CA\"\n}\n},\n        { $group : { _id:\"$city\",\n            population:{$sum:\"$pop\"},\n            zip_codes: { $addToSet : \"$_id\"}\n}\n},\n        { $project : {\n_id:0,\n                city: \"$_id\" ,\n                population:1,\n                zip_codes:1,\n            }\n}\n]);\n</code></pre></p> <p>Note: One thing to note about $match (and $sort) is that they can use indexes, but only if done at the beginning of the aggregation pipeline.</p>"},{"location":"draft/mongodb-notes/#sort-skip-limit","title":"Sort, Skip, Limit:","text":"<ul> <li>it is disk and memory based.</li> <li>it can be done before and after grouping.</li> </ul> <pre><code>    db.zips.aggregate([\n{$match:{\nstate:\"NY\"\n}\n},\n        { $group : { _id:\"$city\",\n                population:{$sum:\"$pop\"}\n}\n},\n        { $project : {\n_id:0,\n                city: \"$_id\" ,\n                population:1\n            }\n},\n        { $sort: {\npopulation: -1\n            }\n},\n        { $skip: 4 },\n        { $limit: 5}\n]);\n</code></pre>"},{"location":"draft/mongodb-notes/#first-and-last","title":"First and Last:","text":"<pre><code>    db.zips.aggregate([\n{ $group : { _id:{state:\"$state\", city:\"$city\"},\n                population:{$sum:\"$pop\"}\n}\n},\n        { $sort: {\n\"_id.state\":1,\n                \"population\":-1\n            }\n},\n        /## group by state, get first city */\n        { $group:{\n_id: \"$_id.state\",\n                city : { $first : \"$_id.city\" },\n                population : {$first:\"population\"}\n}\n},\n        {$sort:{\n_id:1\n            }\n};\n</code></pre>"},{"location":"draft/mongodb-notes/#unwind-example","title":"$unwind example","text":""},{"location":"draft/mongodb-notes/#double-unwind","title":"Double $unwind","text":""},{"location":"draft/mongodb-notes/#mapping-between-sql-and-aggregation","title":"Mapping between SQL and Aggregation","text":""},{"location":"draft/mongodb-notes/#some-common-sql-examples","title":"Some Common SQL examples","text":""},{"location":"draft/mongodb-notes/#limitations-of-the-aggregation-framework","title":"Limitations of the Aggregation Framework","text":"<pre><code>100 mb limit for pipeline.\n16 mb limit for python.\n\nin sharded system when we group by then aggregation query goes to \neach shard. then when we require all data, then all data goes to \nprimary shard. so same level of scalability is not found which can be\nfound in map reduce jobs in Hadoop.\n</code></pre>"},{"location":"draft/mongodb-notes/#aggregation-framework-with-the-java-driver","title":"Aggregation Framework with the Java Driver","text":"<p>*## Week 6 : APPLICATION ENGINEERING *##    </p> <pre><code>durability, that data is on disk\nfault taulerance, what happens on crash.\nsharding, distribution across servers.\n</code></pre>"},{"location":"draft/mongodb-notes/#write-concern","title":"Write Concern","text":"<pre><code>DB writes pages to memory.\nthse are written to disk, depending on mem pressure,\n\njournal is log of every single thing happening on db.\nwrites to journal as well.\nwhen journal is written then data is actually written on disk.\n\n\n\nWhen a data is persistent. we have two values that govern this, \none is W and other is J.\n\nw=1 denotes that data is written, it can be written to memnory or to \ndisk but no surity of it,\n\nj means weather or not we wait for journal to be written to disk before we continue.\n\nby default, j= false and w=1. \nj=true is that data in journal is written to disk. this gives surity\nthat the data is persistent now.\n\nthe operation performed in MongoDB is on memory and not on disk. (fast)\njournal is written periodically.\n\nif server crashed we may loose data. (w came back but journal is not written)\ndisk is 100 to 1000 times slower.\n\nby default w=1, j=false. \nthis means we;ll wait for write to be acknowledged but not journal\nto be written\nthis is fast. lil vulnerable.\n\nw=1.j=true can be done by driver at db or coll level.\nslow. vulnerability is removed.\n\nw=0 is not recommended. and the write is not acknowledged.\n\nin replicated env we have other values of w that have significance.\n</code></pre>"},{"location":"draft/mongodb-notes/#network-errors","title":"Network Errors","text":"<pre><code>we might not get response from server about write when write happened but n/w error occured.\n\nin case of insert we can try again with same _id and can at max\nget duplicate key error.\n\nin update problem occurs. so for $inc we cannot determine that \nupdate occured or not.\n\nto avoid update we can convert update to insert. delete and insert.\n</code></pre>"},{"location":"draft/mongodb-notes/#introduction-to-replication","title":"Introduction to Replication","text":"<pre><code>availability and fault toleracne. (in case of fire.)\nall are mongod.\nreplicates asynchronously to sec.\nsec elect, strict majority.\ndata written to p will be asynchronously will be written to s.\nwhen p goes down then election occurs.\nby majority the s becomes p.\nthen s becomes p and later when p comes up it comes as s.\n\nby default we have 3 replications.\n</code></pre>"},{"location":"draft/mongodb-notes/#replica-set-elections","title":"Replica Set Elections","text":"<pre><code>type of nodes:\n    regular - \n    arbiter (voting) - to vote in case of even nodes.\n    delayed/regular - disaster recovery, can be an hr behind, can;t become primary node. priority 0\n    hidden - can;t be primary. used for analytics. p=0.\n</code></pre>"},{"location":"draft/mongodb-notes/#write-consistency","title":"Write Consistency","text":"<pre><code>write will goto p \nreads can goto s as well. but may be stale data.\nlag is not determined as sync is asynchronous.\n</code></pre>"},{"location":"draft/mongodb-notes/#creating-a-replica-set","title":"Creating a Replica Set","text":"<pre><code>in real we keep on diff phy servers.\nin our case we make on one server with diff dir and diff ports.\n3 mondod instances are started.\n\nreplSet rs1 : this tells that they belong to one replica set.\n\nmkdir -p /data/rs1 /data/rs2 /data/rs3\nmongod --replSet m101 --logpath \"1.log\" --dbpath /data/rs1 --port 27017 --oplogSize 64 --fork --smallfiles\nmongod --replSet m101 --logpath \"2.log\" --dbpath /data/rs2 --port 27018 --oplogSize 64 --smallfiles --fork\nmongod --replSet m101 --logpath \"3.log\" --dbpath /data/rs3 --port 27019 --oplogSize 64 --smallfiles --fork\n\nTo run a mongod process as a daemon (i.e. fork), and write its output to a log file, use the --fork and --logpath options.\n\nwe also need to tie them together so that they can work in sync,\n\nwe need to config and tell that all are associated with each other.\n\nconfig = { _id: \"m101\", members:[\n    { _id : 0, host : \"localhost:27017\"},\n    { _id : 1, host : \"localhost:27018\"},\n    { _id : 2, host : \"localhost:27019\"} ]\n};\n\nrs.initiate(config);\nrs.status();\n\nAfter starting mongod s and then tying them together we start a \nclient using:\nmongo --port 27018\nwe don;t get a normal port.\nrs1.SECONDARY&gt; rs.status();\n\nwe get above. and the result is:\nall nodes status. a big doc.\nsec, pri, sec. all nodes info comes.\n\nwe cannot write on secondary.\n\nwe then move to primary and insert a doc.\nthen goto sec and find the same collection.\nwe can't query sec'.\nwe set \n&gt;rs.slaveOk();\nthen we can read form s.\n</code></pre>"},{"location":"draft/mongodb-notes/#replica-set-internals","title":"Replica Set Internals","text":"<pre><code>the replica sets have oplog. this is a log of change.\nthe primary writes all to oplog.\nthe secondary reads the oplog from primary and makes changes to primary.\n\nto see oplog on primary\n&gt; use local\n&gt; show collections\noplog.rs is one we need.\nit has detail of insert just performed.\n\nnow do:\n$ ps -ef | grep mongod \nthe find process id of mongod primary/\n$ kill 60494\nthis will bring down the primary.\n\nthen secondary becomes primary.\nrs.status() shows the down server as not reachable.\n</code></pre> <p>Failover and Rollback      When p fails and secondary takes its place.     now s may be behind and does not have some writes.     the p gets back as s in some time.     then p syncs with secondary to take new writes and realises that it has extra writes.     it then rollsback those writes and saves to a file.     this is failover and Rollback.</p> <p>Connecting to a Replica Set from the Java Driver </p> <p>When Bad Things Happen to Good Nodes </p>"},{"location":"draft/mongodb-notes/#write-concern-revisited","title":"Write Concern Revisited","text":"<pre><code>concerns which arise when we write to hard disk.\nw and j are the properties that govern how write will work.\nsetting w=1 will wait for primary node to respond to acknowledgement of write.\nw=2 will wait for primary as well as secondary.\nw=3 will wait for 3 to respond.\nj=1 will wait for primary to write the journal to the disk.\nhow long we wait is, is called wtimeout. it can be set in drivers.\nthese 3, w,j and wtimeout define write concern.\nthese can be set in connection, collection driver or when defining replica set.\nw:majority is used to wait until majority acknowledges the write.\n</code></pre>"},{"location":"draft/mongodb-notes/#read-preferences","title":"Read Preferences","text":"<pre><code>we usually read and write to the primary but we can set it to read from the secondary as well.\nit can be set to:\nprimary - read only from primary\nprimary preferred - if not available then read from secondary\nsecondary - only rotate among secondaries\nsecondary preferred - if not available then primary\nNearest - sends to nearest one.\ntags - sends to tagged node.\nwe can configure program to connect to secondary.\nthen we can fail primary by\nrs.stepDown()\nthen also the read continues with on secondary without faliure.\n\ne.g.\nimport pymongo\nimport time\n\nread_pref = pymongo.read_preferences.ReadPreference.SECONDARY\n\nc = pymongo.MongoClient(host=[\"mongodb://localhost:27017\",\n                              \"mongodb://localhost:27018\",\n                              \"mongodb://localhost:27019\"],\n                              read_preference=read_pref)\n\ndb = c.m101\nthings = db.things\n\nfor i in range(1000):\n    doc = things.find_one({'_id':i})\n    print \"Found doc \", doc\n    time.sleep(.1)\n\n\n    during execution if we stepDown primary. the read continues.\n</code></pre>"},{"location":"draft/mongodb-notes/#review-of-implications-of-replication","title":"Review of Implications of Replication","text":"<pre><code>seed lists - this is info and responsibility of driver to elect primary, keep all nodes data and keep track of them all.\nwrite concern - concern that w,j and wtimeout determine.\nread Preferences - how we set the reads.\nerrors can happen - event after replications, errors can happen and will continue to happen because of n/w failure, h/w failure etc. for this knowledge of data and where it goes in application is necessary.\n\nOne thing to remember is that the driver will check, upon attempting to write, whether or not its write concern is valid. It will error if, for example, w=4 but there are 3 data-bearing replica set members. This will happen quickly in both the Java and pymongo drivers. Reading with an invalid readPreference will take longer, but will also result in an error. Be aware, though, that this behavior can vary a little between drivers and between versions.\n</code></pre>"},{"location":"draft/mongodb-notes/#introduction-to-sharding","title":"Introduction to Sharding","text":"<pre><code>horizontal scalabiling.\nShard are dbs distributed.\neach shards can have replicas. these are different hosts.\nso shard s1 can have 3 replicas. R0. so s1-s5 will have 15 hosts. (5*3).\nrouter is calles mongos.\nit does sometimes range based sharding.\nso on Querying mongos knows where that particaular order_id will fall.\nit conncects quesries to diff hosts.\nwe use range based distribution.\ndone on the basis of shard key, may be order_id.\nmongos for certain order no. will send to particaular chuck.\nthese chunks lives on particular shards.\nall replicas in shards are mondgod.\nIf shard key is not in knowledge of mongos then the request is sent to all shards.\nAs of MongoDB 2.4, we also offer hash-based sharding, which offers a more even distribution of data as a function of shard key, at the expense of worse performance for range-based queries.\nSharding is at db level.\nMongoS are stateless and can easily be replicated.\n</code></pre>"},{"location":"draft/mongodb-notes/#building-a-sharded-environment","title":"Building a sharded environment","text":"<pre><code>this more of a DBA task.\nwe can setup 3 shards having 3 replicas in each.\nwe ll  have a mongos server connected to app. it listens to port number 27017 which is default.\nthe other relicas use non standard ports as these are all on same pc (hosts) and act as different hosts.\nthen we have config server (3). these have information about the shards.\nso data is broken into chunks.\nsharding can be done on\nrange based - it uses a range on say some id\nhash based - is uses hash algorithm to shard the data.\n\nbelow is the script to start a sharded system on local computer.\nour mongo will connect to mongos and not mongo d.\nthe sh.status() gives data of sharded system.\n\n\n\n# Andrew Erlichson\n# MongoDB\n# script to start a sharded environment on localhost\n\n# clean everything up\necho \"killing mongod and mongos\"\nkillall mongod\nkillall mongos                                                                                                                                                                                                                                 \necho \"removing data files\"\nrm -rf /data/config\nrm -rf /data/shard*\n\n\n# start a replica set and tell it that it will be shard0\necho \"starting servers for shard 0\"\nmkdir -p /data/shard0/rs0 /data/shard0/rs1 /data/shard0/rs2\nmongod --replSet s0 --logpath \"s0-r0.log\" --dbpath /data/shard0/rs0 --port 37017 --fork --shardsvr --smallfiles\nmongod --replSet s0 --logpath \"s0-r1.log\" --dbpath /data/shard0/rs1 --port 37018 --fork --shardsvr --smallfiles\nmongod --replSet s0 --logpath \"s0-r2.log\" --dbpath /data/shard0/rs2 --port 37019 --fork --shardsvr --smallfiles\n\nsleep 5\n# connect to one server and initiate the set\necho \"Configuring s0 replica set\"\nmongo --port 37017 &lt;&lt; 'EOF'\nconfig = { _id: \"s0\", members:[\n          { _id : 0, host : \"localhost:37017\" },\n          { _id : 1, host : \"localhost:37018\" },\n          { _id : 2, host : \"localhost:37019\" }]};\nrs.initiate(config)\nEOF\n\n# start a replicate set and tell it that it will be a shard1\necho \"starting servers for shard 1\"\nmkdir -p /data/shard1/rs0 /data/shard1/rs1 /data/shard1/rs2\nmongod --replSet s1 --logpath \"s1-r0.log\" --dbpath /data/shard1/rs0 --port 47017 --fork --shardsvr --smallfiles\nmongod --replSet s1 --logpath \"s1-r1.log\" --dbpath /data/shard1/rs1 --port 47018 --fork --shardsvr --smallfiles\nmongod --replSet s1 --logpath \"s1-r2.log\" --dbpath /data/shard1/rs2 --port 47019 --fork --shardsvr --smallfiles\n\nsleep 5\n\necho \"Configuring s1 replica set\"\nmongo --port 47017 &lt;&lt; 'EOF'\nconfig = { _id: \"s1\", members:[\n          { _id : 0, host : \"localhost:47017\" },\n          { _id : 1, host : \"localhost:47018\" },\n          { _id : 2, host : \"localhost:47019\" }]};\nrs.initiate(config)\nEOF\n\n# start a replicate set and tell it that it will be a shard2\necho \"starting servers for shard 2\"\nmkdir -p /data/shard2/rs0 /data/shard2/rs1 /data/shard2/rs2\nmongod --replSet s2 --logpath \"s2-r0.log\" --dbpath /data/shard2/rs0 --port 57017 --fork --shardsvr --smallfiles\nmongod --replSet s2 --logpath \"s2-r1.log\" --dbpath /data/shard2/rs1 --port 57018 --fork --shardsvr --smallfiles\nmongod --replSet s2 --logpath \"s2-r2.log\" --dbpath /data/shard2/rs2 --port 57019 --fork --shardsvr --smallfiles\n\nsleep 5\n\necho \"Configuring s2 replica set\"\nmongo --port 57017 &lt;&lt; 'EOF'\nconfig = { _id: \"s2\", members:[\n          { _id : 0, host : \"localhost:57017\" },\n          { _id : 1, host : \"localhost:57018\" },\n          { _id : 2, host : \"localhost:57019\" }]};\nrs.initiate(config)\nEOF\n\n\n# now start 3 config servers\necho \"Starting config servers\"\nmkdir -p /data/config/config-a /data/config/config-b /data/config/config-c\nmongod --logpath \"cfg-a.log\" --dbpath /data/config/config-a --port 57040 --fork --configsvr --smallfiles\nmongod --logpath \"cfg-b.log\" --dbpath /data/config/config-b --port 57041 --fork --configsvr --smallfiles\nmongod --logpath \"cfg-c.log\" --dbpath /data/config/config-c --port 57042 --fork --configsvr --smallfiles\n\n\n# now start the mongos on a standard port\nmongos --logpath \"mongos-1.log\" --configdb localhost:57040,localhost:57041,localhost:57042 --fork\necho \"Waiting 60 seconds for the replica sets to fully come online\"\nsleep 60\necho \"Connnecting to mongos and enabling sharding\"\n\n# add shards and enable sharding on the test db\nmongo &lt;&lt;'EOF'\ndb.adminCommand( { addshard : \"s0/\"+\"localhost:37017\" } );\ndb.adminCommand( { addshard : \"s1/\"+\"localhost:47017\" } );\ndb.adminCommand( { addshard : \"s2/\"+\"localhost:57017\" } );\ndb.adminCommand({enableSharding: \"school\"})\ndb.adminCommand({shardCollection: \"school.students\", key: {student_id:1}});\nEOF\n</code></pre>"},{"location":"draft/mongodb-notes/#implications-of-sharding","title":"Implications of sharding","text":"<pre><code>every doc shud hav shard key and it is immutable (it can;t be changed).\nindex is req for shard key\nindex should start with shard key if multiKey.\nin update, either shard key should be there or mulli update should be true. \nfor update shard key has to be specified. else it is sent to all nodes.\n\nno unique index can be set unless it is part of shard key.\nreason for no unique is that it doesn;t know about other shards.\nwe should choose shard key as one that we are going to use in most of our query as a key,\n</code></pre>"},{"location":"draft/mongodb-notes/#sharding-and-replication","title":"Sharding and replication","text":"<pre><code>they both are usually done togther.\nmongos connects to pimary of replica mainly\nfor failover within shard, mongos reconnects.\nwrite concers are still there.\nj true or w majority are still there.\nthey apply to each node.\navailability and concerns still apply.\n</code></pre>"},{"location":"draft/mongodb-notes/#choosing-a-shard-key","title":"Choosing a shard key","text":"<pre><code>it shud have sufficient cardinality (variety of values.)\nso that it can be put in all shards\nhotspoting (all requests going to one single place) shud be avoided.\nso inserts should be such that the inserts goto different shards\ne.g so username can be used as shard key. it gives nice parallelism.\n\nhotspoting in writes should be avoided. anything that is monotonously increasing should be avoided\nin shards there are $minKey and $maxKey and values within it goes into the shard.\nwhen any value is greater than highest value of $maxKey then it always goes to the highest chunk,\nso all inserts will goto one shard only.\nsharding on (vendor,order_date) is pretty well as we get lot of cardinallity.\n</code></pre> <p>*## Assignments **</p> <p>4.3     most recent 10 blog posts.     most recent 10 post for given tag.     blog post page by permalink.</p>"},{"location":"draft/notepad/","title":"Notepad Public","text":"<p>Staging area - Start with H1, later move to <code>term-notes.md</code></p>"},{"location":"draft/notepad/#android-notes","title":"Android Notes","text":"<p>ADB is utility to interact with android phone. It can install/uninstall apks. change connections etc.  All commands here, adb shell.</p> <p>Enable Developer Options &gt; USB Debugging</p> <p>adb must be installed on your mac/pc.</p> <p>Uninstall blotwares</p> <ul> <li><code>adb devices</code> see your device</li> <li><code>adb shell</code> enter phone shell</li> <li><code>pm uninstall -k --user 0 com.mipay.wallet.in</code> to use pm is pkg mgr, and uninstall an app.</li> </ul> <p>References:</p> <ul> <li>https://forum.xda-developers.com/t/uninstall-system-apps-without-root-vivo-bloatware.3817230/</li> <li>https://technastic.com/vivo-bloatware-preinstalled-apps-list/</li> </ul>"},{"location":"draft/notepad/#easy-soft-sys","title":"Easy Soft Sys","text":"<p>Color Pallet:</p> <ul> <li>Blue - #00a1e7, rgb(0,161,231) https://www.colorhexa.com/00a1e7</li> <li>Grey - #3f3f3f, rgb()</li> <li>Orange - #e74600, rgb(231,70,0)</li> </ul> <p>Font: Gill Sans Nova Extra Condensed Bold</p>"},{"location":"draft/notepad/#bookdown","title":"Bookdown","text":"<p>Quick getting started.</p> <p>Steps:</p> <ul> <li><code>mkdir bookdown</code></li> <li><code>cd bookdown/</code></li> <li><code>git clone https://github.com/seankross/bookdown-start</code></li> <li><code>cd bookdown-start/</code></li> <li><code>r</code></li> <li> <p><code>bookdown::render_book(\"index.Rmd\")</code></p> </li> <li> <p>all <code># heading 1</code> are chapters.</p> </li> <li>Add <code>Part I</code> before a chapter to make it part in a book, <code># (PART) Data Science {-}</code></li> <li><code>&gt; options(bookdown.render.file_scope = FALSE);</code> to use parts in diff directories.</li> </ul> <p>To support GitHub flavoured MarkDowm, you need to add the following line to <code>_output.yml</code> file:</p> <p><code>md_extensions: +lists_without_preceding_blankline+pipe_tables+raw_html+emoji</code></p> <p>Working on a book:</p> <ul> <li>All mds are in <code>./data_science</code> folder.</li> <li>All images are in <code>./images</code> folder.</li> <li>Add new md file to <code>./_bookdown.yml</code> file. It also has index order.</li> <li>To build and run:</li> <li><code>r</code></li> <li><code>bookdown::render_book(\"index.Rmd\")</code></li> <li>new site availabe at <code>./docs/index.html</code></li> <li><code>quit()</code> to exit R shell</li> </ul> <p>References:</p> <ul> <li>Bookdown cookbook</li> <li>Bookdown</li> <li>Rafalab dsbook</li> <li>Rafalab book source github.</li> <li>Bookdown data science notes book</li> <li>Python Visualizations in bookdown</li> <li>Using Python Environments</li> <li>Show plotly html js in Rmarkdown stackoverflow</li> <li>code options cheat sheet</li> <li>publishing on github</li> <li>pandoc markdown formats.</li> </ul>"},{"location":"draft/notepad/#digital-marketing-notes","title":"Digital Marketing Notes","text":"<p>Instagram page earning:</p> <ul> <li>original images</li> <li>regular posting</li> </ul> <p>Instagram Bot:</p> <ul> <li>Scrapper - https://towardsdatascience.com/increase-your-instagram-followers-with-a-simple-python-bot-fde048dce20d#:~:text=Open%20a%20browser%20and%20login,users%20you%20followed%20using%20the</li> <li>Post - https://www.youtube.com/watch?v=vnfhv1E1dU4</li> </ul>"},{"location":"draft/notepad/#tableau","title":"Tableau","text":""},{"location":"draft/notepad/#writeback-in-tableau","title":"Writeback in Tableau","text":""},{"location":"draft/notepad/#mega-string","title":"Mega String","text":"<p>\"( '\" +[CC interaction_artifact_ID] +\"', '\" +[CC Status] +\"', '\"+[CC Note]+\"', '\"+USERNAME()+\"' )\"</p>"},{"location":"draft/notepad/#hideinsert","title":"HideInsert","text":"<p>[CC W InsertRun] = 0</p>"},{"location":"draft/notepad/#hidereset","title":"HideReset","text":"<p>[CC W InsertRun] = 4</p>"},{"location":"draft/notepad/#incrementadd","title":"IncrementAdd","text":"<p>[CC W Incrementer]+1</p>"},{"location":"draft/notepad/#zero","title":"zero","text":"<p>0</p>"},{"location":"draft/notepad/#one","title":"One","text":"<p>1</p>"},{"location":"draft/notepad/#blank","title":"Blank","text":"<p>\"\"</p>"},{"location":"draft/notepad/#sheet-reset","title":"sheet reset","text":"<p>Saved successfully! Go To Flow View \u2b9e</p>"},{"location":"draft/notepad/#seet-sumbit","title":"Seet Sumbit","text":"<p>Submit \u2b9f</p>"},{"location":"draft/notepad/#cc-submitted","title":"CC Submitted","text":"<p>Writeback proc source</p>"},{"location":"draft/notepad/#actions-on-form","title":"Actions on form","text":"<p>select - reset - go to - next</p> <p>select - reset - set - insertrun to 0</p> <p>select - submit - set - cc mega string</p> <p>select - submit - set - increment to +1</p> <p>select - submit - set - insetRun 1</p>"},{"location":"draft/notepad/#actions-on-table-sheet","title":"actions on table sheet","text":"<p>select - table - set - insertRun 1</p> <p>select - table - set - string blank</p> <p>select - table - set - id to row selected</p>"},{"location":"draft/notepad/#plotly-d3-vizs","title":"Plotly D3 Vizs","text":"<ul> <li>poltly built on top of D3</li> <li>python api uses plotly.js</li> </ul> <p>Plotly Library:</p> <ul> <li>data - result of go.chartType(x=, y=, others=....)</li> <li>layout - title, axis, annotations</li> <li>has param <code>updatemenus</code></li> <li>There are four possible update methods:<ul> <li>\"restyle\": modify data or data attributes</li> <li>\"relayout\": modify layout attributes</li> <li>\"update\": modify data and layout attributes</li> <li>\"animate\": start or pause an animation</li> </ul> </li> <li>frames: used for animations</li> <li>we can add different frames to a chart</li> <li>this can be used to produce the animations</li> <li>Example can be found here.</li> <li>figure - final object combining data and layout.</li> </ul> <p>Dash is putting and linking many plotly charts together.</p>"},{"location":"draft/notepad/#other-plotly-products","title":"Other plotly products","text":"<p>Dash:</p> <ul> <li>Dash is Python framework for building analytical web applications.</li> <li>It is built on Flask, Plotly.js and React.js</li> <li>Just like flask, we define <code>app = dash.Dash()</code> and then at end <code>app.run_server()</code></li> <li>We can create complete site with links.</li> <li>It has intractable story.</li> </ul> <p>Chart Studio</p> <ul> <li>is like Tableau web edit and public.</li> <li>Can create and host data, charts and dashboards.</li> <li>can explore other people's work.</li> <li>charts are interactable and linked together.</li> <li>can be reverse engineered.</li> <li>can host notebooks as well.</li> </ul> <p>ObservableHQ:</p> <ul> <li>Live, web edit, d3 notebooks.</li> <li>markdown and JS blocks</li> <li>lots of d3 features. like counts, action buttons etc</li> <li>can make dasboard as well.</li> </ul> <p>References:</p> <ul> <li>How and why I used Plotly (instead of D3)</li> <li>4 interactive Sankey diagrams made in Python</li> </ul>"},{"location":"draft/notepad/#d3","title":"D3","text":"<p>Add D3 library. Then specific module.</p> <ul> <li>it is collection of module that work together</li> <li>data is bounded to the selections, it join-by-index</li> <li>By default, the data join happens by index: the first element is bound to the first datum, and so on. Thus, either the enter or exit selection will be empty, or both. If there are more data than elements, the extra data are in the enter selection. And if there are fewer data than elements, the extra elements are in the exit selection.</li> <li>selectAll() data() enter() append() - to add elements, SDEA. https://observablehq.com/@d3/d3-hierarchy?collection=@d3/d3-hierarchy</li> </ul>"},{"location":"draft/notepad/#youtube-channel-notes","title":"YouTube Channel Notes","text":"<p>Start creating a web of terms , make understand each thing, chamkao cheezo ko. makeit understnad to 6yr old guy start from docs, make reading a habit, start taking notes. math teacher lessongs, i see, i do, i ... small age learn, big understand, then decision.</p> <p>Follow:</p> <ul> <li>miguel grinberg - https://twitter.com/miguelgrinberg</li> <li>Claudio Bernasconi - https://twitter.com/CHBernasconiC</li> </ul>"},{"location":"draft/notepad/#mac-os-linux-ubuntu-notes","title":"Mac OS (Linux) Ubuntu Notes","text":"<ul> <li><code>alias vynote=\"subl ~/path/to/file/notepad.txt\"</code> add to <code>.bash_profile</code> to make shortcut</li> <li>cheat book - https://github.com/0nn0/terminal-mac-cheatsheet#english-version</li> </ul>"},{"location":"draft/notepad/#python-web-app-development","title":"Python web app development","text":""},{"location":"draft/notepad/#notes-20221224","title":"Notes - 20221224","text":""},{"location":"draft/notepad/#flask","title":"Flask","text":"<p>All Flask applications must create an <code>application instance</code> (object of class Flask). The web server passes all requests it receives from clients to this object for handling, using a protocol called Web Server Gateway Interface (WSGI).</p> <p><code>app = Flask(__name__)</code> name is passed to Flask class constructor so it knows the location of app and hence can locate static and template.</p> <p>Web Browser --request--&gt; web server --&gt; Flask app instance --route--&gt; function to execute</p> <p><code>View Functions</code> handle application URL.</p> <p>Request from client has lot of information in it, like header, user-agent, data etc. This information is available in <code>request object</code> and is made available to a <code>view-route function</code> to handle it. This object is not passed as an argument to function, rather it is made available using <code>contexts</code>. Contexts let certain objects to globally accessible, but are not global variable. They are globally accessible to only one thread. There can be multiple threads serving multiple requests from multiple client.</p> <p>There are two contexts in Flask</p> <ul> <li><code>current_app</code> variable in Application context, has info of active application.</li> <li><code>g</code> variable in Application context, temp access during handling of a request. It is reset once request is served. Holds app info hence app context.</li> <li><code>request</code>, in request context, obj having client req data.</li> <li><code>session</code>, in request context, a dictionary to store values that can be accessed in different requests from same session.</li> </ul> <p>Flask, in backend, makes these availabe to thread before dispaching a request and removes after request is handled. Explicitly, <code>current_app</code> can be made availabe by invoking <code>app.app_context()</code></p> <p>[ ] How does flask differente requests and clients?</p> <p>URL-maps can be seen using <code>app.url_map</code></p> <p><code>/static/&lt;filename&gt;</code> is special route added by Flask to serve static files.</p> <p><code>Requst Object</code> has methods and attributes having info on method, form, args, cookies, files, remote_addr, get_data().</p> <p>Request Hooks are functions that can execute code before or after each request is processed. They are implemented as decorators  (functions that execute on event). These are the four hooks supported by Flask:</p> <ul> <li><code>before_request</code> - like authenticate</li> <li><code>before_first_request</code> - only before the first request is handled. Eg, to add server initialization tasks.</li> <li><code>after_request</code> - after each request, but only if no unhandled exceptions occurred.</li> <li><code>teardown_request</code> - after each request, even if unhandled exceptions occurred.</li> </ul> <p><code>g</code> context global storage can be used to share data between hook and view functons.</p> <p>Response Object - Response is returned by view-funciton as a string (usually HTML) along with <code>status code</code> but can alos contain headers. So rather than sending comma separated tuple values, flask lets create response onject using <code>make_response()</code>.</p> <pre><code>from flask import make_response\n@app.route('/')\ndef index():\nresponse = make_response('&lt;h1&gt;Some response with a cookie!&lt;/h1&gt;')\nresponse.set_cookie('message', '51')\nreturn response\n</code></pre> <p><code>return redirect('http://www.example.com')</code> is a response with URL and status code 302, however Flask lets it do easily using <code>redirect()</code> method. Another such is <code>abort(404)</code> which is treted as exception.</p> <p>Templates <code>return render_template('user.html', name=name)</code> is another return helper function that can make use of template, which is HTML code where dynamic content can be filled on execution.</p> <p>{{ super() }}, includes code from parent block, uif overriding a block.</p> <p><code>@app.errorhandler</code> is decorator that lets return a view from template for error responses like 404 and 500.</p> <pre><code>@app.errorhandler(404)\ndef page_not_found(e):\nreturn render_template('404.html'), 404\n@app.errorhandler(500)\ndef internal_server_error(e):\nreturn render_template('500.html'), 500\n</code></pre> <ul> <li><code>url_for()</code> utility builds URL for view-function giving route from app-url-map. Eg:</li> <li><code>url_for('user', name='john', page=2, version=1)</code> would return <code>/user/john?page=2&amp;version=1</code>, they are good to build dynamic URLs that can be used in templates.</li> <li><code>url_for('user', name='john', _external=True)</code> would return <code>http://localhost:5000/user/john</code>.</li> <li><code>url_for('static', filename='css/styles.css', _external=True)</code> would return <code>http://localhost:5000/static/css/styles.css</code>.</li> </ul>"},{"location":"draft/notepad/#flask-extensions","title":"Flask Extensions","text":"<p>Most extensions use app context to initialize themselve, eg:</p> <pre><code>from flask_bootstrap import Bootstrap\napp = Flask(__name__)\nbootstrap = Bootstrap(app)\n</code></pre> <ul> <li>Bootstrap - provides templates and blocks that can be used in Jinja2 Templates</li> <li>installation <code>pip install flask-bootstrap</code></li> <li>eg, <code>{% extends \"bootstrap/base.html\" %}</code> - base.html does not exist but is available via extension. Others are <code>navbar</code>, <code>content</code>, <code>script</code></li> </ul> <pre><code>{% block scripts %}\n{{ super() }} &lt;!--includes base scripts too, else overrides--&gt;\n&lt;script type=\"text/javascript\" src=\"my-script.js\"&gt;&lt;/script&gt;\n{% endblock %}\n</code></pre> <ul> <li>Moment.js Flask-Moment - Localization of Dates and Times</li> <li>server should send UTC, client should present in local time and formatted to region using JavaScript.</li> <li><code>Moment.js</code> is perfect for this and is available as flask extension. It can be used in Jinja2 template.</li> <li><code>pip install flask-moment</code></li> <li> <p>include the script, <code>jQuery</code> is already attached as part of bootstrap</p> <pre><code>{% block scripts %}\n{{ super() }}\n{{ moment.include_moment() }}\n{% endblock %}\n</code></pre> <pre><code>from flask_moment import Moment\nmoment = Moment(app)\nreturn moment(datetime.utcnow()).format('LLL') # local time\nreturn moment(datetime.utcnow()).fromNow(refresh=True) # a few seconds ago..\n</code></pre> </li> </ul>"},{"location":"draft/notepad/#forms","title":"Forms","text":"<ul> <li>WTForms - Object Oriented Form building, rendering and validations.</li> <li>supports forms validation, CSRF protection, internationalization (I18N), rendering form and more for any Python framework, its generic. WTForms.</li> <li>Cons - It lets you build form in python and help validate it. It adds a extra learning curve than using HTML for the same. It same as ORM that you define things as class variables.</li> <li>You have to build a template but can use <code>Form.field</code></li> <li>Pros - It lets you extend forms.<ul> <li>lets you show error easily without using <code>flash</code>.</li> </ul> </li> <li> <p>Building</p> <pre><code>from wtforms import Form, BooleanField, StringField, validators\nclass RegistrationForm(Form):\nusername     = StringField('Username', [validators.Length(min=4, max=25)])\nemail        = StringField('Email Address', [validators.Length(min=6, max=35)])\naccept_rules = BooleanField('I accept the site rules', [validators.InputRequired()])\n</code></pre> </li> <li> <p>Flask-WTF integration of Flask and WTForms</p> </li> <li>Includes CSRF, file upload, and reCAPTCHA. You mostly have to use formats of WTForms but write less as few things are done automatically that are realted to Flask patter.</li> <li>Form fields are Class variables with different field type</li> <li>validator functions can help validate, liek <code>Email()</code>.</li> <li>Link to Flask-WTF</li> <li> <p>Building</p> <pre><code>from flask import Flask, render_template, session, redirect, url_for\n@app.route('/', methods=['GET', 'POST'])\ndef index():\nform = NameForm() # defined as OOP model\nif form.validate_on_submit(): # cheks POST and validates\nsession['name'] = form.name.data\nreturn redirect(url_for('index')) # POST -&gt; back to this function as GET\nreturn render_template('index.html', form=form, name=session.get('name')) # When GET\n</code></pre> </li> <li> <p>Rendering</p> <pre><code>&lt;form method=\"POST\"&gt;\n{{ form.hidden_tag() }}\n{{ form.name.label }} {{ form.name() }}\n{{ form.submit() }}\n&lt;/form&gt;\n</code></pre> <ul> <li>You can use bootstrap and flask-wtf combine to avoid writing template code and just use</li> </ul> <pre><code>{% import \"bootstrap/wtf.html\" as wtf %}\n{{ wtf.quick_form(form) }}\n</code></pre> </li> </ul>"},{"location":"draft/notepad/#databases","title":"Databases","text":"<p>Python has packages for most database engines like MySQL, Postgres, SQLite, MongoDb etc. If not, you can use ORM that lets you use Python objects to do SQL operations, SQLAlchemy or MongoEngine are such packages.</p> <ul> <li> <p>Flask-SQLAlchemy is wrapper on SQLAlchemy. You have to use SQLAlchemy pattern but it helps by making things tied to Flask way like session of SQLAlchemy is tied to web-request of flask.</p> </li> <li> <p>installation <code>pip install flask-sqlalchemy</code></p> </li> <li> <p>Initiation</p> <pre><code>from flask_sqlalchemy import SQLAlchemy\napp.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + os.path.join(basedir, 'data.sqlite')\napp.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\ndb = SQLAlchemy(app) # Object for all ops\n</code></pre> </li> <li> <p>Model - It is a Class which represents application entities, like, User, Task, Author, Book etc.</p> <ul> <li><code>__tablename__ = 'users'</code></li> <li>It has attributes that represent column name. <code>name = db.Column(db.String(64), unique=True)</code></li> <li>Relationships To create ER in OOPs way <code>users = db.relationship('User', backref='role')</code></li> <li><code>backref</code> adds a backreference to other models</li> <li><code>lazy</code> does not execute until you add executor.<ul> <li>add <code>lazy='dynamic'</code> to prevent query execution.</li> </ul> </li> <li><code>db.create_all()</code> creates all tables if they dont exist</li> </ul> </li> <li> <p>Create a new row</p> <ul> <li>Create an Object of Class to build a new row. <code>user_john = User(username='john', role=admin_role)</code></li> <li>add - <code>db.session.add(user_john)</code></li> <li><code>id</code> is added to object after <code>db.session.commit()commit()</code></li> </ul> </li> <li>Update - <code>db.session.add()</code> also edits.</li> <li>Delete - <code>db.session.delete(obj_name)</code></li> <li> <p>Read - <code>query</code> object is avilable to all model class. It has filter-options and executors that build a SQL Query statement.</p> <ul> <li>Filter-Options - <code>filter()</code>, <code>filter_by()</code>, <code>limit()</code>, <code>offset()</code>, <code>order_by()</code>, <code>group_by()</code></li> <li>Executors - <code>all()</code>, <code>first()</code>, <code>first_or_404()</code>, <code>get()</code>, <code>get_or_404()</code>, <code>count()</code>, <code>paginate()</code></li> <li>Examples</li> <li><code>User.query.all()</code> reads all records</li> <li><code>User.query.filter_by(role=user_role).all()</code></li> <li><code>str(User.query.filter_by(role=user_role))</code> returns SQL query</li> </ul> </li> <li> <p>RAW SQL - give your SQL statements</p> <ul> <li><code>db.session.execute(SQL)</code> returns cursor</li> <li><code>db.session.execute(SQL).all()</code> - returns List result set</li> </ul> </li> <li> <p>Shell Operations - CRUD from Flask Shell</p> <ul> <li><code>flask --app hello.py shell</code> start shell with app_context, python shell will not have that.</li> <li><code>db.cratte_all()</code> creates SQLite file.</li> </ul> </li> <li> <p>Migrations DB changes version controlled</p> </li> <li>When DB is handled using ORM, all changes to DB is done via ORM. If you have to add a column it is added by ORM so it will delete the table and create new but to prevent data loss in table it will create a migration script to create and populate again.</li> <li><code>Flask-Migrate</code> is wrapper on <code>Alembic</code> a SQLAlchemy migration framework.</li> <li><code>pip install flask-migrate</code></li> <li><code>from flask_migrate import Migrate</code></li> <li><code>migrate = Migrate(app, db)</code></li> <li><code>flask --app hello.py db init</code> migration directory, script is generated</li> <li><code>upgrade()</code> has data changes to be done in this migration</li> <li><code>downgrade()</code> rolls back to previous state</li> <li>Steps to make a migration<ul> <li>Make changes to model classes</li> <li><code>flask --app hello.py db migrate -m \"initial migration\"</code> to generate script</li> <li>review for accurate changes. add to source control</li> <li><code>flask--app hello.py db upgrade</code> to do migration in database</li> </ul> </li> </ul>"},{"location":"draft/notepad/#git","title":"GIT","text":"<p>If you want to create a new branch to retain commits you create, you may do so (now or later) by using -c with the switch command. Example:</p> <ul> <li><code>git switch -c &lt;new-branch-name&gt;</code></li> </ul>"},{"location":"draft/notepad/#python","title":"Python","text":"<p><code>Decorators</code> are a standard feature of the Python language. A com\u2010 mon use of decorators is to register functions as handler functions to be invoked when certain events occur.</p> <p>[ ] what is context in python</p>"},{"location":"draft/notepad/#computer-science","title":"Computer Science","text":"<p>A thread is the smallest sequence of instructions that can be man\u2010 aged independently. It is common for a process to have multiple active threads, sometimes sharing resources such as memory or file handles. Multithreaded web servers start a pool of threads and select a thread from the pool to handle each incoming request.</p>"},{"location":"draft/notepad/#oops","title":"OOPS","text":"<ul> <li>Object is a Class and has</li> <li><code>attributes</code> - variables</li> <li><code>methods()</code> - functions</li> </ul>"}]}