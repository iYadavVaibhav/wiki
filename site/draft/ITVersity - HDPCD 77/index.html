
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Data Engineering, Analytics and Visualization">
      
      
        <meta name="author" content="Vaibhav Yadav">
      
      
        <link rel="canonical" href="https://iYadavVaibhav.github.io/wiki/draft/ITVersity%20-%20HDPCD%2077/">
      
      
        <link rel="prev" href="../ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/">
      
      
        <link rel="next" href="../JSPandServlets/">
      
      <link rel="icon" href="../../assets/images/apple-touch-icon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.0.5">
    
    
      
        <title>HDPCD - Vaibhav Yadav's Wiki</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.558e4712.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.2505c338.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/timeago.css">
    
      <link rel="stylesheet" href="../../assets/stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="blue">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#hdpcd" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Vaibhav Yadav&#39;s Wiki" class="md-header__button md-logo" aria-label="Vaibhav Yadav's Wiki" data-md-component="logo">
      
  <img src="../../assets/images/apple-touch-icon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Vaibhav Yadav's Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              HDPCD
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/iYadavVaibhav/wiki" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    iyadavvaibhav.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Vaibhav Yadav&#39;s Wiki" class="md-nav__button md-logo" aria-label="Vaibhav Yadav's Wiki" data-md-component="logo">
      
  <img src="../../assets/images/apple-touch-icon.png" alt="logo">

    </a>
    Vaibhav Yadav's Wiki
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/iYadavVaibhav/wiki" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    iyadavvaibhav.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" tabindex="0" aria-expanded="false">
          Electronics and IoT
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Electronics and IoT" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Electronics and IoT
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Electronics%20and%20IoT/2021-07-27-electronics-notes/" class="md-nav__link">
        Electronics Notes, IoT, DIY, Hobby
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" tabindex="0" aria-expanded="false">
          Functional & Management
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Functional & Management" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Functional & Management
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Functional%20%26%20Management/2020-06-29-banking-notes/" class="md-nav__link">
        Banking and Finance Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Functional%20%26%20Management/2022-06-08-project-management/" class="md-nav__link">
        Project Management Notes
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" tabindex="0" aria-expanded="false">
          JavaScript
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="JavaScript" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          JavaScript
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../JavaScript/2018-07-03-nativescript-notes/" class="md-nav__link">
        NativeScript Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../JavaScript/2018-07-12-javascript-notes/" class="md-nav__link">
        JavaScript Libraries and Frameworks Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../JavaScript/2022-07-11-js-ecma6-notes/" class="md-nav__link">
        ECMA 6+ Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../JavaScript/2022-07-14-react-js-notes/" class="md-nav__link">
        React JS Notes
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" tabindex="0" aria-expanded="false">
          Tech Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Tech Notes" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Tech Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2018-06-28-mac-linux-terminal/" class="md-nav__link">
        Mac and Linux Ways
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2018-07-02-git-notes/" class="md-nav__link">
        Git and GitHub
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2019-03-05-flask-notes/" class="md-nav__link">
        Flask - WebFramework in Python
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2019-06-05-sql-notes/" class="md-nav__link">
        MySQL, SQLite and SQLAlchemy Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2020-07-14-google-cloud-platform-notes/" class="md-nav__link">
        Google Cloud Platform (GCP) and its Services
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2020-07-14-text-editors-notes/" class="md-nav__link">
        Text Editors - Customizations/Plugins/Hacks for Sublime, VS Code and Vim
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2020-07-14-web-server-notes/" class="md-nav__link">
        Web Server Configuration Guide - Ubuntu Apache WSGI Python Flask MySQL PHP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2020-07-15-flutter-notes/" class="md-nav__link">
        Flutter BLoC and Firebase Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2020-07-15-vagrant-virtualbox-notes/" class="md-nav__link">
        Vagrant and Virtual Box Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2020-08-1-pelican-python-notes/" class="md-nav__link">
        Pelican - Static Site Generator in Python - Basic Guide
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2021-05-05-datascience-notes/" class="md-nav__link">
        Data Science Learning Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2021-05-05-python-notes/" class="md-nav__link">
        Python Conda PIPs Venv RegEx R
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2021-08-24-web-development-notes/" class="md-nav__link">
        Web and App Development
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/2022-05-15-tableau-notes/" class="md-nav__link">
        Tableau Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tech%20Notes/material_mkdocs/" class="md-nav__link">
        Material for MkDocs
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" tabindex="0" aria-expanded="false">
          Blog
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Blog" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Blog
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../blog/2018-05-02-Hello-Humans/" class="md-nav__link">
        Hello Humans!
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../blog/2018-06-16-github-pages-jekyll/" class="md-nav__link">
        Github Pages and Jekyll Sites - Complete Setup
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../blog/2018-07-01-data-wrangling-in-python/" class="md-nav__link">
        Data Wrangling in Python using Pandas
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../blog/2019-06-14-syntax-highlight-jekyll/" class="md-nav__link">
        How to add syntax highlighting to Jekyll Sites
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " data-md-toggle="__nav_7" type="checkbox" id="__nav_7" checked>
      
      
      
        <label class="md-nav__link" for="__nav_7" tabindex="0" aria-expanded="true">
          Draft
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Draft" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Draft
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../HDPCD%20-%20rough/" class="md-nav__link">
        HDPCD
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ISB%20AMPBA%20DS%20ML%20AI%20DL%20Notes/" class="md-nav__link">
        ISB AMPBA Data Science Machine Learning AI DL Notes
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        HDPCD
      </a>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../JSPandServlets/" class="md-nav__link">
        JSPs and Servlets Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Java%20Notes/" class="md-nav__link">
        Java Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Lynda%20-%20Negotiating%20Your%20Job%20Offer/" class="md-nav__link">
        Negotiating Your Job Offer - Lynda
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../flume/" class="md-nav__link">
        example.conf: A single-node Flume configuration
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../hive/" class="md-nav__link">
        HIVE
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../mongo%20University%20m101j%20exam%20solutions/" class="md-nav__link">
        M101 | Exam Solutions | Mongo University
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../mongodb-notes/" class="md-nav__link">
        Mongo DB - Notes
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../notepad/" class="md-nav__link">
        Notepad Public
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
    
                  

  
    <a href="https://github.com/iYadavVaibhav/wiki/edit/master/docs/draft/ITVersity - HDPCD 77.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/iYadavVaibhav/wiki/raw/master/docs/draft/ITVersity - HDPCD 77.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.15 8.15 0 0 1-1.23-2Z"/></svg>
    </a>
  



<h1 id="hdpcd">HDPCD</h1>
<p>```sql
create alias of IP address and give it a name in /etc/host</p>
<ul>
<li>
<p>IPs and Ports:
8020  HDFS port
8888  Sandbox Home, Welcome screen
8080  Amabri, web interface to acces and monitor everything.
2222  SSH
4200  Web terminal
21000 Atlas
9995  zeppelin
15000 Falcon
6080  Ranger
50070 Hadoop information, name node to access files.
3306  mysql</p>
</li>
<li>
<p>how to SSH</p>
</li>
</ul>
<h1 id="usage">Usage:</h1>
<p>ssh <username>@<hostname> -p <port>;</p>
<h1 id="example">Example:</h1>
<p>ssh root@127.0.0.1 -p 2222
  ssh root@sandbox.hortonworks.com -p 2222</p>
<ul>
<li>Input local file into HDFS using Hadoop shell</li>
</ul>
<p>ip:50070 - takes you to name node to access files in HDFS.</p>
<blockquote>
<p>id hive
id hdfs
shows the process ID for both the process</p>
</blockquote>
<ul>
<li>HDFS directories</li>
</ul>
<p>to create directory under HDFS login as user
when we ssh root@sandbox.hortonworks.com then it is logged in as a root.</p>
<p>/root in sandbox is the home directory of linux not hdfs.
In this dir we place data and other local files.</p>
<p>Other users are each services. Like hcat,hive,maria_dev,oozie,root,solr,spark,unit,yarn.</p>
<h1 id="make-dir">make dir</h1>
<blockquote>
<p>sudo -u hdfs hadoop fs -mkdir /user/root</p>
</blockquote>
<h1 id="list-dir">list dir</h1>
<blockquote>
<p>hadoop fs -ls /user
here we have dir of all users in hdfs
/apps/hive/warehouse - it has all hive managed tables and databases.
/user/root - we created this for development purpose.</p>
</blockquote>
<ol>
<li>** HDPCD - Copy files to HDFS</li>
</ol>
<h1 id="change-owner">change owner</h1>
<blockquote>
<p>sudo -u hdfs hadoop fs -chown root:hdfs /user/root</p>
</blockquote>
<h1 id="by-default-it-is-above-only">by default it is above only.</h1>
<h1 id="copying-the-data">copying the data</h1>
<blockquote>
<p>hadoop fs -copyFromLocal /root/test /user/root</p>
</blockquote>
<h1 id="for-2nd-practice-we-are-using-hdfc-userhdpcd">for 2nd practice we are using hdfc /user/hdpcd/</h1>
<blockquote>
<p>cd /etc/hadoop/conf
core-site.xml</p>
</blockquote>
<p>fs.defaultFS parameter has information about the port number on which we connect to create the files. Like:</p>
<div class="highlight"><pre><span></span><code>&lt;property&gt;                                                                                                             
  &lt;name&gt;fs.defaultFS&lt;/name&gt;                                                                                            
  &lt;value&gt;hdfs://sandbox.hortonworks.com:8020&lt;/value&gt;                                                                   
  &lt;final&gt;true&lt;/final&gt;                                                                                                  
&lt;/property&gt;
</code></pre></div>
<h1 id="imp-to-copy-file-to-a-particular-name-node-pass-ip-addressport-8020-to-copy-file-to-that-particular-name-node">imp: to copy file to a particular name node, pass ip address:port 8020 to copy file to that particular name node</h1>
<blockquote>
<p>hadoop fs -ls http://192.xxx.xxx.121:8020/user/root</p>
</blockquote>
<p>e.g.
hadoop fs -ls hdfs://sandbox.hortonworks.com/user/</p>
<p>this will list the file on above name node.</p>
<p>it takes connections only on port 8020. rest are refused.</p>
<ol>
<li>** Create new dir in HDFS:</li>
</ol>
<blockquote>
<p>hadoop fs -mkdir /user/root/
hadoop fs -mkdir -p /user/root/dir1/dir2 #will create missing dir1 also.
hadoop fs -ls -R /user/root # dispays sub directories as well</p>
<p>hadoop fs -mkdir /dir/t{1..10} # creates dir t1,t2,...,t10
hadoop fs -mkdir /dir/t{1,2} # creates dir t1,t2</p>
</blockquote>
<p><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong><em> SQOOP </em><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong></p>
<p>** 10. SQOOP Introduction (very important): </p>
<p>https://raw.githubusercontent.com/dgadiraju/code/master/hadoop/edw/hdp/sqoop/sqoop_demo.txt #all commands</p>
<p>default port number for mysql is 3306 else it will be specified.
also dont use localhost. Use full IP or hostname.</p>
<blockquote>
<p>hostname -f #tells the hostname</p>
<p>ls -lrt /usr/share/java/mysql-connector-java*.jar #this is connector to jdbc driver</p>
</blockquote>
<h1 id="find-all-jars">find all jars</h1>
<blockquote>
<p>find / -name "mysql*.jar"</p>
<p>wget -o /dir <URL> #downloads the latest file</p>
</blockquote>
<p>unlink and relink to new version of file downloaded.</p>
<blockquote>
<p>unlink path.jar</p>
<p>ln -s path/jar link</p>
</blockquote>
<h1 id="run-sqoop-import">Run sqoop import</h1>
<p>replace jar in hadoop and hadoop yarn</p>
<blockquote>
<p>hadoop fs -ls /apps/hive/warehouse/retail_stage.db</p>
</blockquote>
<h1 id="lists-all-the-tables-imported-to-hive">lists all the tables imported to HIVE</h1>
<p>** 11 SQOOP: list and eval</p>
<blockquote>
<p>ps -ef | grep -i manager #tells weather node manager and resource manager are running or not</p>
<p>ps -ef | grep -i node #should be upa and running</p>
<p>ps -fu hdfs
ps -fu yarn</p>
</blockquote>
<h1 id="these-also-show-running-tasks">these also show running tasks</h1>
<blockquote>
<p>scp -P 2222 code/hdpcd/retail_db.sql root@sandbox.hortonworks.com:/root/data/retail_db.sql
copies files from mac to remote vm via scp</p>
</blockquote>
<p>mysql&gt; create database retail_db;</p>
<p>mysql&gt; create user retail_dba identified by 'hadoop';</p>
<p>mysql&gt; grant all on retail_db.* to retail_dba;</p>
<p>mysql&gt; flush privileges;</p>
<p>mysql -u retail_dba -p</p>
<p>mysql&gt; source /root/data/retail_db.sql;</p>
<p>mysql&gt; show tables;              </p>
<h1 id="to-verify-the-results">to verify the results</h1>
<p>mysql&gt; select * from categories limit 10;</p>
<p>hadoop fs -copyFromLocal /root/data/dummyText.txt /user/root</p>
<blockquote>
<p>mysql -u retail_dba -p #to check if mysql is running</p>
<p>hadoop fs -mkdir /user/root/sqoop_import # just a dir to import</p>
<p>sqoop help #show commands and how to use the same.</p>
</blockquote>
<p>to list databases use hostname only.
while to list databases use connection param /database_name</p>
<blockquote>
<p>sqoop eval #used to run a query after --query "my sql"</p>
</blockquote>
<p>list-databases</p>
<p>[root@sandbox data]# sqoop list-databases \</p>
<blockquote>
<p>--connect "jdbc:mysql://sandbox.hortonworks.com:3306" \
--username retail_dba \
--password hadoop</p>
<p>sqoop list-tables --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" --username retail_dba --password hadoop</p>
</blockquote>
<ul>
<li>Instead of using the --table, --columns and --where 
  arguments, you can specify a SQL statement with the --query argument</li>
</ul>
<p>e.g.
sqoop import 
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" 
  --username retail_dba --password hadoop 
  --query "select * from categories where category_department_id=8 and $CONDITIONS"<br />
  --target-dir /user/hdpcd/d1/categories_query/ 
  -m 1 
  --delete-target-dir</p>
<ul>
<li>in sqoop import, the target directory should not exist.</li>
</ul>
<p>sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --as-textfile \
  --target-dir=/user/root/departments</p>
<p>sqoop eval \</p>
<blockquote>
<p>--connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
--username retail_dba \
--password hadoop \
--query "select count(1) from order_items"</p>
</blockquote>
<ul>
<li>
<p>in sqoop import-all-tables there is warehouse-dir and not target-dir.</p>
</li>
<li>
<p>during import tablename.java file is created in dir from where we execute the import/import-all-tables statement.</p>
</li>
<li>
<p>these are basically ORM java classes.</p>
</li>
<li>
<p>when saving as avro data file. tablename.avro file is created form where we launched the import command. </p>
</li>
<li>It has table details and columnn names in JSON format.</li>
</ul>
<p>sqoop import-all-tables \
-m 12 \
--connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
--username=retail_dba \
--password=hadoop \
--as-avrodatafile \
--warehouse-dir=/apps/hive/warehouse/retail_stage.db</p>
<p>add driver parameter then above import will work
--driver com.mysql.jdbc.Driver</p>
<ul>
<li>To create avro format externa table in hive. We need to use avro classes and avsc file created while importing data from sqoop to avro file format.
  Copy the avsc file to hdfs dir and provide that in avro.schema.url</li>
</ul>
<p>hadoop fs -put departments.avsc /user/hdpcd/departments.avsc</p>
<p>e.g.:</p>
<p>CREATE EXTERNAL TABLE departments
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION 'hdfs:///user/hdpcd/departments_avro'
TBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/hdpcd/departments.avsc');</p>
<p>eval command can be used to run query and stored procedures as well.</p>
<p>** 12. SQOOP Import</p>
<h1 id="sqoop-import-refer-sqoop-documentation-as-well">sqoop import, refer sqoop documentation as well.</h1>
<blockquote>
<p>sqoop --help #use when u get stuck</p>
</blockquote>
<p>*imp -m is used as number of mappers in import-all-tables</p>
<p>-m 12 #means 12 parallel threads will run together.</p>
<p>--as-avrodatafile #saves in avro format. same as JSON</p>
<p>--warehouse-dir #give directory where data needs to be saved</p>
<p>boundary conditions are set in background to break data into number of buckets.(min,max)
Then this data is inserted into db by usign where clause on data. 
Then this data is processed using parallel threads passed in -m or --num-mappers</p>
<p>the data is placed in import data location and we can see data using ls command.
table is a folder and inside that folder we have different files which have table data in part/</p>
<p>the parts are made by -m command. default is 4.</p>
<p>so if we pass -m 12, the data will be stored in 12 parts.</p>
<p>the data in file is saved as CSV as default delimiter is comma.</p>
<p>to verify records inserted, use:</p>
<blockquote>
<p>hadoop fs -cat /dir/dw/table/part-m-*|wc -l</p>
</blockquote>
<h1 id="this-will-cat-all-parts-and-count-the-records">this will cat all parts and count the records.</h1>
<p>same can be verified using count(*) in :</p>
<blockquote>
<p>sqoop eval --query ""
or mysql.</p>
</blockquote>
<p>** 13 -  SQOOP Import - hive import</p>
<blockquote>
<p>hadoop fs -ls /user/hive/warehouse </p>
</blockquote>
<h1 id="this-is-where-all-data-of-hive-lives">this is where all data of hive lives</h1>
<p>.db extension directories represent database.</p>
<blockquote>
<p>hive #takes to hive CLI</p>
</blockquote>
<p>hive&gt; create database sqoop_import;</p>
<h1 id="creates-database">creates database</h1>
<p>in hive dfs =&gt; hadoop fs</p>
<p>from above command sqoop_import.db directory will be created under warehouse directory</p>
<p>hive&gt;show databases; #shows databases.</p>
<blockquote>
<p>use db; # to show databases.</p>
</blockquote>
<p>if we dont specify the compression codec then it is compressed as gzip. it is default compression.</p>
<p>--outdir java_files 
creats plane old java pojo class. it is optional.</p>
<blockquote>
<p>describe formatted departments;
shows detailed information, location.</p>
<p>dfs -du -s -h /path</p>
</blockquote>
<h1 id="gives-size-of-directory-in-which-data-is-stored">gives size of directory in which data is stored.</h1>
<p>** 14. SQOOP import - add on</p>
<blockquote>
<p>--hive-database can be used to import to a particalar database.</p>
</blockquote>
<p>e.g.:
sqoop import 
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" 
  --username retail_dba --password hadoop --table orders 
  --hive-import --create-hive-table 
  --hive-database hdpcd_d1 
  --driver com.mysql.jdbc.Driver</p>
<p>This also does the same work:
sqoop import 
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" 
  --username retail_dba --password hadoop --table orders 
  --hive-import --create-hive-table --hive-table hdpcd_d1.orders2 
  --driver com.mysql.jdbc.Driver</p>
<p>** 15. sqoop import</p>
<p>we have something called boundry query 
it is used to load the data.
It selects min and max from id (primary key)</p>
<p>no of mappers provide how data needs to be splitted before loading. It creates as number of buckets.</p>
<p>boundary-query is important as it can improve performance of import. The query may take time to compute min and max. so we can use boundry query.</p>
<p>--table and --query cannot be used together.</p>
<p>e.g.
-- Boundary Query and columns
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --target-dir /user/root/departments \
  -m 2 \
  --boundary-query "select 2, 8 from departments limit 1" \
  --columns department_id,department_name</p>
<p>to import table using sqoop import command and table is without primaty key then we have to use --split-by or give number of mappers as 1</p>
<p>to use where clause we have to pass conditions in --where as argument.</p>
<ul>
<li>split by e.g.</li>
</ul>
<p>-- query and split-by
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --query="select * from orders join order_items on orders.order_id = order_items.order_item_order_id where $CONDITIONS" \
  --target-dir /user/root/order_join \
  --split-by order_id \
  --num-mappers 4</p>
<p>** 16 - sqoop import</p>
<p>import from sqoop into hive, an particular table</p>
<p>hive parameters can be used to import data into the table</p>
<p>we have many parameters like table, dir, overwrite. 
These are self explainatory.</p>
<p>in case of error, we might need to clean up the staging directory where the hive imports the data.</p>
<p>It is a dir in non hive warehouse fs.
hdfs://sandbox.hortonworks.com:8020/user/root/departments</p>
<ul>
<li>to load to a particular database use:
  --hive-table retail_ods2.departments \</li>
</ul>
<p>--create-hive-table 
can be used to create table at time of load.</p>
<p>--where 
it can be given as parameters.
col <operator> value</p>
<p>can be given in single/double quotes.</p>
<p>--append
simply appends to end of file.</p>
<p>** 17  SQOOP Import - incremental import</p>
<p>it is used to insert data which is left from being imported.
it uses last value inserted to load remaining values.</p>
<p>so one way to import incremental is to use where clause.
like where id&gt;7; this is traditional way.</p>
<p>sqoop way is:
to use 
--check-column #like id
--incremental #append
--last-value #7</p>
<p>or when using --incremental as lastmodified then pass timestamp in #--last-value</p>
<p>e.g.:
  --check-column "department_id" \
  --incremental append \
  --last-value 7 \</p>
<ul>
<li>SQOOP job:
sqoop job --create sqoop_job \
  -- import \
  --connect ...</li>
</ul>
<p>other job related commands:
sqoop job --list</p>
<p>sqoop job --show sqoop_job</p>
<p>sqoop job --exec sqoop_job</p>
<p>** 18. sqoop export</p>
<p>HDFS to mysql</p>
<p>export appends to the table in mysql by pusing duplicate records as well.</p>
<p>use simple export command with various arguments
it imports into mysql table. we need to pass the direcctory from which data is to be picked.</p>
<p>by default it inserts data and does not update the data. to update the data we need to use separate argument.</p>
<p>so in export the files are divided into splits which is parallel processing of threads.
these threads depends on number of mappers.
so for 10gb file. if we have 128mb block then we will have roughly 80files.
now if we have 4 mappers then each mapper will process 20 files in each thread.</p>
<p>--update-key department_id \
  --update-mode allowinsert</p>
<p>** 19. SQOOP export - merge/upset</p>
<p>for adding data to mysql, we can create file in local fs and then move it to hdfs, then we have to use export command to bring those records to mysql.
we can use update-key and update-mode to append the data.</p>
<p>key can be only primary or unique key only. Composite key can be given by comma seperated. If we dont pass key then we may get duplicate records.</p>
<p>mode can be updateonly or allowinsert.</p>
<p>so old record will be updated and new record will be inserted.</p>
<p>in updateonly mode new data is not inserted only matching key record gets updated.
in allowinsert mode both update and insert occurs.</p>
<p>*imp 
If we dont have primary key in mysql table then the update will not happen, only new records will be inserted.</p>
<p>*how it fails
Insert data into existing table without passing --update-key parameter.
the export fails.
It fails because duplicare record is inserted for primary key. 
If mapper fails it runs the same task 4 times. 
this is built in default feature. 
In this case it will fail all 4 times with same error duplicate entries for primary key.</p>
<p>In other cases, when we dont have primary key, then if export fails then it mapper will commit in batches after say 1000 successfull insert and beyond that whatever fails is attempted to be inserted 4 times.
So data may have 4 duplicate records for last batch.
It might be the case when we are inserting null in not null column.</p>
<p>so sqoop export provides option for staging table with command --clear-staging-table
In this case data in staging table will be cleared and new records will be inserted in staging table. hence, target table will be secured.</p>
<p>we have to give following parameters:
--Merge
sqoop merge 
  --merge-key department_id \
  --new-data /user/root/sqoop_merge/departments_delta \
  --onto /user/root/sqoop_merge/departments \
  --target-dir /user/root/sqoop_merge/departments_stage \
  --class-name departments \
  --jar-file <get_it_from_last_import></p>
<p>** 20. sqoop import delimiters</p>
<p>fields terminated by and line terminated by
null-string, and 
null-non-string</p>
<p>so if we have data having ',' or other delimiters as value then we have to enclose the values.
this can be done by using 
--enclosed-by \"
since double quotes is special charachter hence we have to escape it by back slash.</p>
<p>Now all data in file will be enclosed like "Fan Shop"</p>
<p>--escaped-by
special characters in data can be escaped using this.</p>
<p>--fields and lines
we can terminate filds by say | and lines by say :
and enclose by say double quotes.</p>
<p>all 3 above are special characters and need to be escaped.</p>
<p>--mysql-delimiters
provides regular mysql delimiters.</p>
<p>*Same arguments for import to HIVE.</p>
<p>login to hive shell.
use db;
then</p>
<blockquote>
<p>describe formatted departments_test;</p>
</blockquote>
<p>this will show complete details of departments.</p>
<p>when you have null data then take special care of delimiters.</p>
<p>** 21.  SQOOP Export delimiters.</p>
<p>-1 for numeric null
and nvl for string null</p>
<p>In hive null is \u001
hive default terminated by \001
these are octal numbers.</p>
<p>input-null-string = nvl
input-null-non-string = -1</p>
<p>run export by above parameters</p>
<p>null will be inserted in mysql table.</p>
<p>** 22. Sqoop file formats</p>
<p>to import the data in binary format import data in hdfs as a sequence file.</p>
<p>avsc file format saves the data in somewhat json format.</p>
<p>so delimiters and file formats depends on data we have in the file and 
we should take care of null characters and data as well.</p>
<p>** 23. Sqoop Job, Merge</p>
<p>Sqoop job is a predefined job syntax which is created to run a sqoop command as and when required.</p>
<p>there should be a space between import and --</p>
<p>eg:
sqoop job --create sqoop_job \
 -- import
 -- connect
 other such stuff</p>
<p>**to see all jobs
sqoop job --list
shows a list of jobs</p>
<p>sqoop job --exec
to execute the job</p>
<p>**sqoop Merge</p>
<p>it merges data in hdfs not from db</p>
<p>so it matches on an id and picks new record from the dump.
we have to give a seperate new directory for merged results.</p>
<p>a java file is created for each import command.
we can to java_files directory to see all java files.</p>
<p>when we run sqoop import we get jar file path in the logs.
java class file name is same as that of table we are importing</p>
<p>so on merge, new records are inserted and existing are updated with latest value.</p>
<p>use any command with --help
to find syntax and commands.</p>
<p><strong><em>*</em></strong><strong><em>*</em></strong><strong><em> FLUME </em></strong><strong><em>*</em></strong><strong><em>*</em></strong></p>
<ol>
<li>** Flume Introduction
how to play with memory channel is main thing to learn.</li>
</ol>
<p>** 25. flume configuration and starting an agent</p>
<p>**starting an agent</p>
<blockquote>
<p>flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template</p>
<p>cd /etc/flume/conf #this is where we have template file having sqquence generator.</p>
</blockquote>
<p>so we have 1st an agent named agent.
it has 3 things:
- sources, its name, type:seq, channel
- channels, name, type:memory|file, capacity:100.
- sinks, name, type:logger, channel</p>
<p>so in example we use sequence generator to generate log kind of data that flume will read and bring in to sink/memory.</p>
<p>the command we provide is like</p>
<p>flume-ng agent -n agent -f /etc/conf/flume-conf.properties.template</p>
<p>we can create our own flume conf file anywhere in and just provide the location to the -f parameter.</p>
<p>HISTORY:
cd /etc/flume/conf
ls -lrt
cat flume-conf.properties.template
cd
flume-ng agent -n agent -f /etc/flume/conf/flume-conf.properties.template </p>
<p>** 26. FLUME - Memory channel Capacity</p>
<p>Maily covers properties of memory.</p>
<p>copy example conf file with parameters having details of net logger and web service that run on it
then copy this and paste it in any directory on local machine</p>
<blockquote>
<p>mkdir flume 
vi example.conf</p>
</blockquote>
<p>paste the contents from flume docs</p>
<p>type bind port type capacity all are defined here</p>
<p>now
flume-ng agent -n a1 -f example.conf</p>
<p>it will start flume agent.</p>
<p>now telnet from another terminal</p>
<p>telnet localhost 44444</p>
<p>type what u wish to.
.
.
.
.</p>
<p>this will be passed to flume agent.</p>
<p>the data will be transfed form memory channel</p>
<p>main focus should be on memory properties.</p>
<p>kill telnet session.</p>
<ul>
<li>Memory:
it has two properties:</li>
<li>capacity and</li>
<li>transaction capacity</li>
</ul>
<p>also,</p>
<p>type, 
keep-alive
byteCapacityBufferPercentage
byteCapacity</p>
<p>may be asked to increase/decrease the capacity of various parameters</p>
<p>saving data to hive/hdfs is not in scope of certi.</p>
<p>memory can be cinfugured in conf file, mainly has two parameters capacity and transactionCapacity.</p>
<p>Code:</p>
<h1 id="use-a-channel-which-buffers-events-in-memory">Use a channel which buffers events in memory</h1>
<p>a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100</p>
<p><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong><em> PIG </em><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong></p>
<p>** 27. PIG Introduction</p>
<p>pig commands, how to use.
see if services are up and running.
HDFS, YARN, MapReduce should be up and running</p>
<p>PIG does not have any daemon so it will run if above 3 r running</p>
<p>to run command u can goto pig grunt shell</p>
<p>make a file in /user/root&gt;demo.txt</p>
<p>bag=table
tuple=row</p>
<p>so when we write a pig script, behind the scene a map reduce job is genereated by api.</p>
<p>this is a jar file is executed in background.</p>
<p>take the job id from logs and goto ambari</p>
<p>services&gt; yarn&gt; quick links&gt; resource manager using
see last job that is run
click on history to see details.</p>
<p>if we do not write on grunt shell then we can make pig script file.</p>
<ol>
<li>** write and execute pig latin script.</li>
</ol>
<blockquote>
<p>pig
grunt&gt;exec pig_demo.pig</p>
</blockquote>
<p>this will run MapReduce job of pig.</p>
<p>pig -help show all possible options to be used with pig</p>
<p>** 29. PIG - Word Count</p>
<p>describe shows data type of a bag.</p>
<p>*explain shows MapReduce plan. how data will be read.</p>
<p>we use tokenize to create bag of tuples. each touple is one word.</p>
<p>words = FOREACH lines GENERATE FLATTEN (TOKENIZE(line)) as word;</p>
<p>describe words;
thos shows words as bag of tokens.
explain words shows comma separated words.</p>
<p>TOKENIZE breaks line into words and makes a bag of it
FLATTEN makes words in seperate line.</p>
<p>here word is alias.</p>
<blockquote>
<p>grouped = GROUP words by word;</p>
</blockquote>
<p>this will create a bag for each word.</p>
<p>so key and its bag in which words are repeated.</p>
<blockquote>
<p>wordcount = FOREACH grouped GENERATE group, COUNT(words);</p>
</blockquote>
<p>this will print word and number of times for each word.</p>
<p>** 30. pig execution life cycle</p>
<p>all statements are validated. MapReduce job runs only when dump/illustrate/store is executed.</p>
<p>** 31. LOAD data to pig relation woithout schema</p>
<p>so we should have mysql installed with retail_db database in it.</p>
<p>also this db should be imported to sqoop via hdfs.</p>
<p>data can be imported to /user/root/sqoop_import/</p>
<p>we have to use positional notation to work on relationless schema</p>
<p>we can cast the colums as well</p>
<p>dept_id = FOREACH departments GENERATE (int) $0;</p>
<p>** 32. LOAD data to pig relation with schema</p>
<p>u should have data in sqoop_import directory</p>
<p>department  = LOAD 'dir/sqoop_import/departments' using PigStorage(',') AS (dept_id: int, dept_name: chararray);</p>
<p>** 33. Load data from HIVE table.</p>
<p>fot this we should have HIVe, database in it and tables in it,</p>
<p>we have to use </p>
<blockquote>
<p>pig -useHCatalog</p>
</blockquote>
<p>this will take to grunt shell along with enbling HCatalog</p>
<p>so it will use meta data of hive table and will pick data types from HCatalog.</p>
<p>then when we describe the pig relation we see that all the fields are defined and have schema.</p>
<p>** 34. Transform to match HIVE schema</p>
<p>ok, so here we have two ways to load in pig from hive
one is using:
HCatalog, db.tablename</p>
<p>second:
giving location of hive table which is a file having data.</p>
<p>in 1st case the schema is picked from HCatalog metastore</p>
<p>and in 2nd case we have to define schema as (json)</p>
<p>task is to match schema in both the case.</p>
<p>** 35. Group the data on one or more PIG relation</p>
<ul>
<li>To get count of all rows
group the data.</li>
</ul>
<p>so if we need count of all records then we can pass ALL.</p>
<p>if we need count along with where a=b then we need to pass an expression to GROUP argument</p>
<p>for null in pig use
!= '';</p>
<p>** 36. Grouping one or more relation
* Group by key</p>
<p>we give a key to group the data and then we can count the data.</p>
<p>** 37. remove null values from a relation</p>
<p>so in filer we just have write by column is not null.</p>
<p>** 38. Store data into a folder in HDFS</p>
<p>cannot store in existing directory.</p>
<p>verify if data is present in hdfs directories</p>
<p>then load the data into pig.</p>
<p>simply instead of dump use store/</p>
<p>pig will create just one file in new directory</p>
<p>PigStorage is used to provide the delimiters.</p>
<p>BinStorage stores the data in binary format (machine readable format).</p>
<p>always validate your results.</p>
<blockquote>
<p>STORE <alias> INTO <path> USING JsonStorage('|');
this stores data in json format.</p>
</blockquote>
<p>so data can be stored as text, binary or Json format and other format as well.</p>
<p>** 39. Store the data from a Pig relation into a Hive table.</p>
<p>goto hive shell
create datanase pig_demo;</p>
<p>use pig_demo;</p>
<p>create tables in hive database;</p>
<p>goto pig shell -useHCatalog.
verify that data is in hdfs dir.</p>
<p>LOAD path USING PigStorage(',') as ...;</p>
<p>on STORE pass class of HCatalog.</p>
<p>STORE alias INTO 'db.tbl' USING HCatalog class();</p>
<p>Schema in pig has to match with column names and datatype in hive table. 
Else the STORE will not work.</p>
<ol>
<li>**Sort the output of a Pig relation </li>
</ol>
<p>load data from hive into pig alias.</p>
<p>then
alias = ORDER alias by $1;
sorts by column 2.</p>
<p>alteast 2 MapReduce jobs for orderBy</p>
<p>above is without schema and sorted using positional notation.</p>
<ol>
<li>**Apache Pig - Remove the duplicate tuples of a Pig relation (using distinct)</li>
</ol>
<p>alias = DISTINCT alias [PARTITION] [PARALLEL]</p>
<p>[] are used to provide reducers and performance tuning.</p>
<p>distinct only works on the relations and not the columns</p>
<ol>
<li>** Specify the number of MapReduce job for pig script</li>
</ol>
<p>PARTITION BY is not in scope, it is related to hash partitioning.</p>
<p>PARALLEL is used to give number of reduce tasks</p>
<p>to set default parallel value for all jobs</p>
<blockquote>
<p>set default_parallel 2;</p>
</blockquote>
<p>so for each job it will use parallel tasks as many as possible.</p>
<p>if we provide parallel in transformational level then that many reducers will run.</p>
<ol>
<li>**JOIN Theory:</li>
</ol>
<p>IJ, OJ: LOJ, ROJ, FOJ</p>
<ol>
<li>
<p>** Pig - Join two datasets using Pig - 01 Introduction</p>
</li>
<li>
<p>** PIG - INNER JOIN</p>
</li>
</ol>
<p>in exam validate after each step
COUNT_STAR is a function in pig</p>
<p>validation at each step by describe or explain formatted is imp.</p>
<p>use pig HCatalog so that schema can be loaded.</p>
<ol>
<li>OUTER JOIN</li>
</ol>
<p>cannot use positional notation we have to define the schema.</p>
<p>without schema, join is difficult.</p>
<p>to access column of a table use ::</p>
<ol>
<li>**Replicated join using pig</li>
</ol>
<p>*replicated join is done when data we need to join is small enough to fit into the memory.</p>
<p>it uses replicated cache and the smaller file is distributed across all jvms for mappers</p>
<p>resouce manager is 8088.</p>
<ol>
<li>**run pig using tez.
just use -x tez to run as tez engine</li>
</ol>
<p>in pig.properties</p>
<p>exectype=mapreduece</p>
<p>this is default execution engine</p>
<ol>
<li>** registering JAR, define alias of UDF and invoking UDFs</li>
</ol>
<p>@todo:</p>
<p>jar -tvf to see classes in jar</p>
<p>find / -name "<em>piggybank</em>.jar"</p>
<p>then in pig shell</p>
<p>REGISTER the jar</p>
<p>we can make alias for fully qualified class name.
this alias can be used in statement.</p>
<p><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong> HIVE <strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong></p>
<ol>
<li>
<p>Data Analysis</p>
</li>
<li>
<p>to create orc table, create a text table and then CTAS from text file stored as orc.
e.g. create table sfo_weather stored as ORC as select * from sfo_weather_text;</p>
</li>
<li>
<p>when we create an external table in hive then it has a location, the schema, delimiters should match the data at that location. 
  on mataching the table is itself populated with that data.</p>
</li>
<li>
<p>data can be loded in hive in following way:</p>
</li>
<li>by using load data inpath:
      it can be loaded from local,
      from HDFS.
      can be appended or overwritten.</li>
<li>by using HCatalog in pig storage.</li>
<li>by CTAS.</li>
<li>
<p>INS*F</p>
</li>
<li>
<p>To load data into partition table select the partition columns as well. Also if only particular partition needs to be saved them we can define them in partition (a=b,c=d). then we need not select these columns. </p>
</li>
</ol>
<p>E.G. LOAD DATA INPATH '/user/data/pv_20080608_us.txt' INTO TABLE page_view PARTITION (date='20080608', country='US')</p>
<p>from sfo_weather s
insert into table weather_partitioned_all 
partition (year,month)
select s.station_name,s.DayOfmonth, s.precipitation, s.temperature_min, s.temperature_max,s.year,s.month
;</p>
<ol>
<li>HIVE architecture:</li>
</ol>
<p>HIVE = HDFS, metastore, JARs which compile sql queries to MapReduce jobs</p>
<p>so in hive the database is a dir and inside it are directories of tables, inside them are files/</p>
<p>for hdfs paths onlt hadoop command can go and not linux commands.</p>
<p>etc/hive/conf has hive-set.xml</p>
<p>use this to see db used and name and password.</p>
<p>db=mysql
db_name=metastore</p>
<p>this has all metadata
it has information about the file copied to hdfs.</p>
<p>it is casesensitive</p>
<p>sudo find / -name "<em>hive</em>.jar"</p>
<p>these convert sql query to MapReduce jobs amd submit to hadoop cluster.</p>
<ol>
<li>** Write and execute hive query 01
this has a lot to cover with join and everything.</li>
</ol>
<p>hive
set fs.defaultFS; #tells name node</p>
<p>dfs -ls /user/root # tells ki hive can connect to hdfs</p>
<p>simply write the hive queries and it executes as a map reduce job.</p>
<p>when we load data from hdfs, that is, load data inpath, then the directory is deleted after succesfull import.</p>
<ol>
<li>** Write and execute hive query 02</li>
</ol>
<p>queries on githubusercontent</p>
<ul>
<li>like insert into table, we have insert overwrite table.</li>
<li>
<p>the partition column in table cannot be a normal column in table. It is sepecified separately.
  the values are also not stored in file but are picked from folder names.</p>
</li>
<li>
<p>to insert into partition column we ins</p>
</li>
<li>
<p>** HIVE Managed Table.</p>
</li>
</ul>
<p>it is simple table created using query.</p>
<ol>
<li>** HIVE External table.</li>
</ol>
<p>to copy file form mac to sandbox use scp command.</p>
<blockquote>
<p>scp host guest</p>
</blockquote>
<p>then use put to copy file to hdfs</p>
<p>In EXTERNAL table we can define a location where we can store a table.</p>
<p>When we delete a table only metadata is deleted adn not the files stored/</p>
<p>for managed table use describe formatted and see the directory is a default one.</p>
<p>in external we hve to specify</p>
<p>if table is to be used by pig hive spark and others then make external table
for hive use only use managed table.</p>
<ol>
<li>**Create retail_db tables in hive</li>
</ol>
<p>in exam use STORED AS AVRO  un;ess they give custom input format and output format.</p>
<ol>
<li>**Create Hive Partitioned Table</li>
</ol>
<p>create table (...)
 partitioned by (col datatype)</p>
<p>Type of partitioning:
-list
-range
-hash</p>
<p>these are in hive but without terminology.</p>
<p>in hive we have list partitioning as partition by 
and hash paartitonjing as bucket.</p>
<p>so when we partition we create one more colimn on which we partjioon and this can be an existing column or can be a new column as well.</p>
<p>then for each distinf records inthat column that many folders are created inside a table folder;</p>
<p>so the partion column is not stored in the data file.
when we do select form tbale then value of the partiton column is picked from the folder name it self.</p>
<p>we can either alter table to create a static partition in which er pass a vluer for a column or we cna create dtynamic partition in which we hive created partition on runtime itself.</p>
<p>strict mode should be off is we use dnamic partition.</p>
<ol>
<li>** BUcketed table.</li>
</ol>
<p>CLUSTERED BY [col] SORTED BY [col] INTO n BUCKETS</p>
<p>in partition we mention col names and data types both but in case of buciket we only mention the col names and not the data type;</p>
<p>you can create a bucket and partition seperately but
you in case of both you first have to create a partition and then create a bucket.
after creating a bucker you cannot create a partition.</p>
<p>advantage of bucketing is to divide sdata into equal parts.
we cna use fast sampling queries onthat data.</p>
<ol>
<li>** CTAS AND ORC file formats.</li>
</ol>
<p>ctas = create table as select.</p>
<p>for ORC</p>
<p>create table ..
 ....
use STORED AS orc
as
select ..</p>
<p>show create table orders_orc;</p>
<h1 id="shows-complete-sql-to-create-table">shows complete sql to create table.</h1>
<ol>
<li>**Specifying file format and delimiters</li>
</ol>
<p>formats can be file format like text, rcm orc, paraquet, avro
also we cna use cutom input format and output format.
then we have to give sede class file as well.</p>
<p>so just have to pass stored as .</p>
<p>delimiter row format fields terminated by..</p>
<ol>
<li>**Load data from local file system and HDFS to Hive table</li>
</ol>
<p>LOAD DATA LOCAL INPATH 
load data from local file system, cent os</p>
<p>LOAD DATA INPATH 
loads data from hdfs.</p>
<p>overwrite onto will delet else will append,</p>
<p>load data inpath moves the data from hdfs hence it delets from hdfs.</p>
<ol>
<li>**Load data from hive select query (using insert) and compressed data</li>
</ol>
<p>insert overwrite table select * from..</p>
<p>else the data will be appended..</p>
<p>so COMPRESSED will compress the data in file.</p>
<p>set hive compress value to true.
by default it is false.</p>
<ol>
<li>** DML in HIVE. insert update delete.</li>
</ol>
<p>there are limitations. begin commit and rollback are not supported. 
rest all is auto commit.</p>
<p>only orc file format is supported in update and delete.</p>
<p>hive.txn.manager should be set to DBTxnManager.</p>
<p>tables must be bucketed.</p>
<p>so for dml operations we should have 3 things in table structure.
1. clustered into buckets.
2. stored as orc
3. tblproperties ('transactional'='true')</p>
<p>then only we can make a table ready for update and delete.</p>
<p>for next insert it will again create new directory, having n buckets in each.</p>
<p>for update also new directoy is added. over the time compession will take place and all files will be merged into one file.</p>
<p>only soft delete happens that is data becomes unavailable and will be cleaned on compession.</p>
<ol>
<li>**Hive joins, execution engines (tez and mr) and explain/execution plan</li>
</ol>
<p>set hive.execution.engine=tez;</p>
<p>this will make query run in tez engine.</p>
<p>so for exam remember the parameter that needs to be changed.</p>
<p>*execution plan:
explain can be used to explain the execution of the query.</p>
<p>practice more in joins from certi point of view.</p>
<ol>
<li>**Hive Sub Queries and Total Ordering (ORDER BY)</li>
</ol>
<p>cannot use sub query in IN clause.
can only be used in from clause.</p>
<p>subquery should have alias.</p>
<p>*SORT by: this can be used when we donot have to order entire data.</p>
<p>so basically in this use order by on a derived column having an alias.
like order by count in group by query.</p>
<ol>
<li>**Practice Exam - Getting Started.</li>
</ol>
<p><strong><em>*</em></strong><strong><em>*</em></strong><em> Setting up AWS </em><strong><em>*</em></strong><strong>*</strong></p>
<p>password of amazon account.
used ICICI credit card ending 8009, CVV not asked.</p>
<p>Download KeyValue pair for ssh and save it.</p>
<p>connect vnc on public dns and using port 5901.
password for vm is hadoop, when asked in vnc viewer.</p>
<p>launch terminal
run $ ./start-all-services.sh
this will launch ambari.</p>
<p>ambari has username and password ambari.</p>
<p>wait while services are up and running.</p>
<p>exam overvirew gives all this information, like launching and username, password.</p>
<p>perform all taks using user horton, not root.
veriy by whoami</p>
<p>in exam directory we have exam task.</p>
<p>about 10 questions, 2sqoop, 1hdfs, 3pig, 4hive. may b 1 flume.</p>
<p>stop the instance if you need break. we can continue from where we left when we resume.</p>
<ul>
<li>Practical:</li>
</ul>
<p>Public DNS: ec2-54-201-130-145.us-west-2.compute.amazonaws.com
Public IP:  54.201.130.145</p>
<p>To SSH:     ssh -i ~/Downloads/hwx-practise-exam.pem ubuntu@ec2-54-201-130-145.us-west-2.compute.amazonaws.com</p>
<p>then sudo su - horton</p>
<p>then start working by starting script.</p>
<ul>
<li>To copy from aws vm:</li>
</ul>
<p>scp -i ~/Downloads/hwx-practise-exam.pem ubuntu@ec2-54-187-249-103.us-west-2.compute.amazonaws.com:/home/horton/solutions/a.txt ~/Downloads/datasets/sol/</p>
<p><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong><strong> Mains </strong><strong><em>*</em></strong><strong><em>*</em></strong><strong><em>*</em></strong><strong>*</strong></p>
<p>Passport ID proof. Original and copy.
1600x900 monitor
15minutes before we can start the exam. so for 6-8 slot.
5:45 exam start.
5:30 everything up and running. Mobile DND on, 4G on. 
5:00 home. 
4.45 leave office.</p>

  <hr>
<div class="md-source-file">
  <small>
    
      Last update:
      <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class="timeago" datetime="2023-01-19T21:24:37+00:00" locale="en"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2023-01-19</span>
      
        <br>
        Created:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class="timeago" datetime="2023-01-19T21:24:37+00:00" locale="en"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2023-01-19</span>
      
    
  </small>
</div>





                


              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2016 - 2023 Vaibhav Yadav
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://in.linkedin.com/in/iyadavvaibhav" target="_blank" rel="noopener" title="in.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://github.com/iYadavVaibhav/" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://twitter.com/iYadavVaibhav/" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.kaggle.com/iyadavvaibhav/kernels" target="_blank" rel="noopener" title="www.kaggle.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M304.2 501.5 158.4 320.3 298.2 185c2.6-2.7 1.7-10.5-5.3-10.5h-69.2c-3.5 0-7 1.8-10.5 5.3L80.9 313.5V7.5q0-7.5-7.5-7.5H21.5Q14 0 14 7.5v497q0 7.5 7.5 7.5h51.9q7.5 0 7.5-7.5v-109l30.8-29.3 110.5 140.6c3 3.5 6.5 5.3 10.5 5.3h66.9q5.25 0 6-3z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://public.tableau.com/profile/iyadavvaibhav" target="_blank" rel="noopener" title="public.tableau.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M11.654.174v2.203H9.682v.58h1.972V5.16h.696V2.957h1.97v-.58h-1.97V.174h-.348zm6.03 2.262-.002 1.623v1.623h-2.957v.927h2.957v3.188h1.043l.011-1.582.02-1.576 1.465-.02 1.46-.01v-.927h-2.953V2.436h-.522zm-12.407.06v3.19H2.291v.925h2.986v3.19h.985V6.61h3.013v-.925H6.262V2.496H5.77zm6.086 5.27v3.593H8.06v1.188h3.304v3.596h1.28v-3.596h3.309v-1.188h-3.31V7.766h-.637zm9.721 1.55v2.221h-2.012v.811h2.012v2.261h.887v-2.261H24v-.811h-2.029v-2.22h-.422zm-19.111.131v2.174H0v.621h1.973v2.194h.667v-2.194h2v-.62H2.609V9.446h-.318zm15.709 4.516v3.254h-3.016v.927h3.016v3.217h1.072v-3.216h2.986v-.928h-2.986v-3.254h-.533zm-12.463.008v3.246H2.262v.928h2.957v3.189H6.32v-3.189h2.955v-.928H6.32V13.97h-.55zm6.316 4.578.002 1.103v1.1H9.566v.812h1.971v2.262h.928l.012-1.119.017-1.143h1.969v-.812h-2v-2.203h-.465z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["toc.follow", "navigation.top", "search.highlight", "search.suggest", "content.action.edit", "content.action.view", "content.code.copy"], "search": "../../assets/javascripts/workers/search.e5c33ebb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.51d95adb.min.js"></script>
      
        <script src="../../js/timeago.min.js"></script>
      
        <script src="../../js/timeago_mkdocs_material.js"></script>
      
        <script src="../../assets/javascripts/extra.js"></script>
      
        <script src="../../assets/javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>